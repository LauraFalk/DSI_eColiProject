---
title: "Escherichia coli Data Exploration in the Upper Santa Cruz River"
output: html_notebook
---

```{r setup, include=FALSE}
library(chron)
library(corrplot)
library(data.table)
library(dataRetrieval)
library(janitor)
library(knitr)
library(lubridate)
library(tidyverse)
library(xts)
library(zoo)
opts_chunk$set(echo = TRUE)
```

## Processing the USGS File

### Notes
This data is from the USGS continuous monitoring gage of the Santa Cruz River at Tubac. Data is all published and available for public use.

### Code
I have commented out the data pull for USGS data. It is many years and takes lots of time to grab all 300k records when running this script. To save time, I've outputted the result of: 
readNWISuv(site_id,c('00060','00065'), "2009-07-01","2022-08-30", "America/Phoenix")
as USGS_09481740.CSV and saved this within the raw data folder on 9/12/2022.

Triple hashes (###) within code blocks are meant to indicate code that has been commented-out or added in order to expedite the processing (by use of a saved csv file of processed data rather than repeating the processing each time). If code is fully automated, these chunks are to be altered.
# Original code may be un-commented and re-run if desired. Data after 1/10/2022 are currently provisional and is still subject to change during USGS QAQC procedures.
```{r}
# Inputs formatted for read NWIS function.
site_id <- '09481740'
startDate = "2009-06-01"
endDate = "2022-08-30" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)

# Input timezone for read NWIS and POSIXT functions
tz="America/Phoenix" 

#### Creates table in R
###USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)

###Read in raw data. If using code above, do not run this line.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
```

2015 had some times that were off by a few minutes from the normal 15- minute interval. This will ensure that everything is standardized. Values are rounded to the nearest 15-minute interval.
```{r}
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw

# Round data and put into a format for merging. 
# DateTime2 is used throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
```

There are duplicate entries which have arisen from the 15-minute standardization, continuing the issues from 2015. Some separate values could be rounded to the same DateTime.
```{r}
# Determines how many values are affected by the rounding process (and any other duplicates)
USGS_Standardized %>% 
  group_by(DateTime2) %>% 
  summarise(n=sum(n())) %>% filter(n>1)
```


The duplicate DateTime can be fixed by using the mean of the values (they are within 15 minutes of one-another and should not be hugely variable). The mean will be used in place of the original two values.
```{r}
# Summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))

# Rename the column to something meaningful.
setnames(USGS_Standardized_append,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))

# Merge the rounded data back in and rename the column name to something meaningful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_append, by="DateTime2",all.x=TRUE, all.y = FALSE)

# Remove the pre-averaging flow column.
USGS_Standardized <- USGS_Standardized %>%
  subset(select = -c(X_00060_00000)) %>%
  distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)

# Re-check for duplicate values. This should equal zero.
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)

# Remove append table - data has been merged
rm(USGS_Standardized_append)

```

Though the data is nearly complete, there are still some missing values for the 15 minute intervals. Currently the missing rows are not included in the dataframe.

This will create a full time series to include indicator of no data.
```{r}
# Create the time series, formatted as POSIXct
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-06-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))

# Rename the Column DateTime2 to indicate properly formatted column.
colnames(TimeSeries)[1] = "DateTime2"

# Follow through  - properly format DateTime2
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)

# Left join data with time series, sort by DateTime2
USGS_Standardized <- merge(x=TimeSeries,y=USGS_Standardized,by="DateTime2",all.x=TRUE, all.y = FALSE)

# Remove the created time series. Data has been merged
rm(TimeSeries)
```

Look at the data to ensure that it is doing what we want. I know that 7/1/2009 at 04:30 has no data.
```{r}
# View 7/1/2009 04:30
USGS_Standardized %>%
  filter(DateTime2 == '2009-07-01 04:30:00')
```
Now that nulls are created, lets try and reduce them.
```{r}
# Tests to see how many nulls are present in the current dataset.
# 9/5/2022: 40548 records are null
count(USGS_Standardized %>%
 filter(is.na(Discharge_CFS)))
```

We should be able to fill some of the nulls by taking the average of the two surrounding values. 
```{r}

USGS_Standardized$date <- as_date(USGS_Standardized$DateTime2)
# Create a dummy column.
USGS_Standardized$count <- 1

# Count the number of readings per day
TimeFlag <- USGS_Standardized %>%
  filter(!is.na(Discharge_CFS)) %>%
  group_by(date) %>%
  summarize(value = sum(count))

# Append the reading count value to the standardized data
USGS_Standardized <- right_join(TimeFlag, USGS_Standardized)

# Name this something meaningful
setnames(USGS_Standardized, c("value", "X_00060_00000_cd", "X_00065_00000", "X_00065_00000_cd"), c("DailyFlowCount", "Discharge_cd", "USGS_Staff", "USGS_Staff_cd"), skip_absent = TRUE)

# Remove extraneous columns and reorder
USGS_Standardized <- USGS_Standardized %>%
  select(DateTime2, date, Discharge_CFS, USGS_Staff, Discharge_cd, tz_cd, DailyFlowCount)

# Remove Timeflag table. Merged into data
rm(TimeFlag)

```

Start to fill null data where applicable. This will fill in all data into a new column.
```{r}
#Filter for columns with a daily flow count greater than 88 (no more than 2 hours per day missing.)
USGS_Standardized_Fill <-
  USGS_Standardized %>%
  filter(DailyFlowCount>88)

# Fill in missing data
USGS_Standardized_Fill$Filled <-  na.approx(USGS_Standardized_Fill$Discharge_CFS, rule = 2)

#Join back in
USGS_Standardized <- right_join(USGS_Standardized_Fill,USGS_Standardized)

# Remove the old data
rm(USGS_Standardized_Fill,USGS_Raw)

```

Check how many nulls there are now that some of the data is filled. More data filled would mean more assumptions.
9/12/2022: Current count of null data is decreased to 37385
```{r}
# Fill in the un-filled values
USGS_Standardized$Filled<- ifelse(is.na(USGS_Standardized$Discharge_CFS) == TRUE,USGS_Standardized$Filled,USGS_Standardized$Discharge_CFS)

count(USGS_Standardized %>%
 filter(is.na(Filled)))

```
Clean up the columns nd environment
```{r}
#Remove the un-filled discharge column
USGS_Standardized <-  USGS_Standardized %>%
  subset(select = -c(Discharge_CFS))

#Rename to something meaningful
setnames(USGS_Standardized,old=c("Filled"),new=c("Discharge_CFS"))


```

#To determine if the river is rising, use quantiles
```{r}
CFS_Quantiles<- quantile(USGS_Standardized$Discharge_CFS, na.rm = TRUE)

USGS_Standardized <- USGS_Standardized %>% 
  mutate(DisDif = Discharge_CFS - lag(Discharge_CFS))

# This will create a binary variable or either rise of fall. Rise = 1, fall = 0. It will allow me to more easily create summary statistics.
USGS_Standardized$DisDif2 <- ifelse(USGS_Standardized$DisDif>0,1,0)

# Create a numeric classifier. 
# 1 = Low Flow, 2 = Base flow, 3 = High and Rising Flow 4 = High and Falling Flow
USGS_Standardized$Stage <- ifelse(USGS_Standardized$Discharge_CFS<=CFS_Quantiles[2], 1, 
  ifelse(USGS_Standardized$Discharge_CFS > CFS_Quantiles[2] & USGS_Standardized$Discharge_CFS <= CFS_Quantiles[4],2,
  ifelse(USGS_Standardized$Discharge_CFS > CFS_Quantiles[4] & USGS_Standardized$DisDif2 == 1,3,
  ifelse(USGS_Standardized$Discharge_CFS > CFS_Quantiles[4] & USGS_Standardized$DisDif2 == 0,4, NA))))

# Remove fields with variables used for calculations
rm(CFS_Quantiles)

```


Visualize the data to ensure that it is being accurately classified.
```{r}
# Graph all flow class data
# Missing values error is expected.
USGS_Standardized %>%
 filter(DateTime2>as.POSIXlt.character("2015-01-01") 
        & DateTime2 < as.POSIXlt.character("2015-10-15")) %>%
  ggplot(aes(x=DateTime2, y=Discharge_CFS, color=Stage))+
  geom_point()

# Filter to data below 100 to better view the pattern. 
USGS_Standardized %>%
 filter(DateTime2>as.POSIXlt.character("2015-01-01") 
        & DateTime2 < as.POSIXlt.character("2015-10-15")) %>%
  filter(Discharge_CFS<100) %>%
  ggplot(aes(x=DateTime2, y=Discharge_CFS, color=Stage))+
  geom_point()
```


## Merging the E. coli data sources and limiting to sample size.

### Data Retrieval
I have raw data from the water quality portal (EPA) a summary of ADEQ, USGS, FOSCR and NPS (Tumacacori) data along with a download of NPS-SODN quarterly sampling data (approximately 35 samples). 

Data location and security: https://www.epa.gov/waterdata/water-quality-data. Data is published and public. SODN data is unpublished and should be considered provisional for this project.

Storet: https://www.waterqualitydata.us/ 
Country: United States of America (NWIS, STEWARDS, STORET)
State: Arizona (NWIS, STORET)
County: US, Arizona, Santa Cruz County (NWIS, STORET)
Site type: Stream (NWIS, STEWARDS, STORET)
Date from: 7/1/2009
Date to: 8/30/2022
Sample Media: Water (NWIS, STEWARDS, STORET)
Characteristic Group: Microbiological (NWIS, Storet)
Data profiles: Sample results (biological metadata)

SODN: Data was pulled using a sql query on a DOI/NPS server. More information can be provided by the SODN data manager, currently Helen Thomas. Data was manually manipulated to match the storet output prior to reading into R.

### Sampling Locations Information

Storet: https://www.waterqualitydata.us/ 
Country: United States of America (NWIS, STEWARDS, STORET)
State: Arizona (NWIS, STORET)
County: US, Arizona, Santa Cruz County (NWIS, STORET)
Site type: Stream (NWIS, STEWARDS, STORET)
Date from: 7/1/2009
Date to: 8/30/2022
Sample Media: Water (NWIS, STEWARDS, STORET)
Characteristic Group: Microbiological (NWIS, Storet)
Data profiles: Site Data Only

SODN: Single sampling location coordinates were provided by NPS and manually added to the storet download CSV location file.

### Sampling Location processing
The storet download file was placed into Arcmap and points within the sample range were exported to the included raw csv file. The locations for data download were chosen from the Upper Santa Cruz River. Locations are limited to North of the NIWWTP and Sonoita Creek confluence to avoid data noise from two likely point sources. All locations sampled should contain the same water without significant other inputs

### Below, The data was limited to:
1. sample location (above)
2. E. coli only (not total coliforms)
3. same method (EPA Standard Method 9223B, Colilert, sampling procedures)
4. comparable date range (Post 2009 NIWWTP significant upgrade project)


## Code
Read in the raw data files
```{r}
RawStoret <- "Data/Raw/ecoli_Storet_Raw.csv"
RawSODN <- "Data/Raw/ecoli_SODN_Raw.csv"
RawLocation <- "Data/Raw/ecoli_Locations_StoretandSODN.csv"

RawStoret_data <- read.csv(RawStoret)
RawSODN_data <- read.csv(RawSODN)
Locations <- read.csv(RawLocation)
```

Merge the SODN and Storet Raw Data files
```{r}
ecoli_data <- rbind(RawStoret_data, RawSODN_data)

# Use Janitor to clean null columns
ecoli_data<- remove_empty(ecoli_data, c("cols"))

# Remove the pre-merged raw data
rm(RawStoret_data, RawSODN_data)
```


Now we must clean up the data further.
Keep only the columns which contain E. coli data. Other colifoms or enterolert (etc) procedures are within the STORET output. 

Note: Fecal coliforms may also be applicable, but for the initial study I would like to stick solely to Escherichia coli. Nick Paretti (USGS) indicated that this maybe a better options due to the increased varaiblility. Fecal coliforms (non-species specific) have maximum results much more often and may be less meaningful.
```{r}
# STORET has standardized the output as "Escherichia coli"
ecoli_data <- ecoli_data[ecoli_data$CharacteristicName == "Escherichia coli", ] 
```

I also want to remove anything with a time zone other than MST. I cannot verify if this is a typo or an actual data point.
With the 9/13 data pull this is only one record.
```{r}
# STORET has standardized the output as "Excherichia coli"
ecoli_data <- ecoli_data[ecoli_data$ActivityStartTime.TimeZoneCode == "MST", ] 
```

Remove columns with methods other than 9223B or a synonym.The methods are not standardized in STORET. It is assumed that all forms of Colilert and SM9223B represent comparable results. Colilert comes in both an 18 and 24 hour test, but they are assumed to provide a comparable output. 
```{r}
ecoli_data <- ecoli_data[ecoli_data$ResultAnalyticalMethod.MethodName == "E coli, water, Colilert (24-28)"|
                                    ecoli_data$ResultAnalyticalMethod.MethodName ==  "COLILERT (EDBERG)"|
                                    ecoli_data$ResultAnalyticalMethod.MethodName ==  "COLILERT"|
                                    ecoli_data$ResultAnalyticalMethod.MethodName ==  "SM 9223B", ] 
```

Remove dates before 2009 when the plant was updated. June 2009
Note: SODN data file had a different date format. Raw CSV output from SODN was updated on 9/13/2022
```{r}
ecoli_data$DateAsDate <- ymd(ecoli_data$ActivityStartDate,tz=tz)
ecoli_data$DateAsDate <-format(as.POSIXct(ecoli_data$ActivityStartDate),format='%Y-%m-%d')
ecoli_data <- ecoli_data[ecoli_data$DateAsDate >= "2009-07-01", ] 
```


Merge the files based on monitoring location identifier (outputted by STORET).Monitoring locations were outputted from an ARCGIS lasso of the range between the NIWWTP/Sonoita Creek inputs and the end of the Upper Santa Cruz River. The single SODN monitoring location was manually added to the end of this file before import.

This will decrease the number of rows in the dataset because it excludes measurements outside of the study range (namely the sonoita creek data)
```{r}
ecoli_data<- merge(ecoli_data, Locations, by = c("MonitoringLocationIdentifier"))
```

Remove unnecessary tables. 
```{r}
rm(Locations, RawLocation, RawSODN, RawStoret)
```

Remove any null columns created by reducing the dataset size.
```{r}
# Use Janitor to clean null columns
ecoli_data<- remove_empty(ecoli_data, c("cols"))
```

## Further processing of the e. Coli data

Remove columns that contain only 1 entry. If all entries are the same value it will not be useful for classification. 
```{r}
# Count the number of distinct values.
ecoli_data <- keep(ecoli_data, ~n_distinct(.) > 1)
```

View the data. There are still columns I don't need.
```{r}
# Use the summary function to look at what data exists. 
#Commented out because it is not necessary for continued data processing.
#summary(ecoli_data)
```

Take out columns that don't have meaningful data.
Date - date of sample
ActivityStartTime.Time - time of sample
MonitoringLocationIdentifier - Unique location ID
ResultMeasureValue - reported measured value
ResultDetectionConditionText - identifies above or below detection limits. Currently incomplete/unstandardized.
ActivityConductingOrganizationText - Whose fault this measurement is
ActivityTypeCode - QAQC sample, field sample, etc.
DistFromSonoita - Roughly estimated linear distance (as the crow flies)from the Sonoita input to the sampling location

```{r}
# Select only columns that I want to use for analysis
ecoli_data<- ecoli_data %>%
  select(DateAsDate, 
         ActivityStartTime.Time,  
         MonitoringLocationIdentifier,
         #HydrologicCondition,
         #HydrologicEvent,
         #ResultValueTypeName,
        # ResultStatusIdentifier,
         ResultMeasureValue,
         ResultDetectionConditionText,
         ActivityTypeCode,
         ActivityConductingOrganizationText,
         DistFromSonoita)

```

Create a column to indicate if the data is above or below detection limits. 
```{r}
# The maximum test capacity is 2419 MPN. Above this value was a dilution technique.
ecoli_data$QuantificationLimit <- ifelse(ecoli_data$ResultMeasureValue > 2419.6 | ecoli_data$ResultDetectionConditionText == "Present Above Quantification Limit", "Present Above Quantification Limit", "Value")

# This will standardize the non-detects
ecoli_data$QuantificationLimit <- ifelse(ecoli_data$ResultDetectionConditionText == "Present Below Quantification Limit"|ecoli_data$ResultDetectionConditionText == "Not Detected", "Present Below Quantification Limit", ecoli_data$QuantificationLimit)
```

Remove QAQC samples
```{r}
# Blanks and replicates will be discounted for this analysis. Only field samples should remain.
ecoli_data <- ecoli_data %>%
  filter(ActivityTypeCode != "Quality Control Sample-Field Replicate" & ActivityTypeCode !="Quality Control Sample-Equipment Blank")
```

I want to create a column of data standardized to the detection limit of SODN sampling (2419.6). Some samples were much higher than that because they were diluted, but it appears not all of them were. 
```{r}
ecoli_data$StandardizedResult <- ecoli_data$ResultMeasureValue
ecoli_data$StandardizedResult <- ifelse(ecoli_data$QuantificationLimit == "Present Above Quantification Limit", 2419.6, ecoli_data$StandardizedResult)

ecoli_data$StandardizedResult <- ifelse(ecoli_data$QuantificationLimit == "Present Below Quantification Limit",0, ecoli_data$StandardizedResult)

ecoli_data %>%
  count(StandardizedResult == 2419.6)
# I currently have 431 maximum results.

ecoli_data %>%
  count(StandardizedResult >126)
#772 are greater than the single standard EPA guideline (126 MPN)

ecoli_data %>%
  count(StandardizedResult >235)
#664 samples are greater than 235, my normal comparison for warm effluent water.

ecoli_data %>%
  count(StandardizedResult <1)
#There are only 3 non-detect

ecoli_data %>%
  count(QuantificationLimit == "Present Below Quantification Limit")
#Verified that there are only 3 non-detect

ecoli_data %>%
  count(is.na(StandardizedResult))
# There are no NA values
```

Remove unnecessary, unstandardized columns.
```{r}
ecoli_data<- ecoli_data %>%
  select(-ResultMeasureValue,
         -ResultDetectionConditionText,
         -ActivityTypeCode)
```


## Combine the ecoli, USGS, and environmental variable data.
USGS - discharge and stage data
ClimateAnalyzer (ClimateAnalyzer.org) - Tmax, Tmin and precip
NOAA (https://psl.noaa.gov/enso/climaterisks/years/top24enso.html) - El Nino / La Nina
National Park Service Streams Monitoring Protocol (NPS.GOV/SODN) - Seasonal Data


Merge the ecoli data with the USGS (flow) data.
```{r}
# Ensure that this is in the correct time format. Always problematic, so this is over-coded to ensure success.
ecoli_data$DateTime2 <- as.POSIXct(format(paste(ecoli_data$DateAsDate,ecoli_data$ActivityStartTime.Time), format = '%Y-%m-%d %H:%M'), tz = tz)

ecoli_data$DateTime2 <-
round_date(as.POSIXct(ecoli_data$DateTime2), "15 minutes")


data_Merge<- left_join(ecoli_data, 
                          USGS_Standardized, by = c("DateTime2"))

data_Merge %>%
  count(is.na(Discharge_CFS))
# 75 records are still without discharge. this is the one I care the most about.
```

Remove raw data, now merged
```{r}
rm(USGS_Standardized, ecoli_data)
```


Remove the nulls from the merge.
Total remaining after removoal is 1139 on 9/13/22.
```{r}
data_Merge2 <- data_Merge %>%
  filter(!is.na(Discharge_CFS))
  
```


Look at what the data currently looks like. 

The red line is 235, the single sample maximum for full body contact in Arizona. 

The blue dotted line indicates the state standard for an average of three+ measurements (full body contact in Arizona). It is not technically applicable here, but gives a baseline. It may  be used to flag "Concerning-but-not-technically-bad" levels of E. coli.

The green line is 575, the single sampe maximum for partial body contact in Arizona. 
```{r}
data_Merge2 %>%
  ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
  geom_point()+
  geom_hline(yintercept=126, linetype="dashed", 
                color = "blue", size=0.2) +
  geom_hline(yintercept=235, linetype="dashed", 
                color = "red", size=0.2)+
  geom_hline(yintercept=575, linetype="dashed", 
                color = "dark green", size=0.2)
```

Now I want to see what it looks like without the outlying discharge values.
```{r}
data_Merge2 %>%
  filter(Discharge_CFS < 250) %>%
  ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
  geom_point() +
  geom_hline(yintercept=126, linetype="dashed", 
                color = "blue", size=0.2) +
  geom_hline(yintercept=235, linetype="dashed", 
                color = "red", size=0.2)+
  geom_hline(yintercept=575, linetype="dashed", 
                color = "dark green", size=0.2)
```
Now I want to see what it looks like without the outlying discharge values.
None of these graphs appear to have a random distribution, but it is muddied.
```{r}
data_Merge2 %>%
  filter(Discharge_CFS < 150) %>%
  ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
  geom_point() +
  geom_hline(yintercept=126, linetype="dashed", 
                color = "blue", size=0.2) +
  geom_hline(yintercept=235, linetype="dashed", 
                color = "red", size=0.2)+
  geom_hline(yintercept=575, linetype="dashed", 
                color = "dark green", size=0.2)
```



Let's add some air temperature up in here, up in here.
```{r}
# Read in file 
Rawclimate <- "Data/Raw/ClimateAnalyzer_DailySummaryStats_Raw.csv"
DailyClimate <- read.csv(Rawclimate, skip = 3)
DailyClimate <- head(DailyClimate, - 1)

# Variable no longer needed.
rm(Rawclimate)
```

What about the sum of previous three-days precipitation
```{r}
# Use lag to include only previous data so that forecasting may occur on a more timely basis.Min temp is not published until the day after.
DailyClimate$Previous3Precip <-  lag(rollsumr(DailyClimate$Precipitation..in., k = 3, fill = NA))
DailyClimate$Previous7Precip <-  lag(rollsumr(DailyClimate$Precipitation..in., k = 7, fill = NA))
DailyClimate$Previous30Precip <-  lag(rollsumr(DailyClimate$Precipitation..in., k = 30, fill = NA))

```

Lag Tmax an Tmin
```{r}
# Use the previous day's Tmin due to data posting interval. This may be used for future forecasting.
DailyClimate$PreviousTmin <- lag(DailyClimate$Tmin..F.)
# Use the previous day's Tmin due to data posting interval. This may be used for future forecasting.
DailyClimate$PreviousTmax <- lag(DailyClimate$Tmax..F.)

```

Fix the date.
```{r}
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-10,-1)
DailyClimate$DateAsDate <-as.POSIXct(DailyClimate$DateAsDate, format = "%m/%d/%Y", tz = tz)
```

Merge together with other data

As of 9/13/2022 we have 113 more missing Tmax datapoints. I could possibly manipulate this file to clean nans but I'm not sure how comfortable I feel about this. 
LAURA check in on this with expert.
```{r}
data_Merge2$DateAsDate <-as.POSIXct(data_Merge2$DateAsDate, format = "%Y-%m-%d", tz = tz)


data_Merge2<- left_join(data_Merge2, 
                          DailyClimate, by = c("DateAsDate"))


data_Merge2 %>%
  filter(is.nan(PreviousTmax))

```

Graph again
This isn't as great as I thought it would be. 
Google says temps of 39-113°F are survivable for e. coli and ideal is 98.6°F. THe majority of the samples are survivable.
```{r}
data_Merge2 %>%
  filter(!is.nan(PreviousTmax)) %>%
  ggplot(aes(x=PreviousTmax, y=StandardizedResult)) +
  geom_point() +
  geom_hline(yintercept=126, linetype="dashed", 
                color = "blue", size=0.2) +
  geom_hline(yintercept=235, linetype="dashed", 
                color = "red", size=0.2)+
  geom_hline(yintercept=575, linetype="dashed", 
                color = "dark green", size=0.2)
```
Graph TMin.
It is of note that the max only happens above freezing. This may actually be more meaningful than Tmax. The minimum survivable temperature likely has a significant impact on the buggies.
```{r}

data_Merge2 %>%
  filter(!is.nan(PreviousTmin)) %>%
  ggplot(aes(x=PreviousTmin, y=StandardizedResult)) +
  geom_point() +
  geom_hline(yintercept=126, linetype="dashed", 
                color = "blue", size=0.2) +
  geom_hline(yintercept=235, linetype="dashed", 
                color = "red", size=0.2)+
  geom_hline(yintercept=575, linetype="dashed", 
                color = "dark green", size=0.2)
```

There is probably a better source for precipication, but I'd like to take a look at it. It may be correlated to flow. I'll have to check this later.
```{r}
data_Merge2 %>%
  filter(!is.nan(Precipitation..in.)) %>%
  ggplot(aes(x=Precipitation..in., y=StandardizedResult)) +
  geom_point() +
  geom_hline(yintercept=126, linetype="dashed", 
                color = "blue", size=0.2) +
  geom_hline(yintercept=235, linetype="dashed", 
                color = "red", size=0.2)+
  geom_hline(yintercept=575, linetype="dashed", 
                color = "dark green", size=0.2)
```
Now I want to look at season. According to the NPS protocol the seasons in southern Arizona are:
Fall (October 1 - December 30)
Winter (January 1 - March 30)
Spring (April 1 - June 30)
Monsoon (July 1 - Sept 30)

```{r}
data_Merge2$Season1 <- as.numeric(str_sub(data_Merge2$DateAsDate,6,7))
  
  
data_Merge2$Season <- ifelse(data_Merge2$Season1 > 9 & data_Merge2$Season1 < 13, "Fall", 
                                           ifelse (data_Merge2$Season1 > 0 & data_Merge2$Season1 < 4, "Winter",
                                                   ifelse (data_Merge2$Season1 > 3 & data_Merge2$Season1 < 7, 
                                    "Spring", "Monsoon")))
```

I would also like to include a variable to el nino or la nina years, as per a discussion with NPS regional hydrologist, Salek Shafiqullah. 
```{r}
# Bring in the data
Nin <- read.csv("Data/Raw/Nino_Nina_Raw.csv")

# Pivot - begin creating a date column by converting to long format
Nin <- Nin %>% pivot_longer(cols=c('DJF', 'JFM','FMA', 'MAM', 'AMJ', 'MJJ', 'JJA', 'JAS', 'ASO', 'SON', 'OND', 'NDJ'),
                    names_to='Month',
                    values_to='NinIndex')

# Use the central month
Nin$Month <- c("DJF" = "01", "JFM" = "02", "FMA" = "03", "MAM" = "04",'AMJ'= '05', 'MJJ'='06', 'JJA'='07', 'JAS'='08', 'ASO'='09', 'SON'='10', 'OND'='11', 'NDJ'='12')[Nin$Month]

# Create a date column to represent the first date
Nin$Day = "01"

# Convert all this to numeric
Nin$Date <- as.POSIXct(format(paste(Nin$Year, Nin$Month, Nin$Day, sep = "-"), format = '%Y-%m-%d %H:%M'), tz = tz)

# Remove the columns with no data so that it is properly attributed.
Nin <- head(Nin, -4)

#[https://stackoverflow.com/questions/50167509/converting-monthly-data-to-daily-in-r]
# Create an xts in order to attribute each data point on the first day of the month to ALL days of the month
NinXTS <- xts(Nin$NinIndex,order.by = Nin$Date)

# Lengthen the dataset to represent all dates
Nin <- na.locf(merge(NinXTS, foo=zoo(NA, order.by=seq(start(NinXTS), end(NinXTS),
  "day",drop=F)))[, 1])

Nin <- data.frame(DateAsDate=index(Nin), coredata(Nin))
```

Merge this with e coli dataset
```{r}
data_Merge2 <- left_join(data_Merge2, 
                          Nin, by = c("DateAsDate"))
```

Create an attribute column for time of day. This will get at the daily temp variance since the max/min attributes are daily values rather than 15-min or hourly as per discussion with SOAR scientist Kara Raymond.
```{r}
#modified from https://stackoverflow.com/questions/49370387/convert-time-object-to-categorical-morning-afternoon-evening-night-variable

# Create  vector of times
timep <- as.POSIXct(data_Merge2$ActivityStartTime.Time, format = "%H:%M") %>% format("%H:%M:%S")


cut(chron::times(timep) , breaks = (1/24) * c(0,5,11,16,19,24), 
    labels = c(4, 1, 2, 3, 4))

data_Merge2$TOD <- cut(chron::times(timep) , breaks = (1/24) * c(0,5,11,16,19,24))
data_Merge2$TOD <- c(4, 1, 2, 3, 4)[as.numeric(data_Merge2$TOD)]

```

#The columns are getting messy. Remove duplicates and nulls
```{r}
# Remove extraneous columns and reorder

# I had previously kept hydrologic condition and event in, but there are so many nulls I don't think it will be worth it. LAURA may be able to attribute the condition (rising or falling) based on the staff gage. Will need to speak to hydrologist.

# Columns are organized so that the "metadata" columns are in the beginning. These will not be used for correlation views.
data_Merge3 <- data_Merge2 %>%
  select(ActivityConductingOrganizationText, MonitoringLocationIdentifier, QuantificationLimit, DailyFlowCount, DateTime2, StandardizedResult, Discharge_CFS, USGS_Staff, QuantificationLimit, Precipitation..in., PreviousTmax, PreviousTmin, Previous3Precip, Previous7Precip, Previous30Precip, Season, NinXTS, TOD, Stage, DistFromSonoita)

# Remove old data.
rm(data_Merge, data_Merge2, DailyClimate, Nin, NinXTS)
```


```{r}
data_Merge3 %>%
  ggplot(aes(x=Season, y=StandardizedResult)) +
  geom_point() +
  geom_hline(yintercept=126, linetype="dashed", 
                color = "blue", size=0.2) +
  geom_hline(yintercept=235, linetype="dashed", 
                color = "red", size=0.2)+
  geom_hline(yintercept=575, linetype="dashed", 
                color = "dark green", size=0.2)
```
#Notes on above diagram: Monsoon values seem significantly above blue line. Winter values have many less maximum values.



#graph this too.
```{r}
# Missing values are expected.
data_Merge3 %>%
  ggplot() +
  geom_point(aes(x=Previous3Precip, y=StandardizedResult), color = "red") +
  geom_point(aes(x=Previous7Precip, y=StandardizedResult), color = "blue")+
  geom_point(aes(x=Previous30Precip, y=StandardizedResult), color = "green")+
  geom_hline(yintercept=126, linetype="dashed", 
                color = "blue", size=0.2) +
  geom_hline(yintercept=235, linetype="dashed", 
                color = "red", size=0.2)+
  geom_hline(yintercept=575, linetype="dashed", 
                color = "dark green", size=0.2)
```

I should change the categorical variables back to numeric for corrplot. 

Numerical Categorical swap
```{r}
data_Merge3$SeasonNumeric <- ifelse(data_Merge3$Season == "Fall", 1,
          ifelse(data_Merge3$Season == "Winter", 2,
          ifelse (data_Merge3$Season =="Spring", 3,4)))
```


I want to change the organization to numeric as well.

I'd like to look at organization to see if this has any noteworthy effect on the results. If it appears significant, this could be due to the time of testing (project funding is more likely when there is an issue) or due to the personell collecting the data. A strong correlation will indicate either of these issues.
```{r}
data_Merge3$ACO_Numeric <- ifelse(data_Merge3$ActivityConductingOrganizationText == "U.S. National Park Service", 1,
   ifelse(data_Merge3$ActivityConductingOrganizationText == "NATIONAL PARK SERVICE", 1,
   ifelse(data_Merge3$ActivityConductingOrganizationText == "Friends of the Santa Cruz River, Tubac, AZ", 2, 
   ifelse(data_Merge3$ActivityConductingOrganizationText =="FRIENDS OF THE SANTA CRUZ RIVER", 2,
   ifelse(data_Merge3$ActivityConductingOrganizationText == "U.S. Geological Survey-Water Resources Discipline", 3, 
   ifelse(data_Merge3$ActivityConductingOrganizationText == "ARIZONA DEPT OF ENVIRONMENTAL QUALITY", 4,  
   ifelse(data_Merge3$ActivityConductingOrganizationText == "INTERNATIONAL BOUNDARY AND WATER COMMISSION", 5,  
   ifelse(data_Merge3$ActivityConductingOrganizationText == "UNIVERSITY OF ARIZONA - MARICOPA AGRICULTURAL CENTER", 6, 
   ifelse(data_Merge3$ActivityConductingOrganizationText == "HARRIS ENVIRONMENTAL", 7,
   ifelse(data_Merge3$ActivityConductingOrganizationText == "NPS", 8,  # This is SODN data, Separate from other NPS 
   ifelse(data_Merge3$ActivityConductingOrganizationText == "VOLUNTEER GROUPS", 9,        
          NaN)))))))))))
```


## Begin looking at the correlation

I want to look at nulls from the climate data. I think this will have an effect on my corrplot. With removed rows, I still have ~900 to play with.
```{r}
data_Merge3 <- na.omit(data_Merge3)
```

Create the classifiers, 126 (FBC, geometric mean, minimum of four samples in 30 days), 235 (FBC, single sample ), 575 (PBC) according to EPA regulations for the state of Arizona. 

```{r}
data_Merge3$ecoli_235 <- ifelse(data_Merge3$StandardizedResult > 234.5,1,0)
data_Merge3$ecoli_575 <- ifelse(data_Merge3$StandardizedResult > 574.5,1,0)
```


Determine how balanced the dataset is.
```{r}
# Count total rows
totaln <- data_Merge3 %>%
  summarise(n = n())

# Total positive at 235 is currently (10/22) 51.6%
data_Merge3 %>%
  filter(ecoli_235 ==1) %>% 
  summarise(n = n())/totaln

# Total positives at 575 is currently (10/22) 42.5%
data_Merge3 %>%
  filter(ecoli_575 ==1) %>% 
  summarise(n = n())/totaln

```

## Data Exploration 2.0

Now that I've looked at the overall data, make a test and train set to proceed.
```{r}
# De-randomize (for now, at least). 
set.seed(24)

#subset the data by 70%
subset <- sample(nrow(data_Merge3),nrow(data_Merge3) * 0.7)

#create training and test data from subset
traindata <- data_Merge3[subset, ]
testdata <- data_Merge3[-subset, ]

```

Pairs examination
Use the *Training* data to train the models. Test data comes in for validation procedures.
```{r}
# Ensure that only numeric data is contained. There was a previous cleanup but it still contained metadata.
traindata_numeric <- traindata %>%
  select(c("StandardizedResult", "ecoli_235","ecoli_575", "Discharge_CFS", "USGS_Staff", "Precipitation..in.", "PreviousTmax", "PreviousTmin", "Previous3Precip", "Previous7Precip", "Previous30Precip", "NinXTS", "TOD", "Stage", "SeasonNumeric", "ACO_Numeric", "DistFromSonoita"))

# I can look at pairs
# Now that I've added more variables this isn't exactly useful.
pairs(traindata_numeric)

```
Correlation Matrix
```{r}
# Create correlation matrix
cor <- cor(traindata_numeric)

corrplot(cor, method = 'square', order = 'FPC', type = 'upper', diag = FALSE, addCoef.col="black", number.cex=0.75)
```

```{r}
# looking at only ecoli_575
cor2 <- cor(traindata %>%
  select(c("ecoli_575", "Previous30Precip", "PreviousTmin", "Discharge_CFS", "Stage","NinXTS", "TOD","ACO_Numeric", "DistFromSonoita")))

corrplot(cor2, method = 'square', order = 'FPC', type = 'upper', diag = FALSE, addCoef.col="black", number.cex=0.75)
```
```{r}
# looking at only ecoli_235
cor3<- cor(traindata %>%
  select(c("ecoli_235", "Previous30Precip", "PreviousTmin", "Discharge_CFS", "Stage","NinXTS", "TOD","ACO_Numeric", "DistFromSonoita")))

corrplot(cor3, method = 'square', order = 'FPC', type = 'upper', diag = FALSE, addCoef.col="black", number.cex=0.75)
```
```{r}
# This looks at both classifications together. Repeat of previous but with limited predictors.
cor4<- cor(traindata %>%
  select(c("ecoli_235", "ecoli_575", "Previous30Precip", "PreviousTmin", "Discharge_CFS", "Stage","NinXTS", "TOD","ACO_Numeric", "DistFromSonoita")))

corrplot(cor4, method = 'square', order = 'FPC', type = 'upper', diag = FALSE, addCoef.col="black", number.cex=0.75)
```

It is important to ensure that I do not include correlated predictors in this model. If I choose a precipitation variable, it should only be one. Same for temperature and discharge/staff. 

Season and temperature are also extremely strongly correlated. It may make sense to use only one variable. 

# Export data for analysis in python.
```{r}
write.csv(data_Merge3 %>%
            select("Previous30Precip", "Discharge_CFS", "Stage","NinXTS", "TOD", "DistFromSonoita","ecoli_235", "ecoli_575" ), "Data/Processed/ecoli_attributed2.csv")
```

