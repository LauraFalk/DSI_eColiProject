---
title: "USGS_File_Processing"
author: "Laura Palacios"
date: '2022-08-29'
output: html_document
---

```{r setup, include=FALSE}
library(data.table)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
```

#Processing the USGS File

I have commented out the data pull for USGS data. It is many years and takes lots of time to grab all 300k records when running this script. To save time, I've outputted the result of: 
readNWISuv(site_id,c('00060','00065'), "2009-07-01","2022-08-30", "America/Phoenix")
as USGS_09481740.CSV and saved this within the raw data folder.

Original code may be un-commented and re-run if desired. Data from 2022 is currently provisional and is still subject to change during USGS QAQC procedures.
```{r}
#Inputs formatted for read NWIS function.
site_id <- '09481740'
startDate = "2009-07-01"
endDate = "2022-08-30"
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)

#creates table in R
USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")

#This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
```


2015 had some times that were off by a few minutes from the normal 15- minute interval. This will ensure that everything is standardized. Values are rounded to the nearest 15-minutes.
```{r}
USGS_Standardized <- USGS_Raw
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)

```

Something is wrong here. Why are there duplicate datetimes
```{r}
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
```

Oh, because I rounded.
```{r}
# remove the pre-rounding dateTime column
USGS_Standardized_RmDupls <- USGS_Standardized %>%
  subset(select = -c(dateTime)) 

# This shows any duplicate dates. There are a few because the data was not collected on the standard 15-minute interval. The separate values were rounded to the same one.
USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)

# This summarizes the flow column by the rounded DateTime
USGS_Standardized_RmDupls <- USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))

# Merge the rounded data back in and rename the column name to something more useful.
USGS_RmDupl<- merge(x=USGS_Standardized,y=USGS_Standardized_RmDupls, by="DateTime2",all.x=TRUE, all.y = FALSE)
setnames(USGS_RmDupl,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))

#Remove the pre-averaging flow column
USGS_RmDupl <- USGS_RmDupl %>%
  subset(select = -c(X_00060_00000))

# Removing duplicates from the fixed times and fixed cfs
USGS_RmDupl<- USGS_RmDupl %>% 
 distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)

#Re-check
USGS_RmDupl %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)

```

There are missing timestamps. This will create a full time series to include no data indicators.
```{r}
# Create the time series, formatted as posixt
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))
colnames(TimeSeries)[1] = "DateTime"
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime, format = '%Y-%m-%d %H:%M'), tz = tz)

#Join data with time series, sort by DateTime2
# could also use left_join here. Issues were from duplicate values from the rounding merge.
USGS_TimeSeries <- merge(x=TimeSeries,y=USGS_RmDupl,by="DateTime2",all.x=TRUE, all.y = FALSE)

```


For data exploration: this tests to see how many nulls are present in the current dataset.
note 9/5/2022: 40544
```{r}
count(USGS_TimeSeries %>%
 filter(is.na(Discharge_CFS)))
```

I think I should be able to fill some of the nulls by taking the average of the two surrounding values. How many does this account for?

```{r}
USGS_TimeSeries$date <- as_date(USGS_TimeSeries$DateTime2)
USGS_TimeSeries$count <- 1

TimeFlag <- USGS_TimeSeries %>%
  filter(!is.na(Discharge_CFS)) %>%
  group_by(date) %>%
  summarize(value = sum(count))

USGS_TimeSeries <- right_join(TimeFlag, USGS_TimeSeries)
setnames(USGS_TimeSeries, "value", "CountUSGSbyDate")

```
Start to fill NAs where applicable. This will fill in all data into a new column.
```{r}
USGS_TimeSeries_Fill <-
  USGS_TimeSeries %>%
  filter(CountUSGSbyDate>88)

USGS_TimeSeries_Fill$Filled <-  na.approx(USGS_TimeSeries_Fill$Discharge_CFS, rule = 2)

USGS_ForAnalysis <- right_join(USGS_TimeSeries_Fill,USGS_TimeSeries)

```
Check how many nulls there are now that some of the data is filled. I'm not comfortable auto-filling (the majority) of the remaining values.
```{r}
USGS_ForAnalysis2 <- USGS_ForAnalysis

USGS_ForAnalysis2$Filled<- ifelse(is.na(USGS_ForAnalysis$Discharge_CFS) == TRUE,USGS_ForAnalysis$Filled,USGS_ForAnalysis$Discharge_CFS)

count(USGS_ForAnalysis2 %>%
 filter(is.na(Filled)))

```

```{r}
USGS_ForAnalysis2 <-  USGS_ForAnalysis2 %>%
  subset(select = -c(date, count, Discharge_CFS))

setnames(USGS_ForAnalysis2,old=c("Filled"),new=c("Discharge_CFS"))
```



Remove prior files so that I don't get confused.
```{r}
remove(TimeFlag, TimeSeries, USGS_TimeSeries, USGS_Raw, USGS_Standardized, USGS_TimeSeries_Fill, USGS_ForAnalysis, USGS_RmDupl, USGS_Standardized_RmDupls)
```

```{r}
#write.csv(USGS_ForAnalysis2, "Data/Processed/USGS_ForAnalysis20220905.csv")
```

