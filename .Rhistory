<<<<<<< HEAD
geom_point(aes(x=Previous3Precip, y=StandardizedResult), color = "red") +
geom_point(aes(x=Previous7Precip, y=StandardizedResult), color = "blue")+
=======
View(test)
test <- USGS_ForAnalysis2 %>%
filter(DateTime2 <= as.POSIXct('2020-04-27'))
test <- USGS_ForAnalysis2 %>%
filter(DateTime2 < as.POSIXct('2020-04-28'))
Storet_USGS_Merge_NoNull <- Storet_USGS_Merge %>%
filter(!is.na(Discharge_CFS))
Storet_USGS_Merge_NoNull %<%
ggplot(mtcars, aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point()
Storet_USGS_Merge_NoNull %>%
ggplot(mtcars, aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point()
Storet_USGS_Merge_NoNull %>%
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point()
Storet_USGS_Merge_NoNull %>%
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point()+
geom_hline(yintercept=126, linetype="dashed",
color = "red", size=2)
Storet_USGS_Merge_NoNull %>%
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point()+
geom_hline(yintercept=126, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Merge_NoNull %>%
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point()+
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2)+
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Merge_NoNull %>%
filter(Discharge_CFS < 250)
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point()+
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2)+
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Merge_NoNull %>%
filter(Discharge_CFS < 250) +
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point() +
>>>>>>> 6c26e0b58185896c2b5e9c1bc94928625203d33e
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
<<<<<<< HEAD
Storet_USGS_Climate_Merge$Previous3Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 3, fill = NA)
Storet_USGS_Climate_Merge$Previous30Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 3, fill = NA)
Storet_USGS_Climate_Merge %>%
ggplot() +
geom_point(aes(x=Previous3Precip, y=StandardizedResult), color = "red") +
geom_point(aes(x=Previous30Precip, y=StandardizedResult), color = "blue")+
=======
Storet_USGS_Merge_NoNull %>%
filter(Discharge_CFS < 250) %>%
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point() +
>>>>>>> 6c26e0b58185896c2b5e9c1bc94928625203d33e
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
<<<<<<< HEAD
View(Storet_USGS_Climate_Merge)
Storet_USGS_Climate_Merge$Previous3Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 3, fill = NA)
Storet_USGS_Climate_Merge$Previous7Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 7, fill = NA)
Storet_USGS_Climate_Merge %>%
ggplot() +
geom_point(aes(x=Previous3Precip, y=StandardizedResult), color = "red") +
geom_point(aes(x=Previous7Precip, y=StandardizedResult), color = "blue")+
=======
Storet_USGS_Merge_NoNull %>%
filter(Discharge_CFS < 150) %>%
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point() +
>>>>>>> 6c26e0b58185896c2b5e9c1bc94928625203d33e
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
<<<<<<< HEAD
Storet_USGS_Climate_Merge$Previous3Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 3, fill = NA)
Storet_USGS_Climate_Merge$Previous7Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 7, fill = NA)
Storet_USGS_Climate_Merge$Previous30Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 30, fill = NA)
Storet_USGS_Climate_Merge %>%
ggplot() +
geom_point(aes(x=Previous3Precip, y=StandardizedResult), color = "red") +
geom_point(aes(x=Previous7Precip, y=StandardizedResult), color = "blue")+
geom_point(aes(x=Previous30Precip, y=StandardizedResult), color = "green")+
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge$SeasonNumeric <- ifelse(Storet_USGS_Climate_Merge$Season == "Fall", 1,
ifelse(Storet_USGS_Climate_Merge$Season == "Winter", 2,
ifelse (Storet_USGS_Climate_Merge$Season =="Spring", 3,4)))
View(Storet_USGS_Climate_Merge)
library(corrplot)
ForCorrplot <- Storet_USGS_Climate_Merge %>%
select(StandardizedResult, Disharge_CFS)
ForCorrplot <- Storet_USGS_Climate_Merge %>%
select(StandardizedResult, Discharge_CFS, Precipitation..in., Tmax..F., Tmin..F., PreviousPrecip, Previous3Precip, Previous7Precip, Previous30Precip, SeasonNumeric)
corrplot(ForCorrplot)
cor(ForCorrplot)
corrplot(R1)
R1 <- cor(ForCorrplot)
corrplot(R1)
View(R1)
R1 <- cor(as.numeric(ForCorrplot))
sapply(ForCorrplot, class)
View(ForCorrplot)
Storet_USGS_Climate_Merge$Previous3Precip <- ifelse(is.NAN(Storet_USGS_Climate_Merge$Previous3Precip) | is.na(Storet_USGS_Climate_Merge$Previous3Precip), 0, Storet_USGS_Climate_Merge$Previous3Precip)
Storet_USGS_Climate_Merge$Previous3Precip <- ifelse(is.nan(Storet_USGS_Climate_Merge$Previous3Precip) | is.na(Storet_USGS_Climate_Merge$Previous3Precip), 0, Storet_USGS_Climate_Merge$Previous3Precip)
# Fix the nulls in precip data.
Storet_USGS_Climate_Merge$Previous3Precip[is.na(Storet_USGS_Climate_Merge$Previous3Precip)] <- 0
Storet_USGS_Climate_Merge$Previous3Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 3, fill = NA)
Storet_USGS_Climate_Merge$Previous7Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 7, fill = NA)
Storet_USGS_Climate_Merge$Previous30Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 30, fill = NA)
Storet_USGS_Climate_Merge$PreviousPrecip[is.na(Storet_USGS_Climate_Merge$PreviousPrecip[1:10])]
Storet_USGS_Climate_Merge$PreviousPrecip[is.na(Storet_USGS_Climate_Merge$PreviousPrecip[1:10])] <- 0
Storet_USGS_Climate_Merge$Previous3Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 3, fill = NA)
Storet_USGS_Climate_Merge$Previous7Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 7, fill = NA)
Storet_USGS_Climate_Merge$Previous30Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 30, fill = NA)
View(DailyClimate)
DailyClimate$Previous3Precip <-  rollsumr(DailyClimate$Precipitation..in., k = 3, fill = NA)
DailyClimate$Previous7Precip <-  rollsumr(DailyClimate$Precipitation..in., k = 7, fill = NA)
DailyClimate$Previous30Precip <-  rollsumr(DailyClimate$Precipitation..in., k = 30, fill = NA)
library(data.table)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
#Inputs formatted for read NWIS function.
site_id <- '09481740'
startDate = "2009-07-01"
endDate = "2022-08-30"
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
#creates table in R
USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
library(data.table)
=======
read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats")
read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv")
read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv")
>>>>>>> 6c26e0b58185896c2b5e9c1bc94928625203d33e
library(knitr)
library(lubridate)
library(tidyverse)
library(waterData)
library(zoo)
opts_chunk$set(echo = TRUE)
<<<<<<< HEAD
#Inputs formatted for read NWIS function.
site_id <- '09481740'
startDate = "2009-07-01"
endDate = "2022-08-30"
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
#creates table in R
USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
library(data.table)
library(dataRetrieval)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
#Inputs formatted for read NWIS function.
site_id <- '09481740'
startDate = "2009-07-01"
endDate = "2022-08-30"
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
#creates table in R
USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
#This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
View(USGS_Raw)
# Create a New Dataframe to edit
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# remove the pre-rounding dateTime column, as this will affect the duplicate record determination
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime))
# This shows any duplicate dates. There are a few because the data was not collected on the standard 15-minute interval. The separate values were rounded to the same one.
USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime
USGS_Standardized_RmDupls <- USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
View(USGS_Standardized_RmDupls)
# This summarizes the flow column by the rounded DateTime.
USGS_Standardized_append <- USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# This summarizes the flow column by the rounded DateTime.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
View(USGS_Standardized_append)
USGS_Standardized_RmDupls <- USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(stddev(X_00060_00000))
# This summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
View(USGS_Standardized_append)
View(USGS_Standardized_append)
# Rename the column to something more useful.
setnames(USGS_Standardized_append,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_RmDupls, by="DateTime2",all.x=TRUE, all.y = FALSE)
View(USGS_Standardized)
#Remove the pre-averaging flow column
USGS_Standardized <- USGS_Standardized %>%
subset(select = -c(X_00060_00000))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_RmDupls, by="DateTime2",all.x=TRUE, all.y = FALSE)
library(data.table)
library(dataRetrieval)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
#tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
# This summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Rename the column to something more useful.
setnames(USGS_Standardized_append,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_RmDupls, by="DateTime2",all.x=TRUE, all.y = FALSE)
# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_append, by="DateTime2",all.x=TRUE, all.y = FALSE)
#Remove the pre-averaging flow column
USGS_Standardized <- USGS_Standardized %>%
subset(select = -c(X_00060_00000))
#Remove the pre-averaging flow column
USGS_Standardized <- USGS_Standardized %>%
subset(select = -c(X_00060_00000)) %>%
distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)
# This summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
library(data.table)
library(dataRetrieval)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
#tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Rename the column to something more useful.
setnames(USGS_Standardized_append,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_append, by="DateTime2",all.x=TRUE, all.y = FALSE)
#Remove the pre-averaging flow column
USGS_Standardized <- USGS_Standardized %>%
subset(select = -c(X_00060_00000)) %>%
distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)
#Re-check
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# Remove append table - data has been merged
remove(USGS_Standardized_append)
# Create the time series, formatted as POSIXct
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))
# Rename the Column DateTime2 to indicate properly formatted column.
colnames(TimeSeries)[1] = "DateTime2"
# Follow through on the name and format this bad boy.
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
#Left join data with time series, sort by DateTime2
USGS_Standardized <- merge(x=TimeSeries,y=USGS_Standardized,by="DateTime2",all.x=TRUE, all.y = FALSE)
View(USGS_Standardized)
count(USGS_Standardized %>%
filter(is.na(Discharge_CFS)))
# This column is a dummy column which allows me to count more easily in the next block.
USGS_Standardized$date <- as_date(USGS_Standardized$DateTime2)
USGS_Standardized$count <- 1
# This counts the number of readings per day
TimeFlag <- USGS_Standardized %>%
filter(!is.na(Discharge_CFS)) %>%
group_by(date) %>%
summarize(value = sum(count))
# Append the reading count value to the standardized data
USGS_Standardized <- right_join(TimeFlag, USGS_Standardized)
setnames(USGS_Standardized, "dummy", "CountUSGSbyDate")
# Name this something meaningful
setnames(USGS_Standardized, "value", "dummyValue")
View(USGS_Standardized)
# Remove the created time series. Data has been merged
remove(TimeSeries)
View(TimeFlag)
# This counts the number of readings per day
TimeFlag <- USGS_Standardized %>%
filter(!is.na(Discharge_CFS)) %>%
group_by(date) %>%
summarize(value = sum(count))
View(TimeFlag)
library(data.table)
library(dataRetrieval)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Rename the column to something more useful.
setnames(USGS_Standardized_append,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_append, by="DateTime2",all.x=TRUE, all.y = FALSE)
#Remove the pre-averaging flow column. This allows to remove duplicates from the append pairs created above.
USGS_Standardized <- USGS_Standardized %>%
subset(select = -c(X_00060_00000)) %>%
distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)
# Re-check for duplicate values. This should equal zero.
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# Remove append table - data has been merged
remove(USGS_Standardized_append)
# Create the time series, formatted as POSIXct
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))
# Rename the Column DateTime2 to indicate properly formatted column.
colnames(TimeSeries)[1] = "DateTime2"
# Follow through on the name and format this bad boy.
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
#Left join data with time series, sort by DateTime2
USGS_Standardized <- merge(x=TimeSeries,y=USGS_Standardized,by="DateTime2",all.x=TRUE, all.y = FALSE)
# Remove the created time series. Data has been merged
remove(TimeSeries)
# This tests to see how many nulls are present in the current dataset.
# 9/5/2022: 40544 records are null
count(USGS_Standardized %>%
filter(is.na(Discharge_CFS)))
View(USGS_Standardized)
USGS_Standardized$date <- as_date(USGS_Standardized$DateTime2)
USGS_Standardized$count <- 1
# This counts the number of readings per day
TimeFlag <- USGS_Standardized %>%
filter(!is.na(Discharge_CFS)) %>%
group_by(date) %>%
summarize(value = sum(count))
# Append the reading count value to the standardized data
USGS_Standardized <- right_join(TimeFlag, USGS_Standardized)
View(USGS_Standardized)
View(TimeFlag)
View(TimeFlag)
View(TimeFlag)
View(USGS_Standardized)
# Name this something meaningful
setnames(USGS_Standardized, "value", "DailyFlowCount")
# Name this something meaningful
setnames(USGS_Standardized, "value", "DailyFlowCount")
View(USGS_Standardized)
# Name this something meaningful
setnames(USGS_Standardized, c("value", "X_00060_00000_cd"), c("DailyFlowCount", "Discharge_Rank"), skip_absent = TRUE))
# Name this something meaningful
setnames(USGS_Standardized, c("value", "X_00060_00000_cd"), c("DailyFlowCount", "Discharge_Rank"), skip_absent = TRUE)
# Name this something meaningful
setnames(USGS_Standardized, c("value", "X_00060_00000_cd", "X_00065_00000", "X_00065_00000_cd"), c("DailyFlowCount", "Discharge_cd", "USGS_Staff", "USGS_Staff_cd"), skip_absent = TRUE)
# Remove extraneous columns
USGS_Standardized <- USGS_Standardized %>%
select(-X, -agency_cd, -site_no, -count)
# Remove extraneous columns
USGS_Standardized <- USGS_Standardized %>%
select(-X, -agency_cd, -site_no, -count, -dateTime)
=======
DailyClimate <- read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv")
DailyClimate$DateAsDate <- as.POSIXct(format(DailyClimate$Date, format = '%Y-%m-%d'), tz = tz)
DailyClimate$DateAsDate <- as.POSIXct(format(DailyClimate$Date, format = '%d/%m/%Y'), tz = tz)
View(DailyClimate)
DailyClimate$DateAsDate <- as.POSIXct(format(DailyClimate$Date, format = '%Y-%m-%d'), tz = tz)
DailyClimate <- read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv", skip = 6)
View(DailyClimate)
DailyClimate <- read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv", skip = 3)
View(DailyClimate)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-6,-1)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-8,-1)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-9,-1)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-10,-1)
as.POSIXlt(DailyClimate$DateAsDate, tz = tz)
DailyClimate <- head(DailyClimate, - 1)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-10,-1)
as.POSIXlt(DailyClimate$DateAsDate, tz = tz)
DailyClimate$DateAsDate <-as.POSIXct(DailyClimate$DateAsDate, format = "%mm/%dd/%YYYY", tz = tz)
Storet_USGS_Climate_Merge<- left_join(Storet_USGS_Merge_NoNull,
DailyClimate, by = c("DateAsDate"))
Storet_USGS_Merge_NoNull$DateAsDate <-as.POSIXct(Storet_USGS_Merge_NoNull$DateAsDate, format = "%Y-%m-%d", tz = tz)
Storet_USGS_Climate_Merge<- left_join(Storet_USGS_Merge_NoNull,
DailyClimate, by = c("DateAsDate"))
Storet_USGS_Climate_Merge %>%
filter(!is.na(DailyClimate))
Storet_USGS_Climate_Merge %>%
filter(!is.na(Tmax..F.))
Storet_USGS_Climate_Merge %>%
filter(!is.null(Tmax..F.))
DailyClimate$DateAsDate <-as.POSIXct(DailyClimate$DateAsDate, format = "%m/%d/%Y", tz = tz)
DailyClimate <- read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv", skip = 3)
DailyClimate <- head(DailyClimate, - 1)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-10,-1)
DailyClimate$DateAsDate <-as.POSIXct(DailyClimate$DateAsDate, format = "%m/%d/%Y", tz = tz)
Storet_USGS_Merge_NoNull$DateAsDate <-as.POSIXct(Storet_USGS_Merge_NoNull$DateAsDate, format = "%Y-%m-%d", tz = tz)
Storet_USGS_Climate_Merge<- left_join(Storet_USGS_Merge_NoNull,
DailyClimate, by = c("DateAsDate"))
Storet_USGS_Climate_Merge %>%
filter(!is.null(Tmax..F.))
Storet_USGS_Climate_Merge %>%
filter(is.null(Tmax..F.))
View(Storet_USGS_Climate_Merge)
Storet_USGS_Climate_Merge %>%
filter(Tmax..F. == NaN)
Storet_USGS_Climate_Merge %>%
filter(is.nan(Tmax..F.))
Storet_USGS_Climate_Merge %>%
filter(!is.nan(Tmax..F.)) %>%
ggplot(aes(x=Tmax..F., y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge %>%
filter(!is.nan(Tmax..F.)) %>%
ggplot(aes(x=Tmax..F., y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge %>%
filter(!is.nan(Tmin..F.)) %>%
ggplot(aes(x=Tmin..F., y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge %>%
filter(!is.nan(Precipitation..in.)) %>%
ggplot(aes(x=Precipitation..in., y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge$Season <- as.yearmon(DateAsDate)
Storet_USGS_Climate_Merge$Season <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
View(Storet_USGS_Climate_Merge)
Storet_USGS_Climate_Merge$Season <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season %in% c('May','June','July'), "SUMMER",
ifelse (Storet_USGS_Climate_Merge$Season %in% c('August','September','October'), "AUTUMN",
ifelse (Storet_USGS_Climate_Merge$Season %in% c('November','December','January'),
"WINTER", "SPRING")))
Storet_USGS_Climate_Merge$Season <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season %in% c('October','November','December'), "Fall",
ifelse (Storet_USGS_Climate_Merge$Season %in% c('January','February','March'), "Winter",
ifelse (Storet_USGS_Climate_Merge$Season %in% c('April','May','June'),
"Spring", "Monsoon")))
Storet_USGS_Climate_Merge %>%
ggplot(aes(x=Season, y=StandardizedResult)) +
geom_bar() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge %>%
ggplot(aes(x=Season, y=StandardizedResult)) +
geom_bar(stat='identity') +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge$Season1 <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 %in% c('October','November','December'), "Fall",
ifelse (Storet_USGS_Climate_Merge$Season1 %in% c('January','February','March'), "Winter",
ifelse (Storet_USGS_Climate_Merge$Season1 %in% c('April','May','June'),
"Spring", "Monsoon")))
Storet_USGS_Climate_Merge$Season1 <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 %in% c('Oct','Nov','Dec'), "Fall",
ifelse (Storet_USGS_Climate_Merge$Season1 %in% c('Jan','Feb','Mar'), "Winter",
ifelse (Storet_USGS_Climate_Merge$Season1 %in% c('Apr','May','Jun'),
"Spring", "Monsoon")))
Storet_USGS_Climate_Merge$Season1 <- str_sub(Storet_USGS_Climate_Merge$Season,1,3)
Storet_USGS_Climate_Merge$Season1 <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season1 <- str_sub(Storet_USGS_Climate_Merge$Season1,1,3)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 == c('Oct','Nov','Dec'), "Fall","Monsoon")
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 == c('Oct','Nov','Dec'), "Fall","Monsoon")
Storet_USGS_Climate_Merge$Season1 <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season1 <- str_sub(Storet_USGS_Climate_Merge$Season1,1,3)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 == c('Oct','Nov','Dec'), "Fall", ifelse (Storet_USGS_Climate_Merge$Season1 == c('Jan','Feb','Mar'), "Winter",ifelse (Storet_USGS_Climate_Merge$Season1 == c('Apr','May','Jun'),
"Spring", "Monsoon")))
Storet_USGS_Climate_Merge$Season1 <- as.numeric(str_sub(Storet_USGS_Climate_Merge$DateAsDate,6,7))
View(Storet_USGS_Climate_Merge)
View(Storet_USGS_Climate_Merge)
Storet_USGS_Climate_Merge$Season1 <- as.numeric(str_sub(Storet_USGS_Climate_Merge$DateAsDate,6,7))
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 > 9 & Storet_USGS_Climate_Merge$Season1 < 13, "Fall",
ifelse (Storet_USGS_Climate_Merge$Season1 > 0 & Storet_USGS_Climate_Merge$Season1 < 4, "Winter",
ifelse (Storet_USGS_Climate_Merge$Season1 > 3 & Storet_USGS_Climate_Merge$Season1 < 7,
"Spring", "Monsoon")))
Storet_USGS_Climate_Merge %>%
ggplot(aes(x=Season, y=StandardizedResult)) +
geom_bar(stat='identity') +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
>>>>>>> 6c26e0b58185896c2b5e9c1bc94928625203d33e
library(data.table)
library(dataRetrieval)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Rename the column to something more useful.
setnames(USGS_Standardized_append,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_append, by="DateTime2",all.x=TRUE, all.y = FALSE)
#Remove the pre-averaging flow column. This allows to remove duplicates from the append pairs created above.
USGS_Standardized <- USGS_Standardized %>%
subset(select = -c(X_00060_00000)) %>%
distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)
# Re-check for duplicate values. This should equal zero.
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# Remove append table - data has been merged
remove(USGS_Standardized_append)
# Create the time series, formatted as POSIXct
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))
# Rename the Column DateTime2 to indicate properly formatted column.
colnames(TimeSeries)[1] = "DateTime2"
# Follow through on the name and format this bad boy.
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
#Left join data with time series, sort by DateTime2
USGS_Standardized <- merge(x=TimeSeries,y=USGS_Standardized,by="DateTime2",all.x=TRUE, all.y = FALSE)
# Remove the created time series. Data has been merged
remove(TimeSeries)
# This tests to see how many nulls are present in the current dataset.
# 9/5/2022: 40544 records are null
count(USGS_Standardized %>%
filter(is.na(Discharge_CFS)))
# This column is a dummy column which allows me to count more easily in the next block.
USGS_Standardized$date <- as_date(USGS_Standardized$DateTime2)
USGS_Standardized$count <- 1
# This counts the number of readings per day
TimeFlag <- USGS_Standardized %>%
filter(!is.na(Discharge_CFS)) %>%
group_by(date) %>%
summarize(value = sum(count))
# Append the reading count value to the standardized data
USGS_Standardized <- right_join(TimeFlag, USGS_Standardized)
# Name this something meaningful
setnames(USGS_Standardized, c("value", "X_00060_00000_cd", "X_00065_00000", "X_00065_00000_cd"), c("DailyFlowCount", "Discharge_cd", "USGS_Staff", "USGS_Staff_cd"), skip_absent = TRUE)
# Remove extraneous columns and reorder
USGS_Standardized <- USGS_Standardized %>%
select(DateTime2, date, Discharge_CFS, USGS_Staff, Discharge_cd, tz_cd, DailyFlowCount)
# Remove Timeflag table. Merged into data
remove(TimeFlag)
<<<<<<< HEAD
USGS_Standardized_Fill <-
USGS_Standardized %>%
filter(DailyFlowCount>88)
USGS_Standardized_Fill$Filled <-  na.approx(USGS_Standardized_Fill$Discharge_CFS, rule = 2)
USGS_Standardized_Fill <-
USGS_Standardized %>%
filter(DailyFlowCount>88)
USGS_Standardized$Filled <-  na.approx(USGS_Standardized_Fill$Discharge_CFS, rule = 2)
USGS_Standardized_Fill <-
USGS_Standardized %>%
filter(DailyFlowCount>88)
USGS_Standardized_Fill$Filled <-  na.approx(USGS_Standardized_Fill$Discharge_CFS, rule = 2)
USGS_Standardized <- right_join(USGS_TimeSeries_Fill,USGS_Standardized)
USGS_Standardized_Fill <-
USGS_Standardized %>%
filter(DailyFlowCount>88)
USGS_Standardized_Fill$Filled <-  na.approx(USGS_Standardized_Fill$Discharge_CFS, rule = 2)
USGS_Standardized <- right_join(USGS_Standardized_Fill,USGS_Standardized)
=======
>>>>>>> 6c26e0b58185896c2b5e9c1bc94928625203d33e
#Filter for columns with a daily flow count greater than 88 (no more than 2 hours per day missing.)
USGS_Standardized_Fill <-
USGS_Standardized %>%
filter(DailyFlowCount>88)
# Fill in missing data
USGS_Standardized_Fill$Filled <-  na.approx(USGS_Standardized_Fill$Discharge_CFS, rule = 2)
#Join back in
USGS_Standardized <- right_join(USGS_Standardized_Fill,USGS_Standardized)
# Remove the old data
remove(USGS_Standardized_Fill)
<<<<<<< HEAD
View(USGS_Standardized)
count(USGS_Standardized %>%
filter(is.na(Filled)))
=======
>>>>>>> 6c26e0b58185896c2b5e9c1bc94928625203d33e
# Fill in the un-filled values
USGS_Standardized$Filled<- ifelse(is.na(USGS_Standardized$Discharge_CFS) == TRUE,USGS_Standardized$Filled,USGS_Standardized$Discharge_CFS)
count(USGS_Standardized %>%
filter(is.na(Filled)))
# Remove the un-filled discharge column
<<<<<<< HEAD
USGS_ForAnalysis2 <-  USGS_ForAnalysis2 %>%
subset(select = -c(Discharge_CFS))
# Remove the un-filled discharge column
=======
>>>>>>> 6c26e0b58185896c2b5e9c1bc94928625203d33e
USGS_Standardized <-  USGS_Standardized %>%
subset(select = -c(Discharge_CFS))
# Rename to something meaningful
setnames(USGS_Standardized,old=c("Filled"),new=c("Discharge_CFS"))
<<<<<<< HEAD
RawFileName <- "Data/Raw/ecoli_StoretAndSODN.csv"
# This raw file contains the Storet and SODN data. The 30 SODN records were appended manually.
RawFileName <- "Data/Raw/ecoli_StoretAndSODN.csv"
#The Location file uses points lassoed in arcmap. It uses any sampling locations north of inputs from both the Nogales International Wastewater Treatment Plant and Sonoita Creek. Both the NIWWTP and Sonoita Creek are likely point sources of e. Coli.
LocationFileName <- "Data/Raw/EPA_PortalQuery_Locations_Appended_SODN.csv"
coliform_data <- read.csv(RawFileName)
Locations <- read.csv(LocationFileName)
write.csv(USGS_Standardized,"Data/Processed/USGS_Standardized.csv")
read.csv("Data/Processed/USGS_Standardized.csv")
USGS_Standardized <- read.csv("Data/Processed/USGS_Standardized.csv")
=======
>>>>>>> 6c26e0b58185896c2b5e9c1bc94928625203d33e
#This reads in the checkpoint file, if created.
#USGS_Standardized <- read.csv("Data/Processed/USGS_Standardized.csv")
# This raw file contains the Storet and SODN data. The 30 SODN records were appended manually.
RawStoret <- "Data/Raw/ecoli_Storet_Raw.csv"
RawSODN <- "Data/Raw/ecoli_SODN_Raw.csv"
<<<<<<< HEAD
RawLocation <- "ecoli_Locations_StoretandSODN.csv"
RawStoret_data <- read.csv(RawStoret)
RawSODN_data <- read.csv(RawSODN)
Locations <- read.csv(LocationFileName)
#This reads in the checkpoint file, if created.
#USGS_Standardized <- read.csv("Data/Processed/USGS_Standardized.csv")
# This raw file contains the Storet and SODN data. The 30 SODN records were appended manually.
RawStoret <- "Data/Raw/ecoli_Storet_Raw.csv"
RawSODN <- "Data/Raw/ecoli_SODN_Raw.csv"
RawLocation <- "ecoli_Locations_StoretandSODN.csv"
RawStoret_data <- read.csv(RawStoret)
RawSODN_data <- read.csv(RawSODN)
Locations <- read.csv(RawLocation)
RawLocation <- "Data/Raw/ecoli_Locations_StoretandSODN.csv"
Locations <- read.csv(RawLocation)
ecoli_data <- rbind(RawStoret_data, RawSODN_data)
View(RawStoret_data)
=======
RawLocation <- "Data/Raw/ecoli_Locations_StoretandSODN.csv"
RawStoret_data <- read.csv(RawStoret)
RawSODN_data <- read.csv(RawSODN)
Locations <- read.csv(RawLocation)
library(janitor)
RawStoret_data <- remove_empty_cols(RawStoret_data)
RawStoret_data <- remove_empty(RawStoret_data)
RawStoret_data <- remove_empty(RawStoret_data, c("cols"))
View(RawStoret_data)
RawSODN_data <- remove_empty(RawSODN, c("cols"))
RawSODN_data <- remove_empty(RawSODN_data, c("cols"))
test <- rbind.fill(RawStoret_data, RawSODN_data)
library(tidyverse)
library(plyr)
library(data.table)
library(dataRetrieval)
library(janitor)
library(knitr)
library(lubridate)
library(plyr)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
test <- rbind.fill(RawStoret_data, RawSODN_data)
write.csv(RawStoret_data,"Data/Processed/test.csv")
#This reads in the checkpoint file, if created.
#USGS_Standardized <- read.csv("Data/Processed/USGS_Standardized.csv")
# This raw file contains the Storet and SODN data. The 30 SODN records were appended manually.
RawStoret <- "Data/Raw/ecoli_Storet_Raw.csv"
RawSODN <- "Data/Raw/ecoli_SODN_Raw.csv"
RawLocation <- "Data/Raw/ecoli_Locations_StoretandSODN.csv"
RawStoret_data <- read.csv(RawStoret)
RawSODN_data <- read.csv(RawSODN)
Locations <- read.csv(RawLocation)
ecoli_data <- rbind(RawStoret_data, RawSODN_data)
# Use Janitor to clean null columns
RawStoret_data <- remove_empty(ecoli_data, c("cols"))
#This reads in the checkpoint file, if created.
USGS_Standardized <- read.csv("Data/Processed/USGS_Standardized.csv")
# This raw file contains the Storet and SODN data. The 30 SODN records were appended manually.
RawStoret <- "Data/Raw/ecoli_Storet_Raw.csv"
RawSODN <- "Data/Raw/ecoli_SODN_Raw.csv"
RawLocation <- "Data/Raw/ecoli_Locations_StoretandSODN.csv"
RawStoret_data <- read.csv(RawStoret)
RawSODN_data <- read.csv(RawSODN)
Locations <- read.csv(RawLocation)
# Merge the two dataframes together. SODN raw data was outputted to match the STORET format. Fields were manually filled by data management.
ecoli_data <- rbind(RawStoret_data, RawSODN_data)
# Use Janitor to clean null columns
ecoli_data<- remove_empty(ecoli_data, c("cols"))
# Remove the pre-merged raw data
remove(RawStoret_data, RawSODN_data)
View(ecoli_data)
# STORET has standardized the output as "Excherichia coli"
ecoli_data <- coliform_data[coliform_data$CharacteristicName == "Escherichia coli", ]
# STORET has standardized the output as "Excherichia coli"
ecoli_data <- ecoli_data[ecoli_data$CharacteristicName == "Escherichia coli", ]
# The methods are not standardized in STORET. It is assumed that all forms of Colilert and SM9223B represent comparable results. Colilert comes in both an 18 and 24 hour test, but they are assumed to provide a comparable output.
ecoli_data_9223b <- ecoli_data[ecoli_data$ResultAnalyticalMethod.MethodName == "E coli, water, Colilert (24-28)"|
ecoli_data$ResultAnalyticalMethod.MethodName ==  "COLILERT (EDBERG)"|
ecoli_data$ResultAnalyticalMethod.MethodName ==  "COLILERT"|
ecoli_data$ResultAnalyticalMethod.MethodName ==  "SM 9223B", ]
# The methods are not standardized in STORET. It is assumed that all forms of Colilert and SM9223B represent comparable results. Colilert comes in both an 18 and 24 hour test, but they are assumed to provide a comparable output.
ecoli_data <- ecoli_data[ecoli_data$ResultAnalyticalMethod.MethodName == "E coli, water, Colilert (24-28)"|
ecoli_data$ResultAnalyticalMethod.MethodName ==  "COLILERT (EDBERG)"|
ecoli_data$ResultAnalyticalMethod.MethodName ==  "COLILERT"|
ecoli_data$ResultAnalyticalMethod.MethodName ==  "SM 9223B", ]
ecoli_SampleLimit<- merge(ecoli_data, Locations, by = c("MonitoringLocationIdentifier"))
ecoli_data<- merge(ecoli_data, Locations, by = c("MonitoringLocationIdentifier"))
remove(ecoli_data_9223b, Locations)
remove(ecoli_data_9223b, Locations, ecoli_data)
library(data.table)
library(dataRetrieval)
library(janitor)
library(knitr)
library(lubridate)
library(plyr)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
library(data.table)
library(dataRetrieval)
library(janitor)
library(knitr)
library(lubridate)
library(plyr)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
library(data.table)
library(dataRetrieval)
library(janitor)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
USGS_Standardized %>% group_by(DateTime2) %>% dplyr::summarise(n=sum(n())) %>% filter(n>1)
library(data.table)
library(dataRetrieval)
library(janitor)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% dplyr::summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Rename the column to something more useful.
setnames(USGS_Standardized_append,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_append, by="DateTime2",all.x=TRUE, all.y = FALSE)
library(data.table)
library(dataRetrieval)
library(janitor)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
# Loaded plyr for later testing and it made this piece of the code upset. "dplyr::" should not be necessary, but it avoids issues.
USGS_Standardized %>% group_by(DateTime2) %>% dplyr::summarise(n=sum(n())) %>% filter(n>1)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
>>>>>>> 6c26e0b58185896c2b5e9c1bc94928625203d33e
