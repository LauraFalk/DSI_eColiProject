Storet_USGS_Merge_NoNull %>%
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point()+
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2)+
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Merge_NoNull %>%
filter(Discharge_CFS < 250)
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point()+
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2)+
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Merge_NoNull %>%
filter(Discharge_CFS < 250) +
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Merge_NoNull %>%
filter(Discharge_CFS < 250) %>%
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Merge_NoNull %>%
filter(Discharge_CFS < 150) %>%
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats")
read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv")
read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv")
library(knitr)
library(lubridate)
library(tidyverse)
opts_chunk$set(echo = TRUE)
DailyClimate <- read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv")
DailyClimate$DateAsDate <- as.POSIXct(format(DailyClimate$Date, format = '%Y-%m-%d'), tz = tz)
DailyClimate$DateAsDate <- as.POSIXct(format(DailyClimate$Date, format = '%d/%m/%Y'), tz = tz)
View(DailyClimate)
DailyClimate$DateAsDate <- as.POSIXct(format(DailyClimate$Date, format = '%Y-%m-%d'), tz = tz)
DailyClimate <- read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv", skip = 6)
View(DailyClimate)
DailyClimate <- read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv", skip = 3)
View(DailyClimate)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-6,-1)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-8,-1)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-9,-1)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-10,-1)
as.POSIXlt(DailyClimate$DateAsDate, tz = tz)
DailyClimate <- head(DailyClimate, - 1)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-10,-1)
as.POSIXlt(DailyClimate$DateAsDate, tz = tz)
DailyClimate$DateAsDate <-as.POSIXct(DailyClimate$DateAsDate, format = "%mm/%dd/%YYYY", tz = tz)
Storet_USGS_Climate_Merge<- left_join(Storet_USGS_Merge_NoNull,
DailyClimate, by = c("DateAsDate"))
Storet_USGS_Merge_NoNull$DateAsDate <-as.POSIXct(Storet_USGS_Merge_NoNull$DateAsDate, format = "%Y-%m-%d", tz = tz)
Storet_USGS_Climate_Merge<- left_join(Storet_USGS_Merge_NoNull,
DailyClimate, by = c("DateAsDate"))
Storet_USGS_Climate_Merge %>%
filter(!is.na(DailyClimate))
Storet_USGS_Climate_Merge %>%
filter(!is.na(Tmax..F.))
Storet_USGS_Climate_Merge %>%
filter(!is.null(Tmax..F.))
DailyClimate$DateAsDate <-as.POSIXct(DailyClimate$DateAsDate, format = "%m/%d/%Y", tz = tz)
DailyClimate <- read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv", skip = 3)
DailyClimate <- head(DailyClimate, - 1)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-10,-1)
DailyClimate$DateAsDate <-as.POSIXct(DailyClimate$DateAsDate, format = "%m/%d/%Y", tz = tz)
Storet_USGS_Merge_NoNull$DateAsDate <-as.POSIXct(Storet_USGS_Merge_NoNull$DateAsDate, format = "%Y-%m-%d", tz = tz)
Storet_USGS_Climate_Merge<- left_join(Storet_USGS_Merge_NoNull,
DailyClimate, by = c("DateAsDate"))
Storet_USGS_Climate_Merge %>%
filter(!is.null(Tmax..F.))
Storet_USGS_Climate_Merge %>%
filter(is.null(Tmax..F.))
View(Storet_USGS_Climate_Merge)
Storet_USGS_Climate_Merge %>%
filter(Tmax..F. == NaN)
Storet_USGS_Climate_Merge %>%
filter(is.nan(Tmax..F.))
Storet_USGS_Climate_Merge %>%
filter(!is.nan(Tmax..F.)) %>%
ggplot(aes(x=Tmax..F., y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge %>%
filter(!is.nan(Tmax..F.)) %>%
ggplot(aes(x=Tmax..F., y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge %>%
filter(!is.nan(Tmin..F.)) %>%
ggplot(aes(x=Tmin..F., y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge %>%
filter(!is.nan(Precipitation..in.)) %>%
ggplot(aes(x=Precipitation..in., y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge$Season <- as.yearmon(DateAsDate)
Storet_USGS_Climate_Merge$Season <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
View(Storet_USGS_Climate_Merge)
Storet_USGS_Climate_Merge$Season <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season %in% c('May','June','July'), "SUMMER",
ifelse (Storet_USGS_Climate_Merge$Season %in% c('August','September','October'), "AUTUMN",
ifelse (Storet_USGS_Climate_Merge$Season %in% c('November','December','January'),
"WINTER", "SPRING")))
Storet_USGS_Climate_Merge$Season <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season %in% c('October','November','December'), "Fall",
ifelse (Storet_USGS_Climate_Merge$Season %in% c('January','February','March'), "Winter",
ifelse (Storet_USGS_Climate_Merge$Season %in% c('April','May','June'),
"Spring", "Monsoon")))
Storet_USGS_Climate_Merge %>%
ggplot(aes(x=Season, y=StandardizedResult)) +
geom_bar() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge %>%
ggplot(aes(x=Season, y=StandardizedResult)) +
geom_bar(stat='identity') +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge$Season1 <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 %in% c('October','November','December'), "Fall",
ifelse (Storet_USGS_Climate_Merge$Season1 %in% c('January','February','March'), "Winter",
ifelse (Storet_USGS_Climate_Merge$Season1 %in% c('April','May','June'),
"Spring", "Monsoon")))
Storet_USGS_Climate_Merge$Season1 <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 %in% c('Oct','Nov','Dec'), "Fall",
ifelse (Storet_USGS_Climate_Merge$Season1 %in% c('Jan','Feb','Mar'), "Winter",
ifelse (Storet_USGS_Climate_Merge$Season1 %in% c('Apr','May','Jun'),
"Spring", "Monsoon")))
Storet_USGS_Climate_Merge$Season1 <- str_sub(Storet_USGS_Climate_Merge$Season,1,3)
Storet_USGS_Climate_Merge$Season1 <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season1 <- str_sub(Storet_USGS_Climate_Merge$Season1,1,3)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 == c('Oct','Nov','Dec'), "Fall","Monsoon")
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 == c('Oct','Nov','Dec'), "Fall","Monsoon")
Storet_USGS_Climate_Merge$Season1 <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season1 <- str_sub(Storet_USGS_Climate_Merge$Season1,1,3)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 == c('Oct','Nov','Dec'), "Fall", ifelse (Storet_USGS_Climate_Merge$Season1 == c('Jan','Feb','Mar'), "Winter",ifelse (Storet_USGS_Climate_Merge$Season1 == c('Apr','May','Jun'),
"Spring", "Monsoon")))
Storet_USGS_Climate_Merge$Season1 <- as.numeric(str_sub(Storet_USGS_Climate_Merge$DateAsDate,6,7))
View(Storet_USGS_Climate_Merge)
View(Storet_USGS_Climate_Merge)
Storet_USGS_Climate_Merge$Season1 <- as.numeric(str_sub(Storet_USGS_Climate_Merge$DateAsDate,6,7))
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 > 9 & Storet_USGS_Climate_Merge$Season1 < 13, "Fall",
ifelse (Storet_USGS_Climate_Merge$Season1 > 0 & Storet_USGS_Climate_Merge$Season1 < 4, "Winter",
ifelse (Storet_USGS_Climate_Merge$Season1 > 3 & Storet_USGS_Climate_Merge$Season1 < 7,
"Spring", "Monsoon")))
Storet_USGS_Climate_Merge %>%
ggplot(aes(x=Season, y=StandardizedResult)) +
geom_bar(stat='identity') +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
library(data.table)
library(dataRetrieval)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Rename the column to something more useful.
setnames(USGS_Standardized_append,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_append, by="DateTime2",all.x=TRUE, all.y = FALSE)
#Remove the pre-averaging flow column. This allows to remove duplicates from the append pairs created above.
USGS_Standardized <- USGS_Standardized %>%
subset(select = -c(X_00060_00000)) %>%
distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)
# Re-check for duplicate values. This should equal zero.
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# Remove append table - data has been merged
remove(USGS_Standardized_append)
# Create the time series, formatted as POSIXct
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))
# Rename the Column DateTime2 to indicate properly formatted column.
colnames(TimeSeries)[1] = "DateTime2"
# Follow through on the name and format this bad boy.
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
#Left join data with time series, sort by DateTime2
USGS_Standardized <- merge(x=TimeSeries,y=USGS_Standardized,by="DateTime2",all.x=TRUE, all.y = FALSE)
# Remove the created time series. Data has been merged
remove(TimeSeries)
# This tests to see how many nulls are present in the current dataset.
# 9/5/2022: 40544 records are null
count(USGS_Standardized %>%
filter(is.na(Discharge_CFS)))
# This column is a dummy column which allows me to count more easily in the next block.
USGS_Standardized$date <- as_date(USGS_Standardized$DateTime2)
USGS_Standardized$count <- 1
# This counts the number of readings per day
TimeFlag <- USGS_Standardized %>%
filter(!is.na(Discharge_CFS)) %>%
group_by(date) %>%
summarize(value = sum(count))
# Append the reading count value to the standardized data
USGS_Standardized <- right_join(TimeFlag, USGS_Standardized)
# Name this something meaningful
setnames(USGS_Standardized, c("value", "X_00060_00000_cd", "X_00065_00000", "X_00065_00000_cd"), c("DailyFlowCount", "Discharge_cd", "USGS_Staff", "USGS_Staff_cd"), skip_absent = TRUE)
# Remove extraneous columns and reorder
USGS_Standardized <- USGS_Standardized %>%
select(DateTime2, date, Discharge_CFS, USGS_Staff, Discharge_cd, tz_cd, DailyFlowCount)
# Remove Timeflag table. Merged into data
remove(TimeFlag)
#Filter for columns with a daily flow count greater than 88 (no more than 2 hours per day missing.)
USGS_Standardized_Fill <-
USGS_Standardized %>%
filter(DailyFlowCount>88)
# Fill in missing data
USGS_Standardized_Fill$Filled <-  na.approx(USGS_Standardized_Fill$Discharge_CFS, rule = 2)
#Join back in
USGS_Standardized <- right_join(USGS_Standardized_Fill,USGS_Standardized)
# Remove the old data
remove(USGS_Standardized_Fill)
# Fill in the un-filled values
USGS_Standardized$Filled<- ifelse(is.na(USGS_Standardized$Discharge_CFS) == TRUE,USGS_Standardized$Filled,USGS_Standardized$Discharge_CFS)
count(USGS_Standardized %>%
filter(is.na(Filled)))
# Remove the un-filled discharge column
USGS_Standardized <-  USGS_Standardized %>%
subset(select = -c(Discharge_CFS))
# Rename to something meaningful
setnames(USGS_Standardized,old=c("Filled"),new=c("Discharge_CFS"))
#This reads in the checkpoint file, if created.
#USGS_Standardized <- read.csv("Data/Processed/USGS_Standardized.csv")
# This raw file contains the Storet and SODN data. The 30 SODN records were appended manually.
RawStoret <- "Data/Raw/ecoli_Storet_Raw.csv"
RawSODN <- "Data/Raw/ecoli_SODN_Raw.csv"
RawLocation <- "Data/Raw/ecoli_Locations_StoretandSODN.csv"
RawStoret_data <- read.csv(RawStoret)
RawSODN_data <- read.csv(RawSODN)
Locations <- read.csv(RawLocation)
library(janitor)
RawStoret_data <- remove_empty_cols(RawStoret_data)
RawStoret_data <- remove_empty(RawStoret_data)
RawStoret_data <- remove_empty(RawStoret_data, c("cols"))
View(RawStoret_data)
RawSODN_data <- remove_empty(RawSODN, c("cols"))
RawSODN_data <- remove_empty(RawSODN_data, c("cols"))
test <- rbind.fill(RawStoret_data, RawSODN_data)
library(tidyverse)
library(plyr)
library(data.table)
library(dataRetrieval)
library(janitor)
library(knitr)
library(lubridate)
library(plyr)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
test <- rbind.fill(RawStoret_data, RawSODN_data)
write.csv(RawStoret_data,"Data/Processed/test.csv")
#This reads in the checkpoint file, if created.
#USGS_Standardized <- read.csv("Data/Processed/USGS_Standardized.csv")
# This raw file contains the Storet and SODN data. The 30 SODN records were appended manually.
RawStoret <- "Data/Raw/ecoli_Storet_Raw.csv"
RawSODN <- "Data/Raw/ecoli_SODN_Raw.csv"
RawLocation <- "Data/Raw/ecoli_Locations_StoretandSODN.csv"
RawStoret_data <- read.csv(RawStoret)
RawSODN_data <- read.csv(RawSODN)
Locations <- read.csv(RawLocation)
ecoli_data <- rbind(RawStoret_data, RawSODN_data)
# Use Janitor to clean null columns
RawStoret_data <- remove_empty(ecoli_data, c("cols"))
#This reads in the checkpoint file, if created.
USGS_Standardized <- read.csv("Data/Processed/USGS_Standardized.csv")
# This raw file contains the Storet and SODN data. The 30 SODN records were appended manually.
RawStoret <- "Data/Raw/ecoli_Storet_Raw.csv"
RawSODN <- "Data/Raw/ecoli_SODN_Raw.csv"
RawLocation <- "Data/Raw/ecoli_Locations_StoretandSODN.csv"
RawStoret_data <- read.csv(RawStoret)
RawSODN_data <- read.csv(RawSODN)
Locations <- read.csv(RawLocation)
# Merge the two dataframes together. SODN raw data was outputted to match the STORET format. Fields were manually filled by data management.
ecoli_data <- rbind(RawStoret_data, RawSODN_data)
# Use Janitor to clean null columns
ecoli_data<- remove_empty(ecoli_data, c("cols"))
# Remove the pre-merged raw data
remove(RawStoret_data, RawSODN_data)
View(ecoli_data)
# STORET has standardized the output as "Excherichia coli"
ecoli_data <- coliform_data[coliform_data$CharacteristicName == "Escherichia coli", ]
# STORET has standardized the output as "Excherichia coli"
ecoli_data <- ecoli_data[ecoli_data$CharacteristicName == "Escherichia coli", ]
# The methods are not standardized in STORET. It is assumed that all forms of Colilert and SM9223B represent comparable results. Colilert comes in both an 18 and 24 hour test, but they are assumed to provide a comparable output.
ecoli_data_9223b <- ecoli_data[ecoli_data$ResultAnalyticalMethod.MethodName == "E coli, water, Colilert (24-28)"|
ecoli_data$ResultAnalyticalMethod.MethodName ==  "COLILERT (EDBERG)"|
ecoli_data$ResultAnalyticalMethod.MethodName ==  "COLILERT"|
ecoli_data$ResultAnalyticalMethod.MethodName ==  "SM 9223B", ]
# The methods are not standardized in STORET. It is assumed that all forms of Colilert and SM9223B represent comparable results. Colilert comes in both an 18 and 24 hour test, but they are assumed to provide a comparable output.
ecoli_data <- ecoli_data[ecoli_data$ResultAnalyticalMethod.MethodName == "E coli, water, Colilert (24-28)"|
ecoli_data$ResultAnalyticalMethod.MethodName ==  "COLILERT (EDBERG)"|
ecoli_data$ResultAnalyticalMethod.MethodName ==  "COLILERT"|
ecoli_data$ResultAnalyticalMethod.MethodName ==  "SM 9223B", ]
ecoli_SampleLimit<- merge(ecoli_data, Locations, by = c("MonitoringLocationIdentifier"))
ecoli_data<- merge(ecoli_data, Locations, by = c("MonitoringLocationIdentifier"))
remove(ecoli_data_9223b, Locations)
remove(ecoli_data_9223b, Locations, ecoli_data)
library(data.table)
library(dataRetrieval)
library(janitor)
library(knitr)
library(lubridate)
library(plyr)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
library(data.table)
library(dataRetrieval)
library(janitor)
library(knitr)
library(lubridate)
library(plyr)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
library(data.table)
library(dataRetrieval)
library(janitor)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
USGS_Standardized %>% group_by(DateTime2) %>% dplyr::summarise(n=sum(n())) %>% filter(n>1)
library(data.table)
library(dataRetrieval)
library(janitor)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% dplyr::summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Rename the column to something more useful.
setnames(USGS_Standardized_append,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_append, by="DateTime2",all.x=TRUE, all.y = FALSE)
library(data.table)
library(dataRetrieval)
library(janitor)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
# Loaded plyr for later testing and it made this piece of the code upset. "dplyr::" should not be necessary, but it avoids issues.
USGS_Standardized %>% group_by(DateTime2) %>% dplyr::summarise(n=sum(n())) %>% filter(n>1)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
```{r setup, include=FALSE}
library(data.table)
library(data.table)
library(dataRetrieval)
library(janitor)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
