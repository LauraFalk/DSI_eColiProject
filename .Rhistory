spatiallocs <- read_sf("Data/Processed/Attributed_Location/ecoli_UniqueLocs.shp")
spatiallocs <- read_sf("Data/Processed/Attributed_Location/ecoli_UniqueLocs.shp")
# Retrieve all variables using the functions
predictionDF <- as.data.frame(spatiallocs)
predictionDF$PreviousTmin <- c(Var_TMin)
predictionDF$Discharge_CFS	<- c(Var_Discharge_CFS)
predictionDF$Stage	<- c(Var_Stage)
predictionDF$NinXTS	<- c(Var_NinXTS)
predictionDF$TOD <- c(Var_TOD)
predictionDF <- predictionDF %>%
rename(DistFromSonoita = DistCatego) %>%
select(PreviousTmin, Discharge_CFS, Stage, NinXTS, TOD, DistFromSonoita)
View(predictionDF)
# Run the model for 575
XGBModel <- xgb.load('Data/Processed/XGBmodel575')
predictionDM <- data.matrix(predictionDF)
pred <- predict(XGBModel,predictionDM)
pred <-  as.numeric(pred > 0.4)
spatiallocs$pred575 <- c(pred)
spatiallocs$pred575 <- ifelse(spatiallocs$pred575 > 0, "Bacteria Level >575 Likely", "High Bacteria levels > 575 not predicted")
View(spatiallocs)
# The default is working better. LAURA look at this more. Until then, save the default model to work on the automation.
#tidypredict_fit(finalmodel)
xgb.save(finalmodel, "Data/Processed/XGBmodel235")
# Run the model for 235
XGBModel <- xgb.load('Data/Processed/XGBmodel235')
predictionDM <- data.matrix(predictionDF)
pred <- predict(XGBModel,predictionDM)
pred <-  as.numeric(pred > 0.4)
spatiallocs$pred235 <- c(pred)
spatiallocs$pred35 <- ifelse(spatiallocs$pred235 > 0, "Bacteria Level >235  Likely", "High Bacteria levels > 235 not predicted")
spatiallocs <- spatiallocs %>%
select(-DistCatego) %>%
rename(Samplers = Organiza_2, Prediction = pred)
View(spatiallocs)
########### Functions #################
sysDate1 <- Sys.time()
# T Min
get.Tmin <- function(sysDate) {
formattedEndYear <- as.numeric(format(sysDate, "%Y"))
TMin <- climateAnalyzeR::import_data("daily_wx"
, station_id = 'KA7WSB-1'
, start_year = formattedEndYear-1
, end_year = formattedEndYear
, station_type = 'RAWS')
Var_TMin <- as.numeric(unlist(TMin %>%
mutate(DateasDate = as.POSIXct(TMin$date, format = "%m/%d/%Y")) %>%
subset(DateasDate == as.Date(sysDate) - 2) %>%
select(tmin_f)))
}
Var_TMin <- get.Tmin(sysDate1)
# Discharge
get.DischargeCFS <- function(sysDate) {
startDate <- as.Date(format(sysDate1,'%Y-%m-%d')) - 31
endDate <- as.Date(format(sysDate1,'%Y-%m-%d')) - 1
USGSRaw <- readNWISuv(siteNumbers = '09481740', c('00060','00065'), startDate,endDate, tz = 'America/Phoenix')
tail(USGSRaw$X_00060_00000, n=1)
}
Var_Discharge_CFS <- get.DischargeCFS(sysDate1)
# Stage
get.stage <- function(sysDate) {
startDate <- as.Date(format(sysDate1,'%Y-%m-%d')) - 31
endDate <- as.Date(format(sysDate1,'%Y-%m-%d')) - 1
USGSRaw <- readNWISuv(siteNumbers = '09481740', c('00060','00065'), startDate,endDate, tz = 'America/Phoenix')
# Create quantiles for categorization
CFS_Quantiles<- quantile(USGSRaw$X_00060_00000, na.rm = TRUE)
# Determine the difference between prior reading and current.
USGSRaw <- USGSRaw %>%
mutate(DisDif = X_00060_00000 - lag(X_00060_00000))
# This will create a binary variable or either rise of fall. Rise = 1, fall = 0. It will allow me to more easily create summary statistics.
USGSRaw$DisDif2 <- ifelse(USGSRaw$DisDif>0,1,0)
# Create a numeric classifier.
# 1 = Low Flow, 2 = Base flow, 3 = High and Rising Flow 4 = High and Falling Flow
USGSRaw$Stage <- ifelse(USGSRaw$X_00060_00000 <=CFS_Quantiles[2], 1,
ifelse(USGSRaw$X_00060_00000 > CFS_Quantiles[2] & USGSRaw$X_00060_00000 <= CFS_Quantiles[4],2,
ifelse(USGSRaw$X_00060_00000 > CFS_Quantiles[4] & USGSRaw$DisDif2 == 1,3,
ifelse(USGSRaw$X_00060_00000 > CFS_Quantiles[4] & USGSRaw$DisDif2 == 0,4, NA))))
# Create the stage variable.
tail(USGSRaw$Stage, n=1)
}
Var_Stage <- get.stage(sysDate1)
# El Nino
get.NinXTS <- function(sysDate) {
formattedEndYear <- as.numeric(format(sysDate, "%Y"))
formattedMonth <- as.numeric(format(sysDate,"%m"))
# Bring in the website data
url <- "https://origin.cpc.ncep.noaa.gov/products/analysis_monitoring/ensostuff/ONI_v5.php"
NinXTS <- url %>%
rvest::read_html()
# Grab the data table
NinText <- rvest::html_table(rvest::html_nodes(NinXTS, xpath = './/table[4]//table[2]'))
# Convert ONI index to dataframe
NinTable <- as.data.frame(NinText[1]) %>%
row_to_names(row_number = 1) %>%
mutate(Year = as.numeric(Year)) %>%
drop_na(Year)
# I need to do either month-of or last non-na value to account for delays.
formattedMonth <-12
NinVal <- NinTable %>%
subset(2022 == Year) %>%
select(case_when(formattedMonth == 1 ~ "NDJ",
formattedMonth == 2 ~ "DJF",
formattedMonth == 3 ~ "JFM",
formattedMonth == 4 ~ "FMA",
formattedMonth == 5 ~ "MAM",
formattedMonth == 6 ~ "AMJ",
formattedMonth == 7 ~ "MJJ",
formattedMonth == 8 ~ "JJA",
formattedMonth == 9 ~ "JAS",
formattedMonth == 10 ~ "ASO",
formattedMonth == 11 ~ "SON",
formattedMonth == 12 ~ "OND")) %>%
unlist()
PrevVal <- NinTable %>%
subset(2022 == Year) %>%
select(case_when(formattedMonth == 2 ~ "NDJ",
formattedMonth == 3 ~ "DJF",
formattedMonth == 4 ~ "JFM",
formattedMonth == 5 ~ "FMA",
formattedMonth == 6 ~ "MAM",
formattedMonth == 7 ~ "AMJ",
formattedMonth == 8 ~ "MJJ",
formattedMonth == 9 ~ "JJA",
formattedMonth == 10 ~ "JAS",
formattedMonth == 11 ~ "ASO",
formattedMonth == 12 ~ "SON",
formattedMonth == 1 ~ "OND")) %>%
unlist()
if_else(!is.na(NinVal), NinVal, PrevVal)
}
Var_NinXTS <- get.NinXTS(sysDate1)
# Time of Day
#modified from https://stackoverflow.com/questions/49370387/convert-time-object-to-categorical-morning-afternoon-evening-night-variable
get.TOD <- function(sysTime) {
# Create categorical variables
currenttime <- as.POSIXct(sysTime, format = "%H:%M") %>% format("%H:%M:%S")
currenttime <- cut(chron::times(currenttime) , breaks = (1/24) * c(0,5,11,16,19,24))
Var_TOD <- c(4, 1, 2, 3, 4)[as.numeric(currenttime)]
}
Var_TOD <- get.TOD(sysDate1)
# Dist from Sonoita is within the mapping layer
spatiallocs <- read_sf("Data/Processed/Attributed_Location/ecoli_UniqueLocs.shp")
# Retrieve all variables using the functions
predictionDF <- as.data.frame(spatiallocs)
predictionDF$PreviousTmin <- c(Var_TMin)
predictionDF$Discharge_CFS	<- c(Var_Discharge_CFS)
predictionDF$Stage	<- c(Var_Stage)
predictionDF$NinXTS	<- c(Var_NinXTS)
predictionDF$TOD <- c(Var_TOD)
predictionDF <- predictionDF %>%
rename(DistFromSonoita = DistCatego) %>%
select(PreviousTmin, Discharge_CFS, Stage, NinXTS, TOD, DistFromSonoita)
# Run the model for 235
XGBModel <- xgb.load('Data/Processed/XGBmodel235')
predictionDM <- data.matrix(predictionDF)
pred <- predict(XGBModel,predictionDM)
pred <-  as.numeric(pred > 0.4)
spatiallocs$pred235 <- c(pred)
spatiallocs$pred35 <- ifelse(spatiallocs$pred235 > 0, "Bacteria Level >235  Likely", "High Bacteria levels > 235 not predicted")
# Run the model for 575
XGBModel <- xgb.load('Data/Processed/XGBmodel575')
pred <- predict(XGBModel,predictionDM)
pred <-  as.numeric(pred > 0.4)
spatiallocs$pred575 <- c(pred)
spatiallocs$pred575 <- ifelse(spatiallocs$pred575 > 0, "Bacteria Level >575 Likely", "High Bacteria levels > 575 not predicted")
View(predictionDF)
View(spatiallocs)
# The default is working better. LAURA look at this more. Until then, save the default model to work on the automation.
#tidypredict_fit(finalmodel)
xgb.save(finalmodel, "Data/Processed/XGBmodel_575")
library(caret)
library(data.table)
library(e1071)
library(knitr)
library(mlr)
library(parallel)
library(parallelMap)
library(xgboost)
#library(stackgbm)
# Read in data
ecoli_attr <- read.csv("Data/Processed/ecoli_attributed2.csv")
# Partition the data to include only 236 e coli. 575 is column 10
ecoli_575 <- ecoli_attr[,-1]
ecoli_575 <- ecoli_575[,-7]
#make this example reproducible
set.seed(5)
#split into training (80%) and testing set (20%)
parts <- createDataPartition(ecoli_575$ecoli_575, p = .7, list = F)
train <- ecoli_575[parts, ]
test <- ecoli_575[-parts, ]
train_x <- data.matrix(train[, -7])
train_y <- train[,7]
test_x <- data.matrix(test[, -7])
test_y <- test[, 7]
# Find the model with the lowest log loss
xgb_train <- xgb.DMatrix(data = train_x, label = train_y)
xgb_test <- xgb.DMatrix(data = test_x, label = test_y)
watchlist <- list(train=xgb_train, test=xgb_test)
model <- xgb.train(data = xgb_train, max.depth = 5, watchlist=watchlist, nrounds = 100,objective = "binary:logistic")
# Use those values ad the final nrounds values
finalmodel <- xgboost(data = xgb_train, max.depth = 5, nrounds = 8, verbose = 0)
set.seed(0)
# Predict the new values
pred <- predict(finalmodel, as.matrix(test_x))
# Use 0.4 to favor decreasing the false negative results
pred <-  as.numeric(pred > 0.4)
# Create a confusion matrix of these values
confusionMatrix(factor(pred),factor(test_y))
test2 <- ecoli_575[-parts, ]
test2$PreviousTmin <- test2$PreviousTmin + 16
test_x2 <- data.matrix(test2[, -7])
pred <- predict(finalmodel, as.matrix(test_x2))
pred <-  as.numeric(pred > 0.5)
confusionMatrix(factor(pred),factor(test_y))
# Bring the data back in since it was altered above
test3 <- ecoli_575[-parts, ]
test3$Discharge_CFS <- test3$Discharge_CFS + 20
test_x3 <- data.matrix(test3[, -7])
pred <- predict(finalmodel, as.matrix(test_x3))
pred <-  as.numeric(pred > 0.5)
confusionMatrix(factor(pred),factor(test_y))
test4 <- ecoli_575[-parts, ]
test4$PreviousTmin <- test4$PreviousTmin + 20
test4$Discharge_CFS <- test4$Discharge_CFS + 40
test_x4 <- data.matrix(test4[, -7])
pred <- predict(finalmodel, as.matrix(test_x4))
pred <-  as.numeric(pred > 0.5)
confusionMatrix(factor(pred),factor(test_y))
library(tidyverse)
folds = createFolds(train_x, k = 10)
cv <- lapply(folds, function(x) {
# Use the lowest log loss from the first chunk
classifier = xgboost(data = as.matrix(train_x), label = train_y, max.depth = 5, nrounds = 65, verbose = FALSE)
y_pred = predict(classifier, newdata = as.matrix(test_x)) # again need a matrix
y_pred = (y_pred >= 0.45) # here we are setting up the binary outcome of 0 or 1
cm = table(test_y, y_pred)
accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
return(accuracy)
})
accuracy = mean(as.numeric(cv))
print(accuracy)
# This needs to be scaled.
folds = createFolds(train_x, k = 10)
cv <- lapply(folds, function(x) {
# Use the lowest log loss from the first chunk
classifier = svm(x = as.matrix(train_x), y = as.vector(train_y), type = 'C-classification', cost = 92.101, gamma=36.501)
y_pred = predict(classifier, newdata = as.matrix(test_x)) # again need a matrix
#y_pred = (y_pred >= 0.45) # here we are setting up the binary outcome of 0 or 1
cm = table(test_y, y_pred)
accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
return(accuracy)
})
accuracy = mean(as.numeric(cv))
print(accuracy)
# Create a model using the default parameters
default_params <- list(booster = "gbtree",
objective = "binary:logistic",
eta=0.3,
gamma=0,
max_depth=6,
min_child_weight=1,
subsample=1,
colsample_bytree=1)
#train_matrix <- xgb.DMatrix(train_x)
xgbcv <- xgb.cv(params = default_params,
data = train_x,
label=train_y,
nrounds = 100,
nfold = 5,
showsd = T,
stratified = T,
print_every_n = 10,
early_stopping_round = 20,
maximize = F)
xgb1 <- xgb.train(params = default_params,
data = xgb_train,
nrounds = 22,
watchlist = watchlist,
print_every_n = 10,
early_stopping_round = 5,
maximize = F,
eval_metric = "error")
xgbpred <- predict(xgb1,test_x)
xgbpred <- ifelse(xgbpred > 0.4,1,0)
confusionMatrix(factor(xgbpred),factor(test_y))
mat <- xgb.importance(feature_names = colnames(train_x),model = xgb1)
xgb.plot.importance(importance_matrix = mat[1:6])
train <- as.data.frame(train)
test <- as.data.frame(test)
#create tasks
traintask <- makeClassifTask(data = train,target = "ecoli_575")
testtask <- makeClassifTask(data = test,target = "ecoli_575")
#do one hot encoding`<br/>
traintask <- createDummyFeatures(obj = traintask)
testtask <- createDummyFeatures(obj = testtask)
lrn <- makeLearner("classif.xgboost",
predict.type = "response")
lrn$par.vals <- list(objective="binary:logistic",
eval_metric="error",
nrounds=100L,
eta=0.1)
params <- makeParamSet(#makeDiscreteParam("booster", values = c("gbtree","gblinear")),
makeIntegerParam("max_depth",lower = 3L,upper = 5L),
makeNumericParam("min_child_weight",lower = 1L,upper = 10L),
makeNumericParam("subsample",lower = 0.5,upper = 1),
makeNumericParam("colsample_bytree",lower = 0.5,upper = 1))
rdesc <- makeResampleDesc("LOO")
#rdesc <- makeResampleDesc("CV",stratify = T,iters=5L)
# Maxint is 100 but this takes a long time. Leave at 10L until further notice
ctrl <- makeTuneControlRandom(maxit = 10L)
parallelStartSocket(cpus = detectCores())
#parameter tuning
mytune <- mlr::tuneParams(learner = lrn, task = traintask, resampling = rdesc, measures = acc, par.set = params, control = ctrl, show.info = T)
mytune$y
#set hyperparameters
lrn_tune <- setHyperPars(lrn,par.vals = mytune$x)
#train model
xgmodel2 <- mlr::train(learner = lrn_tune,task = traintask)
#predict model
xgpred2 <- predict(xgmodel2,testtask)
confusionMatrix(xgpred2$data$response,xgpred2$data$truth)
# The default is working better. LAURA look at this more. Until then, save the default model to work on the automation.
#tidypredict_fit(finalmodel)
xgb.save(finalmodel, "Data/Processed/XGBmodel575")
library(caret)
library(data.table)
library(e1071)
library(knitr)
library(mlr)
library(parallel)
library(parallelMap)
library(xgboost)
#library(stackgbm)
# Read in data
ecoli_attr <- read.csv("Data/Processed/ecoli_attributed2.csv")
# Partition the data to include only 236 e coli. 575 is column 10
ecoli_235 <- ecoli_attr[,2:8]
#make this example reproducible
set.seed(5)
#split into training (80%) and testing set (20%)
parts <- createDataPartition(ecoli_235$ecoli_235, p = .7, list = F)
train <- ecoli_235[parts, ]
test <- ecoli_235[-parts, ]
train_x <- data.matrix(train[, -7])
train_y <- train[,7]
test_x <- data.matrix(test[, -7])
test_y <- test[, 7]
# Find the model with the lowest log loss
xgb_train <- xgb.DMatrix(data = train_x, label = train_y)
xgb_test <- xgb.DMatrix(data = test_x, label = test_y)
watchlist <- list(train=xgb_train, test=xgb_test)
model <- xgb.train(data = xgb_train, max.depth = 5, watchlist=watchlist, nrounds = 100,objective = "binary:logistic")
# Use those values ad the final nrounds values
finalmodel <- xgboost(data = xgb_train, max.depth = 5, nrounds = 8, verbose = 0)
set.seed(0)
# Predict the new values
pred <- predict(finalmodel, as.matrix(test_x))
# Use 0.4 to favor decreasing the false negative results
pred <-  as.numeric(pred > 0.4)
# Create a confusion matrix of these values
confusionMatrix(factor(pred),factor(test_y))
test2 <- ecoli_235[-parts, ]
test2$PreviousTmin <- test2$PreviousTmin + 16
test_x2 <- data.matrix(test2[, -7])
pred <- predict(finalmodel, as.matrix(test_x2))
pred <-  as.numeric(pred > 0.5)
confusionMatrix(factor(pred),factor(test_y))
# Bring the data back in since it was altered above
test3 <- ecoli_235[-parts, ]
test3$Discharge_CFS <- test3$Discharge_CFS + 20
test_x3 <- data.matrix(test3[, -7])
pred <- predict(finalmodel, as.matrix(test_x3))
pred <-  as.numeric(pred > 0.5)
confusionMatrix(factor(pred),factor(test_y))
test4 <- ecoli_235[-parts, ]
test4$PreviousTmin <- test4$PreviousTmin + 20
test4$Discharge_CFS <- test4$Discharge_CFS + 40
test_x4 <- data.matrix(test4[, -7])
pred <- predict(finalmodel, as.matrix(test_x4))
pred <-  as.numeric(pred > 0.5)
confusionMatrix(factor(pred),factor(test_y))
library(tidyverse)
folds = createFolds(train_x, k = 10)
cv <- lapply(folds, function(x) {
# Use the lowest log loss from the first chunk
classifier = xgboost(data = as.matrix(train_x), label = train_y, max.depth = 5, nrounds = 65, verbose = FALSE)
y_pred = predict(classifier, newdata = as.matrix(test_x)) # again need a matrix
y_pred = (y_pred >= 0.45) # here we are setting up the binary outcome of 0 or 1
cm = table(test_y, y_pred)
accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
return(accuracy)
})
accuracy = mean(as.numeric(cv))
print(accuracy)
# This needs to be scaled.
folds = createFolds(train_x, k = 10)
cv <- lapply(folds, function(x) {
# Use the lowest log loss from the first chunk
classifier = svm(x = as.matrix(train_x), y = as.vector(train_y), type = 'C-classification', cost = 92.101, gamma=36.501)
y_pred = predict(classifier, newdata = as.matrix(test_x)) # again need a matrix
#y_pred = (y_pred >= 0.45) # here we are setting up the binary outcome of 0 or 1
cm = table(test_y, y_pred)
accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
return(accuracy)
})
accuracy = mean(as.numeric(cv))
print(accuracy)
# Create a model using the default parameters
default_params <- list(booster = "gbtree",
objective = "binary:logistic",
eta=0.3,
gamma=0,
max_depth=6,
min_child_weight=1,
subsample=1,
colsample_bytree=1)
#train_matrix <- xgb.DMatrix(train_x)
xgbcv <- xgb.cv(params = default_params,
data = train_x,
label=train_y,
nrounds = 100,
nfold = 5,
showsd = T,
stratified = T,
print_every_n = 10,
early_stopping_round = 20,
maximize = F)
xgb1 <- xgb.train(params = default_params,
data = xgb_train,
nrounds = 22,
watchlist = watchlist,
print_every_n = 10,
early_stopping_round = 5,
maximize = F,
eval_metric = "error")
xgbpred <- predict(xgb1,test_x)
xgbpred <- ifelse(xgbpred > 0.4,1,0)
confusionMatrix(factor(xgbpred),factor(test_y))
mat <- xgb.importance(feature_names = colnames(train_x),model = xgb1)
xgb.plot.importance(importance_matrix = mat[1:6])
train <- as.data.frame(train)
test <- as.data.frame(test)
#create tasks
traintask <- makeClassifTask(data = train,target = "ecoli_235")
testtask <- makeClassifTask(data = test,target = "ecoli_235")
#do one hot encoding`<br/>
traintask <- createDummyFeatures(obj = traintask)
testtask <- createDummyFeatures(obj = testtask)
lrn <- makeLearner("classif.xgboost",
predict.type = "response")
lrn$par.vals <- list(objective="binary:logistic",
eval_metric="error",
nrounds=100L,
eta=0.1)
params <- makeParamSet(#makeDiscreteParam("booster", values = c("gbtree","gblinear")),
makeIntegerParam("max_depth",lower = 3L,upper = 5L),
makeNumericParam("min_child_weight",lower = 1L,upper = 10L),
makeNumericParam("subsample",lower = 0.5,upper = 1),
makeNumericParam("colsample_bytree",lower = 0.5,upper = 1))
rdesc <- makeResampleDesc("LOO")
#rdesc <- makeResampleDesc("CV",stratify = T,iters=5L)
# Maxint is 100 but this takes a long time. Leave at 10L until further notice
ctrl <- makeTuneControlRandom(maxit = 10L)
parallelStartSocket(cpus = detectCores())
#parameter tuning
mytune <- mlr::tuneParams(learner = lrn, task = traintask, resampling = rdesc, measures = acc, par.set = params, control = ctrl, show.info = T)
mytune$y
ctrl <- makeTuneControlRandom(maxit = 40L)
mytune <- tuneParams(learner = lrn, task = traintask, resampling = rdesc, measures = acc, par.set = params, control = ctrl, show.info = T)
mytune$y
#set hyperparameters
lrn_tune <- setHyperPars(lrn,par.vals = mytune$x)
#train model
xgmodel2 <- mlr::train(learner = lrn_tune,task = traintask)
#predict model
xgpred2 <- predict(xgmodel2,testtask)
confusionMatrix(xgpred2$data$response,xgpred2$data$truth)
# The default is working better. LAURA look at this more. Until then, save the default model to work on the automation.
#tidypredict_fit(finalmodel)
xgb.save(finalmodel, "Data/Processed/XGBmodel235")
# The default is working better. LAURA look at this more. Until then, save the default model to work on the automation.
#tidypredict_fit(finalmodel)
xgb.save(finalmodel, "Data/Processed/XGBmodel235")
shiny::runApp()
shiny::runApp()
runApp('UpperSantaCruz')
runApp('UpperSantaCruz')
spatiallocs <- read_sf("Data/Processed/Attributed_Location/ecoli_UniqueLocs.shp")
View(spatiallocs)
spatiallocs <- data.frame(x = 1:5, y = c("a", "b", "c", "d", "e"))
View(spatiallocs)
spatiallocs <- read_sf("Data/Processed/Attributed_Location/ecoli_UniqueLocs.shp")
print(spatiallocs$DistCatego)
print(spatiallocs$geometry)
spatiallocs <- read_sf("Data/Processed/Attributed_Location/ecoli_UniqueLocs.shp")
runApp('UpperSantaCruz')
runApp()
library(ggplot)
library(ggplot2)
library(shiny); runApp('Capstone_Coliform_Application.R')
runApp('Capstone_Coliform_Application.R')
runApp('Capstone_Coliform_Application.R')
runApp('Capstone_Coliform_Application.R')
runApp('Capstone_Coliform_Application.R')
runApp('Capstone_Coliform_Application.R')
runApp('Capstone_Coliform_Application.R')
runApp('Capstone_Coliform_Application.R')
runApp('Capstone_Coliform_Application.R')
library(shiny); runApp('Capstone_Coliform_Application.R')
runApp('Capstone_Coliform_Application.R')
runApp('Capstone_Coliform_Application.R')
runApp('Capstone_Coliform_Application.R')
