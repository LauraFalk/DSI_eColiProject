count(StandardizedResult >126)
eColi_ForAnalysis %>%
count(StandardizedResult >235)
eColi_ForAnalysis %>%
count(StandardizedResult <1)
eColi_ForAnalysis %>%
count(is.na(StandardizedResult))
eColi_ForAnalysis %>%
count(QuantificationLimit == "Present Above Quantification Limit")
eColi_ForAnalysis %>%
count(QuantificationLimit == "Present Below Quantification Limit")
View(eColi_ForAnalysis)
ecoli_ForAttribution <- eColi_ForAnalysis %>%
select(OrganizationFormalName.x, DateAsDate, ActivityStartTime.Time, ActivityStartTime.TimeZoneCode, MonitoringLocationIdentifier,
HydrologicCondition,
HydrologicEvent,
ResultValueTypeName,
ResultStatusIdentifier,
QuantificationLimit,
StandardizedResult)
View(ecoli_SampleLimit)
library(janitor)
library(knitr)
library(tidyverse)
ecoli_Processing <-remove_empty(ecoli_SampleLimit, which = "cols")
library(janitor)
library(knitr)
library(tidyverse)
ecoli_Processing <-remove_empty(ecoli_SampleLimit, which = "cols")
#library(dplyr)
library(knitr)
library(lubridate)
opts_chunk$set(echo = TRUE)
#setwd("C:/Users/Laura/Desktop/DSI_Project/Data/")
RawFileName <- "Data/Raw/EPA_PortalQuery_Appended_SODN.csv"
#Current file uses points lassoed in arcmap. It uses any sampling locations north of inputs from both the Nogales WWTP and Sonoita Creek in case either of these are point sources of e. Coli.
LocationFileName <- "Data/Raw/EPA_PortalQuery_Locations_Appended_SODN.csv"
coliform_data <- read.csv(RawFileName)
Locations <- read.csv(LocationFileName)
ecoli_data <- coliform_data[coliform_data$CharacteristicName == "Escherichia coli", ]
ecoli_data_9223b <- ecoli_data[ecoli_data$ResultAnalyticalMethod.MethodName == "E coli, water, Colilert (24-28)"|
ecoli_data$ResultAnalyticalMethod.MethodName ==  "COLILERT (EDBERG)"|
ecoli_data$ResultAnalyticalMethod.MethodName ==  "COLILERT"|
ecoli_data$ResultAnalyticalMethod.MethodName ==  "SM 9223B", ]
ecoli_data_9223b$DateAsDate <- mdy(ecoli_data_9223b$ActivityStartDate,tz="America/Phoenix")
ecoli_data_9223b$DateAsDate <-format(as.POSIXct(ecoli_data_9223b$DateAsDate),format='%Y-%m-%d')
ecoli_data_9223b_date <- ecoli_data_9223b[ecoli_data_9223b$DateAsDate >= "2009-07-01", ]
ecoli_SampleLimit<- merge(ecoli_data_9223b_date, Locations, by = c("MonitoringLocationIdentifier"))
remove(coliform_data, ecoli_data, ecoli_data_9223b, ecoli_data_9223b_date, Locations, LocationFileName, RawFileName)
library(janitor)
library(knitr)
library(tidyverse)
ecoli_Processing <-remove_empty(ecoli_SampleLimit, which = "cols")
ecoli_Processing <- keep(ecoli_Processing, ~n_distinct(.) > 1)
summary(ecoli_Processing)
ecoli_Processing <- ecoli_Processing %>%
subset(select = -c(OrganizationIdentifier.x, # data contained in another column
ActivityDepthHeightMeasure.MeasureValue, # all 0 or NA
ActivityDepthHeightMeasure.MeasureUnitCode, # associated column is being removed
ActivityTopDepthHeightMeasure.MeasureValue, # all 0 or NA
ActivityTopDepthHeightMeasure.MeasureUnitCode, # associated column is being removed
ActivityBottomDepthHeightMeasure.MeasureValue, # all 0 or NA
ActivityBottomDepthHeightMeasure.MeasureUnitCode, # associated column is being removed
USGSPCode, # all 50468 or NA
ResultDepthHeightMeasure.MeasureValue, # all 0 or NA
HUCEightDigitCode)) # all 15050301 or NA
ecoli_Processing$QuantificationLimit <- ifelse(ecoli_Processing$ResultMeasureValue > 2419 | ecoli_Processing$ResultDetectionConditionText == "Present Above Quantification Limit", "Present Above Quantification Limit", "Value")
ecoli_Processing$QuantificationLimit <- ifelse(ecoli_Processing$ResultDetectionConditionText == "Present Below Quantification Limit"|ecoli_Processing$ResultDetectionConditionText == "Not Detected", "Present Below Quantification Limit", ecoli_Processing$QuantificationLimit)
ecoli_SamplesOnly <- ecoli_Processing %>%
filter(ActivityTypeCode != "Quality Control Sample-Field Replicate" & ActivityTypeCode !="Quality Control Sample-Equipment Blank")
# this is just a check to ensure that this leads me to the same count as the samples only table.
#ecoli_Processing %>%
#count(ActivityTypeCode == "Sample-Routine")
eColi_ForAnalysis <- ecoli_SamplesOnly
eColi_ForAnalysis$StandardizedResult <- eColi_ForAnalysis$ResultMeasureValue
eColi_ForAnalysis$StandardizedResult <- ifelse(eColi_ForAnalysis$QuantificationLimit == "Present Above Quantification Limit", 2420, eColi_ForAnalysis$StandardizedResult)
eColi_ForAnalysis$StandardizedResult <- ifelse(eColi_ForAnalysis$QuantificationLimit == "Present Below Quantification Limit",0, eColi_ForAnalysis$StandardizedResult)
eColi_ForAnalysis %>%
count(StandardizedResult == 2420)
# I currently have 431 maximum results.
eColi_ForAnalysis %>%
count(StandardizedResult >126)
#772 are greater than the single standard EPA guideline (126 MPN)
eColi_ForAnalysis %>%
count(StandardizedResult >235)
#664 samples are greater than 235, my normal comparison for warm effluent water.
eColi_ForAnalysis %>%
count(StandardizedResult <1)
#There are only 3 non-detect
eColi_ForAnalysis %>%
count(QuantificationLimit == "Present Below Quantification Limit")
#Verified that there are only 3 non-detect
eColi_ForAnalysis %>%
count(is.na(StandardizedResult))
# There are no NA values
ecoli_ForAttribution <- eColi_ForAnalysis %>%
select(OrganizationFormalName.x, DateAsDate, ActivityStartTime.Time, ActivityStartTime.TimeZoneCode, MonitoringLocationIdentifier,
HydrologicCondition,
HydrologicEvent,
ResultValueTypeName,
ResultStatusIdentifier,
QuantificationLimit,
StandardizedResult)
View(ecoli_ForAttribution)
ecoli_ForAttribution <- eColi_ForAnalysis %>%
select(DateAsDate, ActivityStartTime.Time, ActivityStartTime.TimeZoneCode, MonitoringLocationIdentifier,
HydrologicCondition,
HydrologicEvent,
ResultValueTypeName,
ResultStatusIdentifier,
QuantificationLimit,
StandardizedResult)
write.csv(ecoli_ForAttribution, "C:/Users/Laura/Desktop?DSI_ProjectGIT/ecoli_ForAttribution_20220907")
write.csv(ecoli_ForAttribution, "C:/Users/Laura/Desktop/DSI_ProjectGIT/ecoli_ForAttribution_20220907")
remove(ecoli_Processing, ecoli_SampleLimit, ecoli_SamplesOnly, eColi_ForAnalysis)
View(ecoli_ForAttribution)
library(knitr)
opts_chunk$set(echo = TRUE)
ecoli_ForAttribution %>%
filter(is.na(HydrologicCondition))
library(tidyverse)
ecoli_ForAttribution %>%
filter(is.na(HydrologicCondition))
ecoli_ForAttribution %>%
count(is.na(HydrologicCondition))
sum(is.na(ecoli_ForAttribution$HydrologicCondition))
sum(is.null(ecoli_ForAttribution$HydrologicCondition))
ecoli_ForAttribution %>%
count(is.null(HydrologicCondition))
ecoli_ForAttribution %>%
count((HydrologicCondition == ""))
ecoli_ForAttribution %>%
count((HydrologicEvent == ""))
ecoli_ForAttribution %>%
count((HydrologicCondition == ""))
ecoli_ForAttribution %>%
count((HydrologicEvent == ""))
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
site_id <- '09481740'
startDate = "2009-07-01"
#End date is 8/30/2022 to avoid null data at this time (9/5/2022)
endDate = "2022-08-30"
tz="America/Phoenix"
#creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
USGS_Standardized <- USGS_Raw
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# remove the pre-rounding dateTime column
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime))
# This shows any duplicate dates. There are a few because the data was not collected on the standard 15-minute interval. The separate values were rounded to the same one.
USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime
USGS_Standardized_RmDupls <- USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_RmDupl<- merge(x=USGS_Standardized,y=USGS_Standardized_RmDupls, by="DateTime2",all.x=TRUE, all.y = FALSE)
setnames(USGS_RmDupl,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
base::setnames(USGS_RmDupl,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
library(data.table)
# remove the pre-rounding dateTime column
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime))
# This shows any duplicate dates. There are a few because the data was not collected on the standard 15-minute interval. The separate values were rounded to the same one.
USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime
USGS_Standardized_RmDupls <- USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_RmDupl<- merge(x=USGS_Standardized,y=USGS_Standardized_RmDupls, by="DateTime2",all.x=TRUE, all.y = FALSE)
setnames(USGS_RmDupl,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
#Remove the pre-averaging flow column
USGS_RmDupl <- USGS_RmDupl %>%
subset(select = -c(X_00060_00000))
# Removing duplicates from the fixed times and fixed cfs
USGS_RmDupl<- USGS_RmDupl %>%
distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)
#Re-check
USGS_RmDupl %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# Create the time series, formatted as posixt
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))
colnames(TimeSeries)[1] = "DateTime"
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime, format = '%Y-%m-%d %H:%M'), tz = tz)
#Join data with time series, sort by DateTime2
# could also use left_join here. Issues were from duplicate values from the rounding merge.
USGS_TimeSeries <- merge(x=TimeSeries,y=USGS_RmDupl,by="DateTime2",all.x=TRUE, all.y = FALSE)
count(USGS_TimeSeries %>%
filter(is.na(Discharge_CFS)))
USGS_TimeSeries$date <- as_date(USGS_TimeSeries$DateTime2)
USGS_TimeSeries$count <- 1
TimeFlag <- USGS_TimeSeries %>%
filter(!is.na(Discharge_CFS)) %>%
group_by(date) %>%
summarize(value = sum(count))
USGS_TimeSeries <- right_join(TimeFlag, USGS_TimeSeries)
setnames(USGS_TimeSeries, "value", "CountUSGSbyDate")
USGS_TimeSeries_Fill <-
USGS_TimeSeries %>%
filter(CountUSGSbyDate>88)
USGS_TimeSeries_Fill$Filled <-  na.approx(USGS_TimeSeries_Fill$Discharge_CFS, rule = 2)
USGS_ForAnalysis <- right_join(USGS_TimeSeries_Fill,USGS_TimeSeries)
USGS_ForAnalysis2 <- USGS_ForAnalysis
USGS_ForAnalysis2$Filled<- ifelse(is.na(USGS_ForAnalysis$Discharge_CFS) == TRUE,USGS_ForAnalysis$Filled,USGS_ForAnalysis$Discharge_CFS)
count(USGS_ForAnalysis2 %>%
filter(is.na(Filled)))
USGS_ForAnalysis2 <-  USGS_ForAnalysis2 %>%
subset(select = -c(date, count, Discharge_CFS))
setnames(USGS_ForAnalysis2,old=c("Filled"),new=c("Discharge_CFS"))
remove(TimeFlag, TimeSeries, USGS_TimeSeries, USGS_Raw, USGS_Standardized, USGS_TimeSeries_Fill, USGS_ForAnalysis, USGS_RmDupl, USGS_RmDupls, USGS_Standardized_RmDupls, view)
ecoli_ForAttribution$DateTime2 <- as.POSIXct(format(paste(ecoli_ForAttribution$DateAsDate,ecoli_ForAttribution$ActivityStartTime.Time), format = '%Y-%m-%d %H:%M'), tz = tz)
View(ecoli_ForAttribution)
storet_USGS_merge<- merge(ecoli_ForAttribution,
USGS_ForAnalysis2, by = c("DateTime2"))
storet_USGS_merge<- left_merge(ecoli_ForAttribution,
USGS_ForAnalysis2, by = c("DateTime2"))
storet_USGS_merge<- left_join(ecoli_ForAttribution,
USGS_ForAnalysis2, by = c("DateTime2"))
View(storet_USGS_merge)
ecoli_ForAttribution$DateTime2 <-
round_date(as.POSIXct(ecoli_ForAttribution$DateTime2), "15 minutes")
storet_USGS_merge<- left_join(ecoli_ForAttribution,
USGS_ForAnalysis2, by = c("DateTime2"))
Storet_USGS_Merge<- left_join(ecoli_ForAttribution,
USGS_ForAnalysis2, by = c("DateTime2"))
Storet_USGS_Merge %<%
count(filter(is.na(Discharge_CFS)))
library(knitr)
library(tidyverse)
opts_chunk$set(echo = TRUE)
Storet_USGS_Merge %>%
count(filter(is.na(Discharge_CFS)))
Storet_USGS_Merge %>%
count(is.na(Discharge_CFS))
# 75 records are still without discharge. this is the one I care the most about.
remove(storet_USGS_merge)
Storet_USGS_Merge %>%
filter(is.na(Discharge_CFS))
View(Storet_USGS_Merge)
View(USGS_ForAnalysis2)
USGS_ForAnalysis2 %>%
filter(DateTime2 == 4/27/2020)
USGS_ForAnalysis2 %>%
filter(Date < 4/27/2020)
USGS_ForAnalysis2 %>%
filter(DateTime2 < 4/27/2020)
USGS_ForAnalysis2 %>%
filter(DateTime2 < 4/27/2020 00:00)
USGS_ForAnalysis2 %>%
filter(DateTime2 < 4/27/2020 12:00)
USGS_ForAnalysis2 %>%
filter(DateTime2 < as.POSIXct(4/27/2020))
USGS_ForAnalysis2 %>%
filter(DateTime2 < as.POSIXct(4/27/2020))
USGS_ForAnalysis2 %>%
filter(DateTime2 < as.POSIXct(4/27/2020 00:00))
USGS_ForAnalysis2 %>%
filter(DateTime2 < as.POSIXct(4/27/2020 00:00, tz = tz))
USGS_ForAnalysis2 %>%
filter(DateTime2 < as.POSIXct(4/27/2020, tz = tz))
Storet_USGS_Merge %>%
filter(DateTime2 < as.POSIXct(4/27/2020, tz = tz))
Storet_USGS_Merge %>%
filter(DateTime2 < as.POSIXct(2020-04-07, tz = tz))
Storet_USGS_Merge %>%
filter(DateTime2 < as.POSIXct(2020-04-07))
Storet_USGS_Merge %>%
filter(DateTime2 < as.POSIXct('2020-04-07'))
Storet_USGS_Merge %>%
filter(DateTime2 < as.POSIXct('2020-04-07')) %>%
order_by(DateAsDate)
Storet_USGS_Merge %>%
filter(DateTime2 < as.POSIXct('2020-04-07')) %>%
sort(DateAsDate)
Storet_USGS_Merge %>%
filter(DateTime2 < as.POSIXct('2020-04-07')) %>%
sort(DateTime2)
Storet_USGS_Merge %>%
filter(DateTime2 < as.POSIXct('2020-04-07')) %>%
Storet_USGS_Merge %>%
filter(DateTime2 < as.POSIXct('2020-04-07'))
```{r}
Storet_USGS_Merge %>%
filter(DateTime2 < as.POSIXct('2020-04-07'))
test <- Storet_USGS_Merge %>%
filter(DateTime2 < as.POSIXct('2020-04-07'))
View(test)
test <- Storet_USGS_Merge %>%
filter(DateTime2 <= as.POSIXct('2020-04-07'))
test <- USGS_ForAnalysis2 %>%
filter(DateTime2 <= as.POSIXct('2020-04-07'))
test <- USGS_ForAnalysis2 %>%
filter(DateTime2 < as.POSIXct('2020-04-08'))
View(Storet_USGS_Merge)
View(test)
View(test)
test <- USGS_ForAnalysis2 %>%
filter(DateTime2 <= as.POSIXct('2020-04-27'))
test <- USGS_ForAnalysis2 %>%
filter(DateTime2 < as.POSIXct('2020-04-28'))
Storet_USGS_Merge_NoNull <- Storet_USGS_Merge %>%
filter(!is.na(Discharge_CFS))
Storet_USGS_Merge_NoNull %<%
ggplot(mtcars, aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point()
Storet_USGS_Merge_NoNull %>%
ggplot(mtcars, aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point()
Storet_USGS_Merge_NoNull %>%
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point()
Storet_USGS_Merge_NoNull %>%
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point()+
geom_hline(yintercept=126, linetype="dashed",
color = "red", size=2)
Storet_USGS_Merge_NoNull %>%
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point()+
geom_hline(yintercept=126, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Merge_NoNull %>%
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point()+
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2)+
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Merge_NoNull %>%
filter(Discharge_CFS < 250)
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point()+
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2)+
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Merge_NoNull %>%
filter(Discharge_CFS < 250) +
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Merge_NoNull %>%
filter(Discharge_CFS < 250) %>%
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Merge_NoNull %>%
filter(Discharge_CFS < 150) %>%
ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats")
read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv")
read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv")
library(knitr)
library(lubridate)
library(tidyverse)
opts_chunk$set(echo = TRUE)
DailyClimate <- read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv")
DailyClimate$DateAsDate <- as.POSIXct(format(DailyClimate$Date, format = '%Y-%m-%d'), tz = tz)
DailyClimate$DateAsDate <- as.POSIXct(format(DailyClimate$Date, format = '%d/%m/%Y'), tz = tz)
View(DailyClimate)
DailyClimate$DateAsDate <- as.POSIXct(format(DailyClimate$Date, format = '%Y-%m-%d'), tz = tz)
DailyClimate <- read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv", skip = 6)
View(DailyClimate)
DailyClimate <- read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv", skip = 3)
View(DailyClimate)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-6,-1)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-8,-1)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-9,-1)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-10,-1)
as.POSIXlt(DailyClimate$DateAsDate, tz = tz)
DailyClimate <- head(DailyClimate, - 1)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-10,-1)
as.POSIXlt(DailyClimate$DateAsDate, tz = tz)
DailyClimate$DateAsDate <-as.POSIXct(DailyClimate$DateAsDate, format = "%mm/%dd/%YYYY", tz = tz)
Storet_USGS_Climate_Merge<- left_join(Storet_USGS_Merge_NoNull,
DailyClimate, by = c("DateAsDate"))
Storet_USGS_Merge_NoNull$DateAsDate <-as.POSIXct(Storet_USGS_Merge_NoNull$DateAsDate, format = "%Y-%m-%d", tz = tz)
Storet_USGS_Climate_Merge<- left_join(Storet_USGS_Merge_NoNull,
DailyClimate, by = c("DateAsDate"))
Storet_USGS_Climate_Merge %>%
filter(!is.na(DailyClimate))
Storet_USGS_Climate_Merge %>%
filter(!is.na(Tmax..F.))
Storet_USGS_Climate_Merge %>%
filter(!is.null(Tmax..F.))
DailyClimate$DateAsDate <-as.POSIXct(DailyClimate$DateAsDate, format = "%m/%d/%Y", tz = tz)
DailyClimate <- read.csv("Data/Raw/ClimateAnalyzer_DailySummaryStats.csv", skip = 3)
DailyClimate <- head(DailyClimate, - 1)
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-10,-1)
DailyClimate$DateAsDate <-as.POSIXct(DailyClimate$DateAsDate, format = "%m/%d/%Y", tz = tz)
Storet_USGS_Merge_NoNull$DateAsDate <-as.POSIXct(Storet_USGS_Merge_NoNull$DateAsDate, format = "%Y-%m-%d", tz = tz)
Storet_USGS_Climate_Merge<- left_join(Storet_USGS_Merge_NoNull,
DailyClimate, by = c("DateAsDate"))
Storet_USGS_Climate_Merge %>%
filter(!is.null(Tmax..F.))
Storet_USGS_Climate_Merge %>%
filter(is.null(Tmax..F.))
View(Storet_USGS_Climate_Merge)
Storet_USGS_Climate_Merge %>%
filter(Tmax..F. == NaN)
Storet_USGS_Climate_Merge %>%
filter(is.nan(Tmax..F.))
Storet_USGS_Climate_Merge %>%
filter(!is.nan(Tmax..F.)) %>%
ggplot(aes(x=Tmax..F., y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge %>%
filter(!is.nan(Tmax..F.)) %>%
ggplot(aes(x=Tmax..F., y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge %>%
filter(!is.nan(Tmin..F.)) %>%
ggplot(aes(x=Tmin..F., y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge %>%
filter(!is.nan(Precipitation..in.)) %>%
ggplot(aes(x=Precipitation..in., y=StandardizedResult)) +
geom_point() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge$Season <- as.yearmon(DateAsDate)
Storet_USGS_Climate_Merge$Season <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
View(Storet_USGS_Climate_Merge)
Storet_USGS_Climate_Merge$Season <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season %in% c('May','June','July'), "SUMMER",
ifelse (Storet_USGS_Climate_Merge$Season %in% c('August','September','October'), "AUTUMN",
ifelse (Storet_USGS_Climate_Merge$Season %in% c('November','December','January'),
"WINTER", "SPRING")))
Storet_USGS_Climate_Merge$Season <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season %in% c('October','November','December'), "Fall",
ifelse (Storet_USGS_Climate_Merge$Season %in% c('January','February','March'), "Winter",
ifelse (Storet_USGS_Climate_Merge$Season %in% c('April','May','June'),
"Spring", "Monsoon")))
Storet_USGS_Climate_Merge %>%
ggplot(aes(x=Season, y=StandardizedResult)) +
geom_bar() +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge %>%
ggplot(aes(x=Season, y=StandardizedResult)) +
geom_bar(stat='identity') +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge$Season1 <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 %in% c('October','November','December'), "Fall",
ifelse (Storet_USGS_Climate_Merge$Season1 %in% c('January','February','March'), "Winter",
ifelse (Storet_USGS_Climate_Merge$Season1 %in% c('April','May','June'),
"Spring", "Monsoon")))
Storet_USGS_Climate_Merge$Season1 <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 %in% c('Oct','Nov','Dec'), "Fall",
ifelse (Storet_USGS_Climate_Merge$Season1 %in% c('Jan','Feb','Mar'), "Winter",
ifelse (Storet_USGS_Climate_Merge$Season1 %in% c('Apr','May','Jun'),
"Spring", "Monsoon")))
Storet_USGS_Climate_Merge$Season1 <- str_sub(Storet_USGS_Climate_Merge$Season,1,3)
Storet_USGS_Climate_Merge$Season1 <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season1 <- str_sub(Storet_USGS_Climate_Merge$Season1,1,3)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 == c('Oct','Nov','Dec'), "Fall","Monsoon")
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 == c('Oct','Nov','Dec'), "Fall","Monsoon")
Storet_USGS_Climate_Merge$Season1 <- as.yearmon(Storet_USGS_Climate_Merge$DateAsDate)
Storet_USGS_Climate_Merge$Season1 <- str_sub(Storet_USGS_Climate_Merge$Season1,1,3)
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 == c('Oct','Nov','Dec'), "Fall", ifelse (Storet_USGS_Climate_Merge$Season1 == c('Jan','Feb','Mar'), "Winter",ifelse (Storet_USGS_Climate_Merge$Season1 == c('Apr','May','Jun'),
"Spring", "Monsoon")))
Storet_USGS_Climate_Merge$Season1 <- as.numeric(str_sub(Storet_USGS_Climate_Merge$DateAsDate,6,7))
View(Storet_USGS_Climate_Merge)
View(Storet_USGS_Climate_Merge)
Storet_USGS_Climate_Merge$Season1 <- as.numeric(str_sub(Storet_USGS_Climate_Merge$DateAsDate,6,7))
Storet_USGS_Climate_Merge$Season <- ifelse(Storet_USGS_Climate_Merge$Season1 > 9 & Storet_USGS_Climate_Merge$Season1 < 13, "Fall",
ifelse (Storet_USGS_Climate_Merge$Season1 > 0 & Storet_USGS_Climate_Merge$Season1 < 4, "Winter",
ifelse (Storet_USGS_Climate_Merge$Season1 > 3 & Storet_USGS_Climate_Merge$Season1 < 7,
"Spring", "Monsoon")))
Storet_USGS_Climate_Merge %>%
ggplot(aes(x=Season, y=StandardizedResult)) +
geom_bar(stat='identity') +
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
