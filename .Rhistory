geom_point(aes(x=Previous3Precip, y=StandardizedResult), color = "red") +
geom_point(aes(x=Previous7Precip, y=StandardizedResult), color = "blue")+
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge$Previous3Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 3, fill = NA)
Storet_USGS_Climate_Merge$Previous30Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 3, fill = NA)
Storet_USGS_Climate_Merge %>%
ggplot() +
geom_point(aes(x=Previous3Precip, y=StandardizedResult), color = "red") +
geom_point(aes(x=Previous30Precip, y=StandardizedResult), color = "blue")+
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
View(Storet_USGS_Climate_Merge)
Storet_USGS_Climate_Merge$Previous3Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 3, fill = NA)
Storet_USGS_Climate_Merge$Previous7Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 7, fill = NA)
Storet_USGS_Climate_Merge %>%
ggplot() +
geom_point(aes(x=Previous3Precip, y=StandardizedResult), color = "red") +
geom_point(aes(x=Previous7Precip, y=StandardizedResult), color = "blue")+
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge$Previous3Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 3, fill = NA)
Storet_USGS_Climate_Merge$Previous7Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 7, fill = NA)
Storet_USGS_Climate_Merge$Previous30Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 30, fill = NA)
Storet_USGS_Climate_Merge %>%
ggplot() +
geom_point(aes(x=Previous3Precip, y=StandardizedResult), color = "red") +
geom_point(aes(x=Previous7Precip, y=StandardizedResult), color = "blue")+
geom_point(aes(x=Previous30Precip, y=StandardizedResult), color = "green")+
geom_hline(yintercept=126, linetype="dashed",
color = "blue", size=0.2) +
geom_hline(yintercept=235, linetype="dashed",
color = "red", size=0.2)
Storet_USGS_Climate_Merge$SeasonNumeric <- ifelse(Storet_USGS_Climate_Merge$Season == "Fall", 1,
ifelse(Storet_USGS_Climate_Merge$Season == "Winter", 2,
ifelse (Storet_USGS_Climate_Merge$Season =="Spring", 3,4)))
View(Storet_USGS_Climate_Merge)
library(corrplot)
ForCorrplot <- Storet_USGS_Climate_Merge %>%
select(StandardizedResult, Disharge_CFS)
ForCorrplot <- Storet_USGS_Climate_Merge %>%
select(StandardizedResult, Discharge_CFS, Precipitation..in., Tmax..F., Tmin..F., PreviousPrecip, Previous3Precip, Previous7Precip, Previous30Precip, SeasonNumeric)
corrplot(ForCorrplot)
cor(ForCorrplot)
corrplot(R1)
R1 <- cor(ForCorrplot)
corrplot(R1)
View(R1)
R1 <- cor(as.numeric(ForCorrplot))
sapply(ForCorrplot, class)
View(ForCorrplot)
Storet_USGS_Climate_Merge$Previous3Precip <- ifelse(is.NAN(Storet_USGS_Climate_Merge$Previous3Precip) | is.na(Storet_USGS_Climate_Merge$Previous3Precip), 0, Storet_USGS_Climate_Merge$Previous3Precip)
Storet_USGS_Climate_Merge$Previous3Precip <- ifelse(is.nan(Storet_USGS_Climate_Merge$Previous3Precip) | is.na(Storet_USGS_Climate_Merge$Previous3Precip), 0, Storet_USGS_Climate_Merge$Previous3Precip)
# Fix the nulls in precip data.
Storet_USGS_Climate_Merge$Previous3Precip[is.na(Storet_USGS_Climate_Merge$Previous3Precip)] <- 0
Storet_USGS_Climate_Merge$Previous3Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 3, fill = NA)
Storet_USGS_Climate_Merge$Previous7Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 7, fill = NA)
Storet_USGS_Climate_Merge$Previous30Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 30, fill = NA)
Storet_USGS_Climate_Merge$PreviousPrecip[is.na(Storet_USGS_Climate_Merge$PreviousPrecip[1:10])]
Storet_USGS_Climate_Merge$PreviousPrecip[is.na(Storet_USGS_Climate_Merge$PreviousPrecip[1:10])] <- 0
Storet_USGS_Climate_Merge$Previous3Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 3, fill = NA)
Storet_USGS_Climate_Merge$Previous7Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 7, fill = NA)
Storet_USGS_Climate_Merge$Previous30Precip <-  rollsumr(Storet_USGS_Climate_Merge$Precipitation..in., k = 30, fill = NA)
View(DailyClimate)
DailyClimate$Previous3Precip <-  rollsumr(DailyClimate$Precipitation..in., k = 3, fill = NA)
DailyClimate$Previous7Precip <-  rollsumr(DailyClimate$Precipitation..in., k = 7, fill = NA)
DailyClimate$Previous30Precip <-  rollsumr(DailyClimate$Precipitation..in., k = 30, fill = NA)
library(data.table)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
#Inputs formatted for read NWIS function.
site_id <- '09481740'
startDate = "2009-07-01"
endDate = "2022-08-30"
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
#creates table in R
USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
library(data.table)
library(knitr)
library(lubridate)
library(tidyverse)
library(waterData)
library(zoo)
opts_chunk$set(echo = TRUE)
#Inputs formatted for read NWIS function.
site_id <- '09481740'
startDate = "2009-07-01"
endDate = "2022-08-30"
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
#creates table in R
USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
library(data.table)
library(dataRetrieval)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
#Inputs formatted for read NWIS function.
site_id <- '09481740'
startDate = "2009-07-01"
endDate = "2022-08-30"
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
#creates table in R
USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
#This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
View(USGS_Raw)
# Create a New Dataframe to edit
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# remove the pre-rounding dateTime column, as this will affect the duplicate record determination
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime))
# This shows any duplicate dates. There are a few because the data was not collected on the standard 15-minute interval. The separate values were rounded to the same one.
USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime
USGS_Standardized_RmDupls <- USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
View(USGS_Standardized_RmDupls)
# This summarizes the flow column by the rounded DateTime.
USGS_Standardized_append <- USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# This summarizes the flow column by the rounded DateTime.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
View(USGS_Standardized_append)
USGS_Standardized_RmDupls <- USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(stddev(X_00060_00000))
# This summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
View(USGS_Standardized_append)
View(USGS_Standardized_append)
# Rename the column to something more useful.
setnames(USGS_Standardized_append,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_RmDupls, by="DateTime2",all.x=TRUE, all.y = FALSE)
View(USGS_Standardized)
#Remove the pre-averaging flow column
USGS_Standardized <- USGS_Standardized %>%
subset(select = -c(X_00060_00000))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_RmDupls, by="DateTime2",all.x=TRUE, all.y = FALSE)
library(data.table)
library(dataRetrieval)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
#tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
# This summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Rename the column to something more useful.
setnames(USGS_Standardized_append,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_RmDupls, by="DateTime2",all.x=TRUE, all.y = FALSE)
# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_append, by="DateTime2",all.x=TRUE, all.y = FALSE)
#Remove the pre-averaging flow column
USGS_Standardized <- USGS_Standardized %>%
subset(select = -c(X_00060_00000))
#Remove the pre-averaging flow column
USGS_Standardized <- USGS_Standardized %>%
subset(select = -c(X_00060_00000)) %>%
distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)
# This summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
library(data.table)
library(dataRetrieval)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
#tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Rename the column to something more useful.
setnames(USGS_Standardized_append,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_append, by="DateTime2",all.x=TRUE, all.y = FALSE)
#Remove the pre-averaging flow column
USGS_Standardized <- USGS_Standardized %>%
subset(select = -c(X_00060_00000)) %>%
distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)
#Re-check
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# Remove append table - data has been merged
remove(USGS_Standardized_append)
# Create the time series, formatted as POSIXct
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))
# Rename the Column DateTime2 to indicate properly formatted column.
colnames(TimeSeries)[1] = "DateTime2"
# Follow through on the name and format this bad boy.
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
#Left join data with time series, sort by DateTime2
USGS_Standardized <- merge(x=TimeSeries,y=USGS_Standardized,by="DateTime2",all.x=TRUE, all.y = FALSE)
View(USGS_Standardized)
count(USGS_Standardized %>%
filter(is.na(Discharge_CFS)))
# This column is a dummy column which allows me to count more easily in the next block.
USGS_Standardized$date <- as_date(USGS_Standardized$DateTime2)
USGS_Standardized$count <- 1
# This counts the number of readings per day
TimeFlag <- USGS_Standardized %>%
filter(!is.na(Discharge_CFS)) %>%
group_by(date) %>%
summarize(value = sum(count))
# Append the reading count value to the standardized data
USGS_Standardized <- right_join(TimeFlag, USGS_Standardized)
setnames(USGS_Standardized, "dummy", "CountUSGSbyDate")
# Name this something meaningful
setnames(USGS_Standardized, "value", "dummyValue")
View(USGS_Standardized)
# Remove the created time series. Data has been merged
remove(TimeSeries)
View(TimeFlag)
# This counts the number of readings per day
TimeFlag <- USGS_Standardized %>%
filter(!is.na(Discharge_CFS)) %>%
group_by(date) %>%
summarize(value = sum(count))
View(TimeFlag)
library(data.table)
library(dataRetrieval)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Rename the column to something more useful.
setnames(USGS_Standardized_append,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_append, by="DateTime2",all.x=TRUE, all.y = FALSE)
#Remove the pre-averaging flow column. This allows to remove duplicates from the append pairs created above.
USGS_Standardized <- USGS_Standardized %>%
subset(select = -c(X_00060_00000)) %>%
distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)
# Re-check for duplicate values. This should equal zero.
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# Remove append table - data has been merged
remove(USGS_Standardized_append)
# Create the time series, formatted as POSIXct
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))
# Rename the Column DateTime2 to indicate properly formatted column.
colnames(TimeSeries)[1] = "DateTime2"
# Follow through on the name and format this bad boy.
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
#Left join data with time series, sort by DateTime2
USGS_Standardized <- merge(x=TimeSeries,y=USGS_Standardized,by="DateTime2",all.x=TRUE, all.y = FALSE)
# Remove the created time series. Data has been merged
remove(TimeSeries)
# This tests to see how many nulls are present in the current dataset.
# 9/5/2022: 40544 records are null
count(USGS_Standardized %>%
filter(is.na(Discharge_CFS)))
View(USGS_Standardized)
USGS_Standardized$date <- as_date(USGS_Standardized$DateTime2)
USGS_Standardized$count <- 1
# This counts the number of readings per day
TimeFlag <- USGS_Standardized %>%
filter(!is.na(Discharge_CFS)) %>%
group_by(date) %>%
summarize(value = sum(count))
# Append the reading count value to the standardized data
USGS_Standardized <- right_join(TimeFlag, USGS_Standardized)
View(USGS_Standardized)
View(TimeFlag)
View(TimeFlag)
View(TimeFlag)
View(USGS_Standardized)
# Name this something meaningful
setnames(USGS_Standardized, "value", "DailyFlowCount")
# Name this something meaningful
setnames(USGS_Standardized, "value", "DailyFlowCount")
View(USGS_Standardized)
# Name this something meaningful
setnames(USGS_Standardized, c("value", "X_00060_00000_cd"), c("DailyFlowCount", "Discharge_Rank"), skip_absent = TRUE))
# Name this something meaningful
setnames(USGS_Standardized, c("value", "X_00060_00000_cd"), c("DailyFlowCount", "Discharge_Rank"), skip_absent = TRUE)
# Name this something meaningful
setnames(USGS_Standardized, c("value", "X_00060_00000_cd", "X_00065_00000", "X_00065_00000_cd"), c("DailyFlowCount", "Discharge_cd", "USGS_Staff", "USGS_Staff_cd"), skip_absent = TRUE)
# Remove extraneous columns
USGS_Standardized <- USGS_Standardized %>%
select(-X, -agency_cd, -site_no, -count)
# Remove extraneous columns
USGS_Standardized <- USGS_Standardized %>%
select(-X, -agency_cd, -site_no, -count, -dateTime)
library(data.table)
library(dataRetrieval)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"
# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)
# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw
# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Rename the column to something more useful.
setnames(USGS_Standardized_append,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_append, by="DateTime2",all.x=TRUE, all.y = FALSE)
#Remove the pre-averaging flow column. This allows to remove duplicates from the append pairs created above.
USGS_Standardized <- USGS_Standardized %>%
subset(select = -c(X_00060_00000)) %>%
distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)
# Re-check for duplicate values. This should equal zero.
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# Remove append table - data has been merged
remove(USGS_Standardized_append)
# Create the time series, formatted as POSIXct
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))
# Rename the Column DateTime2 to indicate properly formatted column.
colnames(TimeSeries)[1] = "DateTime2"
# Follow through on the name and format this bad boy.
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
#Left join data with time series, sort by DateTime2
USGS_Standardized <- merge(x=TimeSeries,y=USGS_Standardized,by="DateTime2",all.x=TRUE, all.y = FALSE)
# Remove the created time series. Data has been merged
remove(TimeSeries)
# This tests to see how many nulls are present in the current dataset.
# 9/5/2022: 40544 records are null
count(USGS_Standardized %>%
filter(is.na(Discharge_CFS)))
# This column is a dummy column which allows me to count more easily in the next block.
USGS_Standardized$date <- as_date(USGS_Standardized$DateTime2)
USGS_Standardized$count <- 1
# This counts the number of readings per day
TimeFlag <- USGS_Standardized %>%
filter(!is.na(Discharge_CFS)) %>%
group_by(date) %>%
summarize(value = sum(count))
# Append the reading count value to the standardized data
USGS_Standardized <- right_join(TimeFlag, USGS_Standardized)
# Name this something meaningful
setnames(USGS_Standardized, c("value", "X_00060_00000_cd", "X_00065_00000", "X_00065_00000_cd"), c("DailyFlowCount", "Discharge_cd", "USGS_Staff", "USGS_Staff_cd"), skip_absent = TRUE)
# Remove extraneous columns and reorder
USGS_Standardized <- USGS_Standardized %>%
select(DateTime2, date, Discharge_CFS, USGS_Staff, Discharge_cd, tz_cd, DailyFlowCount)
# Remove Timeflag table. Merged into data
remove(TimeFlag)
USGS_Standardized_Fill <-
USGS_Standardized %>%
filter(DailyFlowCount>88)
USGS_Standardized_Fill$Filled <-  na.approx(USGS_Standardized_Fill$Discharge_CFS, rule = 2)
USGS_Standardized_Fill <-
USGS_Standardized %>%
filter(DailyFlowCount>88)
USGS_Standardized$Filled <-  na.approx(USGS_Standardized_Fill$Discharge_CFS, rule = 2)
USGS_Standardized_Fill <-
USGS_Standardized %>%
filter(DailyFlowCount>88)
USGS_Standardized_Fill$Filled <-  na.approx(USGS_Standardized_Fill$Discharge_CFS, rule = 2)
USGS_Standardized <- right_join(USGS_TimeSeries_Fill,USGS_Standardized)
USGS_Standardized_Fill <-
USGS_Standardized %>%
filter(DailyFlowCount>88)
USGS_Standardized_Fill$Filled <-  na.approx(USGS_Standardized_Fill$Discharge_CFS, rule = 2)
USGS_Standardized <- right_join(USGS_Standardized_Fill,USGS_Standardized)
#Filter for columns with a daily flow count greater than 88 (no more than 2 hours per day missing.)
USGS_Standardized_Fill <-
USGS_Standardized %>%
filter(DailyFlowCount>88)
# Fill in missing data
USGS_Standardized_Fill$Filled <-  na.approx(USGS_Standardized_Fill$Discharge_CFS, rule = 2)
#Join back in
USGS_Standardized <- right_join(USGS_Standardized_Fill,USGS_Standardized)
# Remove the old data
remove(USGS_Standardized_Fill)
View(USGS_Standardized)
count(USGS_Standardized %>%
filter(is.na(Filled)))
# Fill in the un-filled values
USGS_Standardized$Filled<- ifelse(is.na(USGS_Standardized$Discharge_CFS) == TRUE,USGS_Standardized$Filled,USGS_Standardized$Discharge_CFS)
count(USGS_Standardized %>%
filter(is.na(Filled)))
# Remove the un-filled discharge column
USGS_ForAnalysis2 <-  USGS_ForAnalysis2 %>%
subset(select = -c(Discharge_CFS))
# Remove the un-filled discharge column
USGS_Standardized <-  USGS_Standardized %>%
subset(select = -c(Discharge_CFS))
# Rename to something meaningful
setnames(USGS_Standardized,old=c("Filled"),new=c("Discharge_CFS"))
RawFileName <- "Data/Raw/ecoli_StoretAndSODN.csv"
# This raw file contains the Storet and SODN data. The 30 SODN records were appended manually.
RawFileName <- "Data/Raw/ecoli_StoretAndSODN.csv"
#The Location file uses points lassoed in arcmap. It uses any sampling locations north of inputs from both the Nogales International Wastewater Treatment Plant and Sonoita Creek. Both the NIWWTP and Sonoita Creek are likely point sources of e. Coli.
LocationFileName <- "Data/Raw/EPA_PortalQuery_Locations_Appended_SODN.csv"
coliform_data <- read.csv(RawFileName)
Locations <- read.csv(LocationFileName)
write.csv(USGS_Standardized,"Data/Processed/USGS_Standardized.csv")
read.csv("Data/Processed/USGS_Standardized.csv")
USGS_Standardized <- read.csv("Data/Processed/USGS_Standardized.csv")
#This reads in the checkpoint file, if created.
#USGS_Standardized <- read.csv("Data/Processed/USGS_Standardized.csv")
# This raw file contains the Storet and SODN data. The 30 SODN records were appended manually.
RawStoret <- "Data/Raw/ecoli_Storet_Raw.csv"
RawSODN <- "Data/Raw/ecoli_SODN_Raw.csv"
RawLocation <- "ecoli_Locations_StoretandSODN.csv"
RawStoret_data <- read.csv(RawStoret)
RawSODN_data <- read.csv(RawSODN)
Locations <- read.csv(LocationFileName)
#This reads in the checkpoint file, if created.
#USGS_Standardized <- read.csv("Data/Processed/USGS_Standardized.csv")
# This raw file contains the Storet and SODN data. The 30 SODN records were appended manually.
RawStoret <- "Data/Raw/ecoli_Storet_Raw.csv"
RawSODN <- "Data/Raw/ecoli_SODN_Raw.csv"
RawLocation <- "ecoli_Locations_StoretandSODN.csv"
RawStoret_data <- read.csv(RawStoret)
RawSODN_data <- read.csv(RawSODN)
Locations <- read.csv(RawLocation)
RawLocation <- "Data/Raw/ecoli_Locations_StoretandSODN.csv"
Locations <- read.csv(RawLocation)
ecoli_data <- rbind(RawStoret_data, RawSODN_data)
View(RawStoret_data)
