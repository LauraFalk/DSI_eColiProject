#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
View(USGS_Raw)
USGS_Standardized <- USGS_Raw
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
# Create the time series, formatted as posixt
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))
colnames(TimeSeries)[1] = "DateTime"
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime, format = '%Y-%m-%d %H:%M'), tz = tz)
#Join data with time series, sort by DateTime2
USGS_TimeSeries <- right_join(USGS_Standardized, TimeSeries)
USGS_TimeSeries <-USGS_TimeSeries[order(USGS_TimeSeries$DateTime2),]
View(USGS_TimeSeries)
View(TimeSeries)
TimeSeries %>%
filter(is.na(DateTime))
USGS_TimeSeries %>%
filter(is.na(DateTime))
max(USGS_TimeSeries$DateTime)
max(USGS_TimeSeries$dateTime)
USGS_Standardized %>%
filter(is.na(dateTime))
remove(TimeFlag, TimeSeries, USGS_Raw, USGS_Standardized, USGS_TimeSeries_Fill)
site_id <- '09481740'
startDate = "2009-07-01"
#End date is 8/30/2022 to avoid null data at this time (9/5/2022)
endDate = "2022-08-30"
tz="America/Phoenix"
#creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
USGS_Standardized <- USGS_Raw
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
# Create the time series, formatted as posixt
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))
colnames(TimeSeries)[1] = "DateTime"
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime, format = '%Y-%m-%d %H:%M'), tz = tz)
View(USGS_Standardized)
#Join data with time series, sort by DateTime2
USGS_TimeSeries <- right_join(USGS_Standardized, TimeSeries)
#Join data with time series, sort by DateTime2
USGS_TimeSeries <- merge(x=TimeSeries,y=USGS_Standardized,by="DateTime2",all.x=TRUE)
View(USGS_TimeSeries)
count(USGS_TimeSeries %>%
filter(is.na(X_00060_00000)))
#Join data with time series, sort by DateTime2
# tried and failed to use a sided-join here. unsure why. base r merge leads to the correct number of records.
USGS_TimeSeries <- merge(x=TimeSeries,y=USGS_Standardized,by="DateTime2",all.x=TRUE)
USGS_TimeSeries <- merge(x=TimeSeries,y=USGS_Standardized,by="DateTime2",all.x=TRUE, all.y = FALSE)
count(unique(TimeSeries$DateTime2))
count(unique(TimeSeries$DateTime)
count(unique(TimeSeries$DateTime))
USGS_Standardized %>% count(unique(dateTime))
count(unique(USGS_Standardized$dateTime))
n_distinct(USGS_Standardized$DateTime2)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
view <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
View(view)
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime) )
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime) ) %>%
subset(distinct(USGS_Standardized_RmDupls))
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime) ) %>%
distinct(USGS_Standardized_RmDupls)
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime) ) %>%
unique(USGS_Standardized_RmDupls)
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime) ) %>%
subset(unique(USGS_Standardized_RmDupls))
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime)) %>%
distinct(USGS_Standardized)
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime)) %>%
duplicated()
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime))
duplicated(USGS_Standardized_RmDupls)
USGS_Standardized_RmDupls[duplicated(USGS_Standardized_RmDupls), ]
USGS_Standardized_RmDupls[!duplicated(USGS_Standardized_RmDupls), ]
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime))
View(USGS_Standardized_RmDupls)
USGS_Standardized_RmDupls <-  distinct(USGS_Standardized_RmDupls, c("dateTime2","X_00060_00000"))
USGS_Standardized_RmDupls %>%
distinct(DateTime2, X_00060_00000)
USGS_Standardized_RmDupls %>%
distinct(DateTime2, X_00060_00000, .keep_all = TRUE)
USGS_Standardized_RmDupls<- USGS_Standardized_RmDupls %>%
distinct(DateTime2, X_00060_00000, .keep_all = TRUE)
USGS_Standardized_RmDupls %>%
distinct(DateTime2)
view <- USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
View(view)
USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
USGS_Standardized_RmDupls %>% filter(DateTime2 == "2014-11-21 13:15:00")
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime))
USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
keys <- colnames(USGS_Standardized_RmDupls)[!grepl('DateTime2',colnames(USGS_Standardized_RmDupls))]
USGS_Standardized_RmDupls[,list(mm= mean(value)),keys]
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime))
#this shows any duplicates
USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
keys <- colnames(USGS_Standardized_RmDupls)[!grepl('DateTime2',colnames(USGS_Standardized_RmDupls))]
library(data.table)
X <- as.data.table(dat)
X[,list(mm= mean(value)),keys]
X <- as.data.table(USGS_Standardized_RmDupls)
X[,list(mm= mean(DateTime2)),keys]
USGS_Standardized_RmDupls <- X[,list(mm= mean(DateTime2)),keys]
USGS_Standardized_RmDupls <- X[,list(mm= mean(X_00060_00000),keys]
USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime))
USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
USGS_Standardized_RmDupls <- USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
View(USGS_Standardized_RmDupls)
USGS_RmDupl<- merge(x=USGS_Standardized,y=USGS_Standardized_RmDupls, by="DateTime2",all.x=TRUE, all.y = FALSE)
`colnames<-`(USGS_RmDupl$`mean(X_00060_00000)`, "CFS")
colnames<-(USGS_RmDupl$mean(X_00060_00000), "CFS")
colnames<-(USGS_RmDupl[11], "CFS")
colnames(USGS_RmDupl$`mean(X_00060_00000)`)<-"CFS"
colnames(USGS_RmDupl$`mean(X_00060_00000)`)<-"CFS"
View(USGS_RmDupl)
setnames(USGS_RmDupl,old=c("mean(X_00060_00000"),new=c("Discharge_CFS"))
setnames(USGS_RmDupl,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
View(USGS_RmDupl)
USGS_RmDupls <- USGS_RmDupl %>%
subset(select = -c(X_00060_00000))
# Removing duplicates doesn't work because there are still 3.
USGS_RmDupls<- USGS_RmDupls %>%
distinct(DateTime2, X_00060_00000, .keep_all = TRUE)
# Removing duplicates doesn't work because there are still 3.
USGS_RmDupls<- USGS_RmDupls %>%
distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)
USGS_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
site_id <- '09481740'
startDate = "2009-07-01"
#End date is 8/30/2022 to avoid null data at this time (9/5/2022)
endDate = "2022-08-30"
tz="America/Phoenix"
#creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
USGS_Standardized <- USGS_Raw
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
view <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# remove the pre-rounding dateTime column
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime))
# This shows any duplicate dates. There are a few because the data was not collected on the standard 15-minute interval. The separate values were rounded to the same one.
USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime
USGS_Standardized_RmDupls <- USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_RmDupl<- merge(x=USGS_Standardized,y=USGS_Standardized_RmDupls, by="DateTime2",all.x=TRUE, all.y = FALSE)
setnames(USGS_RmDupl,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
#Remove the pre-averaging flow column
USGS_RmDupls <- USGS_RmDupl %>%
subset(select = -c(X_00060_00000))
# Removing duplicates from the fixed times and fixed cfs
USGS_RmDupls<- USGS_RmDupls %>%
distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)
#Re-check
USGS_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# Create the time series, formatted as posixt
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))
colnames(TimeSeries)[1] = "DateTime"
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime, format = '%Y-%m-%d %H:%M'), tz = tz)
#Join data with time series, sort by DateTime2
# tried and failed to use a sided-join here. unsure why. base r merge leads to the correct number of records.
USGS_TimeSeries <- merge(x=TimeSeries,y=USGS_RmDupls,by="DateTime2",all.x=TRUE, all.y = FALSE)
View(USGS_TimeSeries)
View(USGS_Standardized)
View(USGS_Standardized)
View(USGS_RmDupl)
View(USGS_Raw)
View(USGS_TimeSeries)
count(USGS_TimeSeries %>%
filter(is.na(X_00060_00000)))
count(USGS_TimeSeries %>%
filter(is.na(Discharge_CFS)))
USGS_TimeSeries$date <- as_date(USGS_TimeSeries$DateTime2)
USGS_TimeSeries$count <- 1
TimeFlag <- USGS_TimeSeries %>%
filter(!is.na(Discharge_CFS)) %>%
group_by(date) %>%
summarize(value = sum(count))
USGS_TimeSeries <- right_join(TimeFlag, USGS_TimeSeries)
colnames(USGS_TimeSeries)[2] = "CountUSGSbyDate"
USGS_TimeSeries_Fill <-
USGS_TimeSeries %>%
filter(CountUSGSbyDate>88)
USGS_TimeSeries_Fill$Filled <-  na.approx(USGS_TimeSeries_Fill$Discharge_CFS, rule = 2)
USGS_ForAnalysis <- right_join(USGS_TimeSeries_Fill,USGS_TimeSeries)
USGS_ForAnalysis2 <- USGS_ForAnalysis
USGS_ForAnalysis2$Filled<- ifelse(is.na(USGS_ForAnalysis$Discharge_CFS) == TRUE,USGS_ForAnalysis$Filled,USGS_ForAnalysis$Discharge_CFS)
count(USGS_ForAnalysis2 %>%
filter(is.na(Filled)))
USGS_TimeSeries_Fill <-
USGS_TimeSeries %>%
filter(CountUSGSbyDate>70)
USGS_TimeSeries_Fill$Filled <-  na.approx(USGS_TimeSeries_Fill$Discharge_CFS, rule = 2)
USGS_ForAnalysis <- right_join(USGS_TimeSeries_Fill,USGS_TimeSeries)
USGS_ForAnalysis2 <- USGS_ForAnalysis
USGS_ForAnalysis2$Filled<- ifelse(is.na(USGS_ForAnalysis$Discharge_CFS) == TRUE,USGS_ForAnalysis$Filled,USGS_ForAnalysis$Discharge_CFS)
count(USGS_ForAnalysis2 %>%
filter(is.na(Filled)))
USGS_TimeSeries_Fill <-
USGS_TimeSeries %>%
filter(CountUSGSbyDate>88)
USGS_TimeSeries_Fill$Filled <-  na.approx(USGS_TimeSeries_Fill$Discharge_CFS, rule = 2)
USGS_ForAnalysis <- right_join(USGS_TimeSeries_Fill,USGS_TimeSeries)
USGS_TimeSeries_Fill <-
USGS_TimeSeries %>%
filter(CountUSGSbyDate>1)
USGS_TimeSeries_Fill$Filled <-  na.approx(USGS_TimeSeries_Fill$Discharge_CFS, rule = 2)
USGS_ForAnalysis <- right_join(USGS_TimeSeries_Fill,USGS_TimeSeries)
USGS_ForAnalysis2 <- USGS_ForAnalysis
USGS_ForAnalysis2$Filled<- ifelse(is.na(USGS_ForAnalysis$Discharge_CFS) == TRUE,USGS_ForAnalysis$Filled,USGS_ForAnalysis$Discharge_CFS)
count(USGS_ForAnalysis2 %>%
filter(is.na(Filled)))
USGS_TimeSeries_Fill <-
USGS_TimeSeries %>%
filter(CountUSGSbyDate>0)
USGS_TimeSeries_Fill$Filled <-  na.approx(USGS_TimeSeries_Fill$Discharge_CFS, rule = 2)
USGS_ForAnalysis <- right_join(USGS_TimeSeries_Fill,USGS_TimeSeries)
USGS_ForAnalysis2 <- USGS_ForAnalysis
USGS_ForAnalysis2$Filled<- ifelse(is.na(USGS_ForAnalysis$Discharge_CFS) == TRUE,USGS_ForAnalysis$Filled,USGS_ForAnalysis$Discharge_CFS)
count(USGS_ForAnalysis2 %>%
filter(is.na(Filled)))
USGS_TimeSeries_Fill <-
USGS_TimeSeries %>%
filter(CountUSGSbyDate>88)
USGS_TimeSeries_Fill$Filled <-  na.approx(USGS_TimeSeries_Fill$Discharge_CFS, rule = 2)
USGS_ForAnalysis <- right_join(USGS_TimeSeries_Fill,USGS_TimeSeries)
USGS_ForAnalysis2 <- USGS_ForAnalysis
USGS_ForAnalysis2$Filled<- ifelse(is.na(USGS_ForAnalysis$Discharge_CFS) == TRUE,USGS_ForAnalysis$Filled,USGS_ForAnalysis$Discharge_CFS)
count(USGS_ForAnalysis2 %>%
filter(is.na(Filled)))
USGS_TimeSeries$date <- as_date(USGS_TimeSeries$DateTime2)
USGS_TimeSeries$count <- 1
TimeFlag <- USGS_TimeSeries %>%
filter(!is.na(Discharge_CFS)) %>%
group_by(date) %>%
summarize(value = sum(count))
USGS_TimeSeries <- right_join(TimeFlag, USGS_TimeSeries)
colnames(USGS_TimeSeries)[2] = "CountUSGSbyDate"
View(USGS_TimeSeries)
USGS_TimeSeries$date <- as_date(USGS_TimeSeries$DateTime2)
USGS_TimeSeries$count <- 1
TimeFlag <- USGS_TimeSeries %>%
filter(!is.na(Discharge_CFS)) %>%
group_by(date) %>%
summarize(value = sum(count))
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
site_id <- '09481740'
startDate = "2009-07-01"
#End date is 8/30/2022 to avoid null data at this time (9/5/2022)
endDate = "2022-08-30"
tz="America/Phoenix"
#creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
USGS_Standardized <- USGS_Raw
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
view <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# remove the pre-rounding dateTime column
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime))
# This shows any duplicate dates. There are a few because the data was not collected on the standard 15-minute interval. The separate values were rounded to the same one.
USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime
USGS_Standardized_RmDupls <- USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_RmDupl<- merge(x=USGS_Standardized,y=USGS_Standardized_RmDupls, by="DateTime2",all.x=TRUE, all.y = FALSE)
setnames(USGS_RmDupl,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
#Remove the pre-averaging flow column
USGS_RmDupls <- USGS_RmDupl %>%
subset(select = -c(X_00060_00000))
# Removing duplicates from the fixed times and fixed cfs
USGS_RmDupls<- USGS_RmDupls %>%
distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)
#Re-check
USGS_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# Create the time series, formatted as posixt
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))
colnames(TimeSeries)[1] = "DateTime"
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime, format = '%Y-%m-%d %H:%M'), tz = tz)
#Join data with time series, sort by DateTime2
# could also use left_join here. Issues were from duplicate values from the rounding merge.
USGS_TimeSeries <- merge(x=TimeSeries,y=USGS_RmDupls,by="DateTime2",all.x=TRUE, all.y = FALSE)
count(USGS_TimeSeries %>%
filter(is.na(Discharge_CFS)))
USGS_TimeSeries$date <- as_date(USGS_TimeSeries$DateTime2)
USGS_TimeSeries$count <- 1
TimeFlag <- USGS_TimeSeries %>%
filter(!is.na(Discharge_CFS)) %>%
group_by(date) %>%
summarize(value = sum(count))
USGS_TimeSeries <- right_join(TimeFlag, USGS_TimeSeries)
colnames(USGS_TimeSeries)[2] = "CountUSGSbyDate"
USGS_TimeSeries <- right_join(TimeFlag, USGS_TimeSeries)
colnames(USGS_TimeSeries)[2] = "CountUSGSbyDate"
USGS_TimeSeries_Fill <-
USGS_TimeSeries %>%
filter(CountUSGSbyDate>88)
USGS_TimeSeries$date <- as_date(USGS_TimeSeries$DateTime2)
USGS_TimeSeries$count <- 1
TimeFlag <- USGS_TimeSeries %>%
filter(!is.na(Discharge_CFS)) %>%
group_by(date) %>%
summarize(value = sum(count))
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
site_id <- '09481740'
startDate = "2009-07-01"
#End date is 8/30/2022 to avoid null data at this time (9/5/2022)
endDate = "2022-08-30"
tz="America/Phoenix"
#creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
USGS_Standardized <- USGS_Raw
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
view <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# remove the pre-rounding dateTime column
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime))
# This shows any duplicate dates. There are a few because the data was not collected on the standard 15-minute interval. The separate values were rounded to the same one.
USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime
USGS_Standardized_RmDupls <- USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_RmDupl<- merge(x=USGS_Standardized,y=USGS_Standardized_RmDupls, by="DateTime2",all.x=TRUE, all.y = FALSE)
setnames(USGS_RmDupl,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
#Remove the pre-averaging flow column
USGS_RmDupls <- USGS_RmDupl %>%
subset(select = -c(X_00060_00000))
# Removing duplicates from the fixed times and fixed cfs
USGS_RmDupls<- USGS_RmDupls %>%
distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)
#Re-check
USGS_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# Create the time series, formatted as posixt
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))
colnames(TimeSeries)[1] = "DateTime"
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime, format = '%Y-%m-%d %H:%M'), tz = tz)
#Join data with time series, sort by DateTime2
# could also use left_join here. Issues were from duplicate values from the rounding merge.
USGS_TimeSeries <- merge(x=TimeSeries,y=USGS_RmDupls,by="DateTime2",all.x=TRUE, all.y = FALSE)
count(USGS_TimeSeries %>%
filter(is.na(Discharge_CFS)))
USGS_TimeSeries$date <- as_date(USGS_TimeSeries$DateTime2)
USGS_TimeSeries$count <- 1
TimeFlag <- USGS_TimeSeries %>%
filter(!is.na(Discharge_CFS)) %>%
group_by(date) %>%
summarize(value = sum(count))
USGS_TimeSeries <- right_join(TimeFlag, USGS_TimeSeries)
colnames(USGS_TimeSeries$value) = "CountUSGSbyDate"
colnames(USGS_TimeSeries$value) <-"CountUSGSbyDate"
colnames(USGS_TimeSeries$value) <-c("CountUSGSbyDate")
setnames(USGS_TimeSeries, "value", "CountUSGSbyDate")
USGS_TimeSeries_Fill <-
USGS_TimeSeries %>%
filter(CountUSGSbyDate>88)
USGS_TimeSeries_Fill$Filled <-  na.approx(USGS_TimeSeries_Fill$Discharge_CFS, rule = 2)
USGS_ForAnalysis <- right_join(USGS_TimeSeries_Fill,USGS_TimeSeries)
USGS_ForAnalysis2 <- USGS_ForAnalysis
USGS_ForAnalysis2$Filled<- ifelse(is.na(USGS_ForAnalysis$Discharge_CFS) == TRUE,USGS_ForAnalysis$Filled,USGS_ForAnalysis$Discharge_CFS)
count(USGS_ForAnalysis2 %>%
filter(is.na(Filled)))
remove(TimeFlag, TimeSeries, USGS_Raw, USGS_Standardized, USGS_TimeSeries_Fill)
remove(TimeFlag, TimeSeries, USGS_Raw, USGS_Standardized, USGS_TimeSeries_Fill, USGS_ForAnalysis, USGS_RmDupl, USGS_RmDupls, USGS_Standardized_RmDupls
remove(TimeFlag, TimeSeries, USGS_Raw, USGS_Standardized, USGS_TimeSeries_Fill, USGS_ForAnalysis, USGS_RmDupl, USGS_RmDupls, USGS_Standardized_RmDupls)
remove(TimeFlag, TimeSeries, USGS_Raw, USGS_Standardized, USGS_TimeSeries_Fill, USGS_ForAnalysis, USGS_RmDupl, USGS_RmDupls, USGS_Standardized_RmDupls, view)
USGS_ForAnalysis2 %>%
filter(is.na(Discharge_CFS))
USGS_ForAnalysis2 %>%
filter(is.na(Filled))
USGS_ForAnalysis2 %>%
subset(select = -c(date, count, Discharge_CFS))
setnames(USGS_ForAnalysis2,old=c("Filled"),new=c("Discharge_CFS"))
USGS_ForAnalysis2 <-  USGS_ForAnalysis2 %>%
subset(select = -c(date, count, Discharge_CFS))
setnames(USGS_ForAnalysis2,old=c("Filled"),new=c("Discharge_CFS"))
site_id <- '09481740'
startDate = "2009-07-01"
#End date is 8/30/2022 to avoid null data at this time (9/5/2022)
endDate = "2022-08-30"
tz="America/Phoenix"
#creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
USGS_Standardized <- USGS_Raw
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
view <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# remove the pre-rounding dateTime column
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime))
# This shows any duplicate dates. There are a few because the data was not collected on the standard 15-minute interval. The separate values were rounded to the same one.
USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime
USGS_Standardized_RmDupls <- USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_RmDupl<- merge(x=USGS_Standardized,y=USGS_Standardized_RmDupls, by="DateTime2",all.x=TRUE, all.y = FALSE)
setnames(USGS_RmDupl,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
#Remove the pre-averaging flow column
USGS_RmDupls <- USGS_RmDupl %>%
subset(select = -c(X_00060_00000))
# Removing duplicates from the fixed times and fixed cfs
USGS_RmDupls<- USGS_RmDupls %>%
distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)
#Re-check
USGS_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
site_id <- '09481740'
startDate = "2009-07-01"
#End date is 8/30/2022 to avoid null data at this time (9/5/2022)
endDate = "2022-08-30"
tz="America/Phoenix"
#creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
USGS_Standardized <- USGS_Raw
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# remove the pre-rounding dateTime column
USGS_Standardized_RmDupls <- USGS_Standardized %>%
subset(select = -c(dateTime))
# This shows any duplicate dates. There are a few because the data was not collected on the standard 15-minute interval. The separate values were rounded to the same one.
USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# This summarizes the flow column by the rounded DateTime
USGS_Standardized_RmDupls <- USGS_Standardized_RmDupls %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))
# Merge the rounded data back in and rename the column name to something more useful.
USGS_RmDupl<- merge(x=USGS_Standardized,y=USGS_Standardized_RmDupls, by="DateTime2",all.x=TRUE, all.y = FALSE)
setnames(USGS_RmDupl,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))
#Remove the pre-averaging flow column
USGS_RmDupl <- USGS_RmDupl %>%
subset(select = -c(X_00060_00000))
# Removing duplicates from the fixed times and fixed cfs
USGS_RmDupl<- USGS_RmDupl %>%
distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)
#Re-check
USGS_RmDupl %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)
# Create the time series, formatted as posixt
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))
colnames(TimeSeries)[1] = "DateTime"
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime, format = '%Y-%m-%d %H:%M'), tz = tz)
#Join data with time series, sort by DateTime2
# could also use left_join here. Issues were from duplicate values from the rounding merge.
USGS_TimeSeries <- merge(x=TimeSeries,y=USGS_RmDupl,by="DateTime2",all.x=TRUE, all.y = FALSE)
count(USGS_TimeSeries %>%
filter(is.na(Discharge_CFS)))
USGS_TimeSeries$date <- as_date(USGS_TimeSeries$DateTime2)
USGS_TimeSeries$count <- 1
TimeFlag <- USGS_TimeSeries %>%
filter(!is.na(Discharge_CFS)) %>%
group_by(date) %>%
summarize(value = sum(count))
USGS_TimeSeries <- right_join(TimeFlag, USGS_TimeSeries)
setnames(USGS_TimeSeries, "value", "CountUSGSbyDate")
USGS_TimeSeries_Fill <-
USGS_TimeSeries %>%
filter(CountUSGSbyDate>88)
USGS_TimeSeries_Fill$Filled <-  na.approx(USGS_TimeSeries_Fill$Discharge_CFS, rule = 2)
USGS_ForAnalysis <- right_join(USGS_TimeSeries_Fill,USGS_TimeSeries)
USGS_ForAnalysis2 <- USGS_ForAnalysis
USGS_ForAnalysis2$Filled<- ifelse(is.na(USGS_ForAnalysis$Discharge_CFS) == TRUE,USGS_ForAnalysis$Filled,USGS_ForAnalysis$Discharge_CFS)
count(USGS_ForAnalysis2 %>%
filter(is.na(Filled)))
USGS_ForAnalysis2 <-  USGS_ForAnalysis2 %>%
subset(select = -c(date, count, Discharge_CFS))
setnames(USGS_ForAnalysis2,old=c("Filled"),new=c("Discharge_CFS"))
remove(TimeFlag, TimeSeries, USGS_Raw, USGS_Standardized, USGS_TimeSeries_Fill, USGS_ForAnalysis, USGS_RmDupl, USGS_RmDupls, USGS_Standardized_RmDupls, view)
remove(TimeFlag, TimeSeries, USGS_TimeSeries, USGS_Raw, USGS_Standardized, USGS_TimeSeries_Fill, USGS_ForAnalysis, USGS_RmDupl, USGS_RmDupls, USGS_Standardized_RmDupls, view)
write.csv(USGS_ForAnalysis2, "Data/Processed/USGS_ForAnalysis20220905.csv")
