---
title: "Exploration_XGB_CV"
author: "Laura Palacios"
date: '2022-11-17'
output: html_document
---

```{r setup, include=FALSE}
library(caret)
library(xgboost)
library(readr)
library(dplyr)
library(tidyr)
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Read in data
ecoli_attr <- read.csv("Data/Processed/ecoli_attributed2.csv")

# Partition the data to include only 236 e coli. 575 is column 10
ecoli_235 <- ecoli_attr[,2:8]

#make this example reproducible
set.seed(5)

#split into training (80%) and testing set (20%)
parts <- createDataPartition(ecoli_235$ecoli_235, p = .7, list = F)

train <- ecoli_235[parts, ]
test <- ecoli_235[-parts, ]

train_x <- data.matrix(train[, -7])
train_y <- train[,7]

test_x <- data.matrix(test[, -7])
test_y <- test[, 7]
```


https://stats.stackexchange.com/questions/171043/how-to-tune-hyperparameters-of-xgboost-trees

```{r}
# xgboost fitting with arbitrary parameters
xgb_params_1 = list(
  objective = "binary:logistic",                                               # binary classification
  eta = 0.01,                                                                  # learning rate
  max.depth = 6,                                                               # max tree depth
  eval_metric = "rmsle"                                                          # evaluation/loss metric
)

# fit the model with the arbitrary parameters specified above
xgb_1 = xgboost(data = as.matrix(train %>%
                                   select(-ecoli_235)),
                label = train$ecoli_235,
                params = xgb_params_1,
                nrounds = 300,                                                 # max number of trees to build
                verbose = TRUE,                                         
                print_every_n = 1, 
                early_stoping_round = 5                                          # stop if no improvement within 10 trees
)

# cross-validate xgboost to get the accurate measure of error
xgb_cv_1 = xgb.cv(params = xgb_params_1,
                  data = as.matrix(train %>%
                                   select(-ecoli_235)),
                  label = train$ecoli_235,
                  nrounds = 300, 
                  nfold = 5,                                                   # number of folds in K-fold
                  prediction = TRUE,                                           # return the prediction using the final model 
                  showsd = TRUE,                                               # standard deviation of loss across folds
                  stratified = TRUE,                                           # sample is unbalanced; use stratified sampling
                  verbose = TRUE,
                  print_every_n = 1, 
                  early_stoping_round = 5
)

# plot the AUC for the training and testing samples
xgb_cv_1$evaluation_log %>%
  select(-contains("std")) %>%
  select(-contains("iter")) %>%
  mutate(IterationNum = 1:n()) %>%
  gather(TestOrTrain, RMSLE, -IterationNum) %>%
  ggplot(aes(x = IterationNum, y = RMSLE, group = TestOrTrain, color = TestOrTrain)) + 
  geom_line() + 
  theme_bw()
```



