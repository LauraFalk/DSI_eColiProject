---
title: "eColi_Data_Exploration_R1"
author: "Laura Palacios"
date: '2022-08-29'
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
```

###ONLY RUN THIS ONCE.
This is the data pull for USGS data. It is many years and takes lots of time to grab all 300k records. USe CSV for data beterrn 7/1/2009 and 8/30/2022
```{r}
site_id <- '09481740'
startDate = "2009-07-01"
#End date is 8/30/2022 to avoid null data at this time (9/5/2022)
endDate = "2022-08-30"
tz="America/Phoenix"

#creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")

USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
```


2015 had some times that were off by a few minutes from the normal 15- minute interval. This will ensure that everything is standardized. Values are rounded to the nearest 15-minutes.
```{r}
USGS_Standardized <- USGS_Raw
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)

```

There are missing timestamps. This will create a full time series to include no data indicators.
```{r}
# Create the time series, formatted as posixt
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))
colnames(TimeSeries)[1] = "DateTime"
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime, format = '%Y-%m-%d %H:%M'), tz = tz)

#Join data with time series, sort by DateTime2
USGS_TimeSeries <- right_join(USGS_Standardized, TimeSeries)
USGS_TimeSeries <-USGS_TimeSeries[order(USGS_TimeSeries$DateTime2),]
```

For data exploration: this tests to see how many nulls are present in the current dataset.
note 9/5/2022: 40578
```{r}
count(USGS_TimeSeries %>%
 filter(is.na(X_00060_00000)))
```

I think I should be able to fill some of the nulls by taking the average of the two surrounding values. How many does this account for?

```{r}
USGS_TimeSeries$date <- as_date(USGS_TimeSeries$DateTime2)
USGS_TimeSeries$count <- 1

TimeFlag <- USGS_TimeSeries %>%
  filter(!is.na(X_00060_00000)) %>%
  group_by(date) %>%
  summarize(value = sum(count))

USGS_TimeSeries <- right_join(TimeFlag, USGS_TimeSeries)
colnames(USGS_TimeSeries)[2] = "CountUSGSbyDate"
```
Start to fill NAs where applicable. This will fill in all data into a new column.
```{r}
USGS_TimeSeries_Fill <-
  USGS_TimeSeries %>%
  filter(CountUSGSbyDate>88)

USGS_TimeSeries_Fill$Filled <-  data.frame(na.approx(USGS_TimeSeries_Fill$X_00060_00000, rule = 2))

USGS_ForAnalysis <- inner_join(USGS_TimeSeries_Fill,USGS_TimeSeries)

```
Check how many nulls there are now that some of the data is filled. I'm not comfortable auto-filling (the majority) of the remaining values.But there is currently only 3159.
```{r}
count(USGS_ForAnalysis %>%
 filter(is.na(X_00060_00000)))
```

