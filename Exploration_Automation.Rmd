---
title: "Automation Processing"
author: "Laura Palacios"
date: '2022-11-13'
output: html_document
---
# Libraries
```{r setup, include=FALSE}
library(dataRetrieval)
library(knitr)
library(tidyverse)

# Not currently on CRAN - devtools::install_github("scoyoc/climateAnalyzeR")
library(climateAnalyzeR)

opts_chunk$set(echo = TRUE)
```

# Read in the data

I will need to read in:

1. PreviousTmin(Climateanalyzer - lagged raw value)
2. Discharge_CFS (USGS WaterData - Raw value)
3. Stage (USGS Waterdata - Calculated Categorical
4. NinXTS (NOAA?)
5. TOD (Use Current?)
6. DistFromSonoita (Think about this one. Can I calc using latlong?) https://stackoverflow.com/questions/32363998/function-to-calculate-geospatial-distance-between-two-points-lat-long-using-r

# Default values for data pulls - only need to do this once

```{r}
# Find start and end dates to use for data pulls
startDate <- Sys.Date() - 31
endDate <- Sys.Date() - 1
startYear <- as.numeric(format(Sys.Date(), "%Y"))-1
endYear <- as.numeric(format(Sys.Date(), "%Y"))
tz="America/Phoenix"
```

# 1 - Previous T min (2 days prior). I need to lag this one more time in the processing due to the outputs of climate analyzer. This was previously used as the 30 day precipitation values, but the rain gage data is not trustworthy anymore due to birds and mowing vehicles.

```{r}
# Pulls in current and 1 previous year's data (to account for January calcs)
TMin <- climateAnalyzeR::import_data("daily_wx", station_id = 'KA7WSB-1', start_year = startYear, end_year = endYear, station_type = 'RAWS')

TMin$DateasDate <- as.POSIXct(TMin$date, format = "%m/%d/%Y", tz = tz)


Var_TMin <- TMin %>%
  subset(DateasDate == endDate - 2) %>%
  select(tmin_f)

Var_TMin <- as.numeric(unlist(Var_TMin))

rm(TMin)
```

# Discharge data

Use dataRetrieval to pull in the data from the USGS waterdata website.
```{r}
# Inputs formatted for read NWIS function.
site_id <- '09481740'

# Input timezone for read NWIS and POSIXT functions
tz="America/Phoenix" 

# Creates table in R
USGSRaw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)

Var_Discharge_CFS<- tail(USGSRaw$X_00060_00000, n=1)


```

# Stage
```{r}
# Create quantiles for categorization
CFS_Quantiles<- quantile(USGSRaw$X_00060_00000, na.rm = TRUE)

# Determine the difference between prior reading and current.
USGSRaw <- USGSRaw %>% 
  mutate(DisDif = X_00060_00000 - lag(X_00060_00000))

# This will create a binary variable or either rise of fall. Rise = 1, fall = 0. It will allow me to more easily create summary statistics.
USGSRaw$DisDif2 <- ifelse(USGSRaw$DisDif>0,1,0)

# Create a numeric classifier. 
# 1 = Low Flow, 2 = Base flow, 3 = High and Rising Flow 4 = High and Falling Flow
USGSRaw$Stage <- ifelse(USGSRaw$X_00060_00000 <=CFS_Quantiles[2], 1, 
  ifelse(USGSRaw$X_00060_00000 > CFS_Quantiles[2] & USGSRaw$X_00060_00000 <= CFS_Quantiles[4],2,
  ifelse(USGSRaw$X_00060_00000 > CFS_Quantiles[4] & USGSRaw$DisDif2 == 1,3,
  ifelse(USGSRaw$X_00060_00000 > CFS_Quantiles[4] & USGSRaw$DisDif2 == 0,4, NA))))

# Remove fields with variables used for calculations
rm(CFS_Quantiles)

# Create the stage variable.
Var_Stage<- tail(USGSRaw$Stage, n=1)

rm(USGSRaw)
```

4. NinXTS (NOAA?)
```{r}
# Bring in the website data
url <- "https://origin.cpc.ncep.noaa.gov/products/analysis_monitoring/ensostuff/ONI_v5.php"
NinXTS <- url %>%
  rvest::read_html()

# Grab the data table
NinText <- rvest::html_table(rvest::html_nodes(NinXTS, xpath = './/table[4]//table[2]'))

# Convert ONI index to dataframe
NinTable <- as.data.frame(NinText[1])

# Remove NA data from last row 
NinXTSRow <- tail(NinTable, n = 1) %>%
  select_if(~ !any(is.na(.)))

# Pull last reported value
Var_NinXTS <- as.numeric(unlist(NinXTSRow[length(NinXTSRow)]))

rm(NinXTS, NinXTSRow, NinTable, NinText)

```


5. TOD (Use Current?)

```{r}
#modified from https://stackoverflow.com/questions/49370387/convert-time-object-to-categorical-morning-afternoon-evening-night-variable

# Create categorical variables
currenttime <- as.POSIXct(Sys.time(), format = "%H:%M") %>% format("%H:%M:%S")

currenttime <- cut(chron::times(currenttime) , breaks = (1/24) * c(0,5,11,16,19,24))
Var_TOD <- c(4, 1, 2, 3, 4)[as.numeric(currenttime)]

```

6. DistFromSonoita (Think about this one. Can I calc using latlong?) Just use a lookup?

```{r}
library(mapview)
library(sf)
samplelocs <- read.csv("Data/Raw/ecoli_Locations_StoretandSODN.csv")

samplelocs <- samplelocs %>%
  select(c('OrganizationIdentifier', 'MonitoringLocationIdentifier', 'MonitoringLocationName', 'LatitudeMeasure', 'LongitudeMeasure'))

spatiallocs <- read_sf("Data/Processed/ecoli_Locations_StoretandSODN_XYtableToPoint1.shp")

mapview(samplelocs, xcol = "LatitudeMeasure", ycol = "LongituddeMeasure", zcol = "MonitoringLocationIdentifier")
```

