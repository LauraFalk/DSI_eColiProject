---
title: "Escherichia coli Data Exploration in the Upper Santa Cruz River"
output: html_notebook
---

```{r setup, include=FALSE}
library(corrplot)
library(data.table)
library(dataRetrieval)
library(janitor)
library(knitr)
library(lubridate)
library(tidyverse)
library(xts)
library(zoo)
opts_chunk$set(echo = TRUE)
```

## Processing the USGS File

### Notes
This data is from the USGS continuous monitoring gage of the Santa Cruz River at Tubac. Data is all published and available for public use.

### Code
I have commented out the data pull for USGS data. It is many years and takes lots of time to grab all 300k records when running this script. To save time, I've outputted the result of: 
readNWISuv(site_id,c('00060','00065'), "2009-07-01","2022-08-30", "America/Phoenix")
as USGS_09481740.CSV and saved this within the raw data folder on 9/12/2022.

Original code may be un-commented and re-run if desired. Data after 1/10/2022 is currently provisional and is still subject to change during USGS QAQC procedures.
```{r}
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"

# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)

# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")

# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
```

2015 had some times that were off by a few minutes from the normal 15- minute interval. This will ensure that everything is standardized. Values are rounded to the nearest 15-minute interval.
```{r}
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw

# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
```

This indicates that there are duplicate entries which have arisen from the 15-minute standardization. This is because the data was not collected on the standard 15-minute interval. The separate values were rounded to the same value.
```{r}
# Loaded plyr for later testing and it made this piece of the code upset. "dplyr::" should not be necessary, but it avoids issues.
USGS_Standardized %>% group_by(DateTime2) %>% dplyr::summarise(n=sum(n())) %>% filter(n>1)
```

Ah. This is due to the rounding process.
The following chunk creates a mean of the values (which will be within 15 minutes of each other, so it is assumed the values will be similar enough to merge) to use in place of the two readings.
```{r}
# This summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))

# Rename the column to something more useful.
setnames(USGS_Standardized_append,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))

# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_append, by="DateTime2",all.x=TRUE, all.y = FALSE)

#Remove the pre-averaging flow column. This allows to remove duplicates from the append pairs created above.
USGS_Standardized <- USGS_Standardized %>%
  subset(select = -c(X_00060_00000)) %>%
  distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)

# Re-check for duplicate values. This should equal zero.
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)

# Remove append table - data has been merged
remove(USGS_Standardized_append)

```

Though the data is nearly complete, there are still some missing values for the 15 minute intervals. Currently the missing rows are not included in the dataframe.
This will create a full time series to include indicator of no data.
```{r}
# Create the time series, formatted as POSIXct
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))

# Rename the Column DateTime2 to indicate properly formatted column.
colnames(TimeSeries)[1] = "DateTime2"
# Follow through on the name and format this bad boy.
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)

#Left join data with time series, sort by DateTime2
USGS_Standardized <- merge(x=TimeSeries,y=USGS_Standardized,by="DateTime2",all.x=TRUE, all.y = FALSE)

# Remove the created time series. Data has been merged
remove(TimeSeries)

```
Notice that 2009-07-01 04:30:00 is now a row which indicates no data.


We now have to handle the nulls we created.
```{r}
# This tests to see how many nulls are present in the current dataset.
# 9/5/2022: 40544 records are null
count(USGS_Standardized %>%
 filter(is.na(Discharge_CFS)))
```

I think I should be able to fill some of the nulls by taking the average of the two surrounding values. 
```{r}
# This column is a dummy column which allows me to count more easily in the next block.
USGS_Standardized$date <- as_date(USGS_Standardized$DateTime2)
USGS_Standardized$count <- 1

# This counts the number of readings per day
TimeFlag <- USGS_Standardized %>%
  filter(!is.na(Discharge_CFS)) %>%
  group_by(date) %>%
  summarize(value = sum(count))

# Append the reading count value to the standardized data
USGS_Standardized <- right_join(TimeFlag, USGS_Standardized)

# Name this something meaningful
setnames(USGS_Standardized, c("value", "X_00060_00000_cd", "X_00065_00000", "X_00065_00000_cd"), c("DailyFlowCount", "Discharge_cd", "USGS_Staff", "USGS_Staff_cd"), skip_absent = TRUE)

# Remove extraneous columns and reorder
USGS_Standardized <- USGS_Standardized %>%
  select(DateTime2, date, Discharge_CFS, USGS_Staff, Discharge_cd, tz_cd, DailyFlowCount)

# Remove Timeflag table. Merged into data
remove(TimeFlag)

```

Start to fill null data where applicable. This will fill in all data into a new column.
```{r}
#Filter for columns with a daily flow count greater than 88 (no more than 2 hours per day missing.)
USGS_Standardized_Fill <-
  USGS_Standardized %>%
  filter(DailyFlowCount>88)

# Fill in missing data
USGS_Standardized_Fill$Filled <-  na.approx(USGS_Standardized_Fill$Discharge_CFS, rule = 2)

#Join back in
USGS_Standardized <- right_join(USGS_Standardized_Fill,USGS_Standardized)

# Remove the old data
remove(USGS_Standardized_Fill)

```

Check how many nulls there are now that some of the data is filled. More data filled would mean more assumptions.
# 9/12/2022: Current count of null data is decreased to 37385
```{r}

# Fill in the un-filled values
USGS_Standardized$Filled<- ifelse(is.na(USGS_Standardized$Discharge_CFS) == TRUE,USGS_Standardized$Filled,USGS_Standardized$Discharge_CFS)

count(USGS_Standardized %>%
 filter(is.na(Filled)))

```
Clean up the columns 
```{r}
# Remove the un-filled discharge column
USGS_Standardized <-  USGS_Standardized %>%
  subset(select = -c(Discharge_CFS))

# Rename to something meaningful
setnames(USGS_Standardized,old=c("Filled"),new=c("Discharge_CFS"))
```

Remove Raw columns
```{r}
remove(USGS_Raw)
```


## Merging the E. coli data sources and limiting to sample size.

### Data Retrieval
I have raw data from the water quality portal (EPA) a summary of ADEQ, USGS, FOSCR and NPS (Tumacacori) data along with a download of NPS-SODN quarterly sampling data (approximately 35 samples). 

Data location and security: https://www.epa.gov/waterdata/water-quality-data. Data is published and public. SODN data is unpublished and should be considered provisional for this project.

Storet: https://www.waterqualitydata.us/ 
Country: United States of America (NWIS, STEWARDS, STORET)
State: Arizona (NWIS, STORET)
County: US, Arizona, Santa Cruz County (NWIS, STORET)
Site type: Stream (NWIS, STEWARDS, STORET)
Date from: 7/1/2009
Date to: 8/30/2022
Sample Media: Water (NWIS, STEWARDS, STORET)
Characteristic Group: Microbiological (NWIS, Storet)
Data profiles: Sample results (biological metadata)

SODN: Data was pulled using a sql query on a DOI/NPS server. More information can be provided by the SODN data manager, currently Helen Thomas.

### Sampling Locations Information

Storet: https://www.waterqualitydata.us/ 
Country: United States of America (NWIS, STEWARDS, STORET)
State: Arizona (NWIS, STORET)
County: US, Arizona, Santa Cruz County (NWIS, STORET)
Site type: Stream (NWIS, STEWARDS, STORET)
Date from: 7/1/2009
Date to: 8/30/2022
Sample Media: Water (NWIS, STEWARDS, STORET)
Characteristic Group: Microbiological (NWIS, Storet)
Data profiles: Site Data Only

SODN: Single sampling location coordinates were provided by NPS and manually added to the storet download CSV location file.

### Sampling Location processing
The storet download file was placed into Arcmap and points within the sample range were exported to the included raw csv file. The locations for data download were chosen from the Upper Santa Cruz River. Locations are limited to North of the NIWWTP and Sonoita Creek confluence to avoid data noise from two likely point sources. All locations sampled should contain the same water without significant other inputs

### Below, The data was limited to:
1. sample location (above)
2. E. coli only (not total coliforms)
3. same method (EPA Standard Method 9223B, Colilert, sampling procedures)
4. comparable date range (Post 2009 NIWWTP significant upgrade project)


## Code
Read in the raw data files
```{r}
#This reads in the checkpoint file, if created.
#USGS_Standardized <- read.csv("Data/Processed/USGS_Standardized.csv")

# This raw file contains the Storet and SODN data. The 30 SODN records were appended manually.
RawStoret <- "Data/Raw/ecoli_Storet_Raw.csv"
RawSODN <- "Data/Raw/ecoli_SODN_Raw.csv"
RawLocation <- "Data/Raw/ecoli_Locations_StoretandSODN.csv"

RawStoret_data <- read.csv(RawStoret)
RawSODN_data <- read.csv(RawSODN)
Locations <- read.csv(RawLocation)
```

Merge the SODN and Storet Raw Data files
```{r}
# Merge the two dataframes together. SODN raw data was outputted to match the STORET format. Fields were manually filled by data management.
ecoli_data <- rbind(RawStoret_data, RawSODN_data)

# Use Janitor to clean null columns
ecoli_data<- remove_empty(ecoli_data, c("cols"))

# Remove the pre-merged raw data
remove(RawStoret_data, RawSODN_data)
```


Now we must clean up the data further.
Keep only the columns which contain E. coli data. Other colifoms or enterolert (etc) procedures are within the STORET output. 

Note: Fecal coliforms may also be applicable, but for the initial study I would like to stick solely to Escherichia coli. Nick Paretti (USGS) indicated that this maybe a better options due to the increased varaiblility. Fecal coliforms (non-species specific) have maximum results much more often and may be less meaningful.
```{r}
# STORET has standardized the output as "Excherichia coli"
ecoli_data <- ecoli_data[ecoli_data$CharacteristicName == "Escherichia coli", ] 
```

I also want to remove anything with a time zone other than MST. I cannot verify if this is a typo or an actual data point.
With the 9/13 data pull this is only one record.
```{r}
# STORET has standardized the output as "Excherichia coli"
ecoli_data <- ecoli_data[ecoli_data$ActivityStartTime.TimeZoneCode == "MST", ] 
```

Remove columns with methods other than 9223B
```{r}
# The methods are not standardized in STORET. It is assumed that all forms of Colilert and SM9223B represent comparable results. Colilert comes in both an 18 and 24 hour test, but they are assumed to provide a comparable output. 
ecoli_data <- ecoli_data[ecoli_data$ResultAnalyticalMethod.MethodName == "E coli, water, Colilert (24-28)"|
                                    ecoli_data$ResultAnalyticalMethod.MethodName ==  "COLILERT (EDBERG)"|
                                    ecoli_data$ResultAnalyticalMethod.MethodName ==  "COLILERT"|
                                    ecoli_data$ResultAnalyticalMethod.MethodName ==  "SM 9223B", ] 
```

Remove dates before 2009 when the plant was updated. June 2009
Note: SODN data file had a different date format. CSV output from SODN was updated on 9/13/2022
```{r}
ecoli_data$DateAsDate <- ymd(ecoli_data$ActivityStartDate,tz=tz)
ecoli_data$DateAsDate <-format(as.POSIXct(ecoli_data$ActivityStartDate),format='%Y-%m-%d')
ecoli_data <- ecoli_data[ecoli_data$DateAsDate >= "2009-07-01", ] 
```


Merge the files based on monitoring location identifier.Monitoring locations were outputted from an ARCGIS lasso of the range between the NIWWTP/Sonoita Creek inputs and the end of the Upper Santa Cruz River.

This will decrease the number of rows in the dataset because it excludes measurements outside of the study range (namely the sonoita creek data)
```{r}
# Merge on the Monitoring Location Identifier (outputted by STORET). The single SODN monitoring location was manually added to the end of this file before import.
ecoli_data<- merge(ecoli_data, Locations, by = c("MonitoringLocationIdentifier"))
```

Remove unnecessary tables. 
```{r}
remove(Locations, RawLocation, RawSODN, RawStoret)
```

Remove any null columns created by reducing the dataset size.
```{r}
# Use Janitor to clean null columns
ecoli_data<- remove_empty(ecoli_data, c("cols"))
```

## Further processing of the e. Coli data

Remove columns that contain only 1 entry
```{r}
# Count the number of distinct values. If all values are the same, this will not be used for analysis and column is dropped.
ecoli_data <- keep(ecoli_data, ~n_distinct(.) > 1)
```

View the data. There are still columns I don't need.
```{r}
# Use the summary function to look at what data exists. Commented out because it is unnecessary.
#summary(ecoli_data)
```

Take out columns that don't have meaningful data. Numeric First
```{r}
# Select only columns that I want to use for analysis
ecoli_data<- ecoli_data %>%
  select(DateAsDate, 
         ActivityStartTime.Time,  
         MonitoringLocationIdentifier,
         HydrologicCondition,
         HydrologicEvent,
         ResultValueTypeName,
         ResultStatusIdentifier,
         ResultMeasureValue,
         ResultDetectionConditionText,
         ActivityTypeCode,
         ActivityConductingOrganizationText)

```

This column will indicate if the data is above or below detection limits. 
```{r}
# The maximum test capacity is 2419 MPN. Above this value was a dilution technique.
ecoli_data$QuantificationLimit <- ifelse(ecoli_data$ResultMeasureValue > 2419 | ecoli_data$ResultDetectionConditionText == "Present Above Quantification Limit", "Present Above Quantification Limit", "Value")

# This will standardize the non-detects
ecoli_data$QuantificationLimit <- ifelse(ecoli_data$ResultDetectionConditionText == "Present Below Quantification Limit"|ecoli_data$ResultDetectionConditionText == "Not Detected", "Present Below Quantification Limit", ecoli_data$QuantificationLimit)
```

Remove QAQC samples
```{r}
# Blanks and replicates will be discounted for this analysis. Only field samples should remain.
ecoli_data <- ecoli_data %>%
  filter(ActivityTypeCode != "Quality Control Sample-Field Replicate" & ActivityTypeCode !="Quality Control Sample-Equipment Blank")

# this is just a check to ensure that this leads me to the same count as the samples only table.
#ecoli_data %>%
#count(ActivityTypeCode == "Sample-Routine")
```

I want to create a column of data standardized to the detection limit of sodn sampling (2420). Some samples were much higher than that because they were diluted, but it appears not all of them were. 
```{r}
ecoli_data$StandardizedResult <- ecoli_data$ResultMeasureValue
ecoli_data$StandardizedResult <- ifelse(ecoli_data$QuantificationLimit == "Present Above Quantification Limit", 2420, ecoli_data$StandardizedResult)

ecoli_data$StandardizedResult <- ifelse(ecoli_data$QuantificationLimit == "Present Below Quantification Limit",0, ecoli_data$StandardizedResult)

ecoli_data %>%
  count(StandardizedResult == 2420)
# I currently have 431 maximum results.

ecoli_data %>%
  count(StandardizedResult >126)
#772 are greater than the single standard EPA guideline (126 MPN)

ecoli_data %>%
  count(StandardizedResult >235)
#664 samples are greater than 235, my normal comparison for warm effluent water.

ecoli_data %>%
  count(StandardizedResult <1)
#There are only 3 non-detect

ecoli_data %>%
  count(QuantificationLimit == "Present Below Quantification Limit")
#Verified that there are only 3 non-detect

ecoli_data %>%
  count(is.na(StandardizedResult))
# There are no NA values
```


Remove unnecessary, unstandardized columns.
```{r}
ecoli_data<- ecoli_data %>%
  select(-ResultMeasureValue,
         -ResultDetectionConditionText,
         -ActivityTypeCode)
```


## Combine the ecoli, USGS, and climate data. Add seasonal attributes and create precipitation numeric attributes. May continue to change.

1. flow or water pulses
2. air temperature
3. season (Monsoon, Fall, Winter, Summer - USE SODN criteria)
4. possibly precip data: options - rainfall in last month, rainfall in last year, time since last rainfall
5. possibly location - miles from the Nogales plant
6. what else?


First step. Merge the ecoli data with the USGS (flow) data.
```{r}
# Ensure that this is in the correct time format. Always problematic, so this is over-coded to ensure success.
ecoli_data$DateTime2 <- as.POSIXct(format(paste(ecoli_data$DateAsDate,ecoli_data$ActivityStartTime.Time), format = '%Y-%m-%d %H:%M'), tz = tz)

ecoli_data$DateTime2 <-
round_date(as.POSIXct(ecoli_data$DateTime2), "15 minutes")


data_Merge<- left_join(ecoli_data, 
                          USGS_Standardized, by = c("DateTime2"))

data_Merge %>%
  count(is.na(Discharge_CFS))
# 75 records are still without discharge. this is the one I care the most about.
```

Remove raw data, now merged
```{r}
remove(USGS_Standardized, ecoli_data)
```


Remove the nulls from the merge. This is performed here rather than by using a sided join so that I may decide to do something different later on.
```{r}
data_Merge2 <- data_Merge %>%
  filter(!is.na(Discharge_CFS))
  
```


Look at what the data currently looks like. The red line is the actual standard for warm water effluent in AZ. The blue dotted line indicates the state standard for an average of three+ measurements. It is not technically applicable here, but gives a baseline. It may  be used to flag "Concerning-but-not-technically-bad" levels of E. coli.
```{r}
data_Merge2 %>%
  ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
  geom_point()+
  geom_hline(yintercept=126, linetype="dashed", 
                color = "blue", size=0.2)+
  geom_hline(yintercept=235, linetype="dashed", 
                color = "red", size=0.2)
```

Now I want to see what it looks like without the outlying discharge values.
```{r}
data_Merge2 %>%
  filter(Discharge_CFS < 250) %>%
  ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
  geom_point() +
  geom_hline(yintercept=126, linetype="dashed", 
                color = "blue", size=0.2) +
  geom_hline(yintercept=235, linetype="dashed", 
                color = "red", size=0.2)
```
Now I want to see what it looks like without the outlying discharge values. And one more for good measure.
```{r}
data_Merge2 %>%
  filter(Discharge_CFS < 150) %>%
  ggplot(aes(x=Discharge_CFS, y=StandardizedResult)) +
  geom_point() +
  geom_hline(yintercept=126, linetype="dashed", 
                color = "blue", size=0.2) +
  geom_hline(yintercept=235, linetype="dashed", 
                color = "red", size=0.2)
```
well, it definitely doesn't look randomly distributed.


Let's add some air temperature up in here up in here.
```{r}
# Raw file location
Rawclimate <- "Data/Raw/ClimateAnalyzer_DailySummaryStats_Raw.csv"

# Read in raw file.
DailyClimate <- read.csv(Rawclimate, skip = 3)
DailyClimate <- head(DailyClimate, - 1)

# Variable no longer needed.
remove(Rawclimate)
```

What about the sum of previous three-days precipitation
```{r}
DailyClimate$Previous3Precip <-  rollsumr(DailyClimate$Precipitation..in., k = 3, fill = NA)
DailyClimate$Previous7Precip <-  rollsumr(DailyClimate$Precipitation..in., k = 7, fill = NA)
DailyClimate$Previous30Precip <-  rollsumr(DailyClimate$Precipitation..in., k = 30, fill = NA)
```


```{r}
DailyClimate$DateAsDate <- str_sub(DailyClimate$Date,-10,-1)
DailyClimate$DateAsDate <-as.POSIXct(DailyClimate$DateAsDate, format = "%m/%d/%Y", tz = tz)
```

merge together with other data
```{r}
data_Merge2$DateAsDate <-as.POSIXct(data_Merge2$DateAsDate, format = "%Y-%m-%d", tz = tz)


data_Merge2<- left_join(data_Merge2, 
                          DailyClimate, by = c("DateAsDate"))



data_Merge2 %>%
  filter(is.nan(Tmax..F.))
# we have 113 more missing Tmax datapoints. I could possibly manipulate this file to clean nans but I'm not sure how comfortable I feel about this. LAURA check in on this with expert.
```

Graph again

```{r}
data_Merge2 %>%
  filter(!is.nan(Tmax..F.)) %>%
  ggplot(aes(x=Tmax..F., y=StandardizedResult)) +
  geom_point() +
  geom_hline(yintercept=126, linetype="dashed", 
                color = "blue", size=0.2) +
  geom_hline(yintercept=235, linetype="dashed", 
                color = "red", size=0.2)
```
This isn't as great as I thought it would be. 
Google says temps of 39-113°F are survivable for e. coli. ideal is 98.6°F.


```{r}
data_Merge2 %>%
  filter(!is.nan(Tmin..F.)) %>%
  ggplot(aes(x=Tmin..F., y=StandardizedResult)) +
  geom_point() +
  geom_hline(yintercept=126, linetype="dashed", 
                color = "blue", size=0.2) +
  geom_hline(yintercept=235, linetype="dashed", 
                color = "red", size=0.2)
```
It is of note that the max only happens above freezing. This may actually be more meaningful.

There is probably a better source for precip, but I'd like to take a look at it. It may be correlated to flow. I'll have to check this later.
```{r}
data_Merge2 %>%
  filter(!is.nan(Precipitation..in.)) %>%
  ggplot(aes(x=Precipitation..in., y=StandardizedResult)) +
  geom_point() +
  geom_hline(yintercept=126, linetype="dashed", 
                color = "blue", size=0.2) +
  geom_hline(yintercept=235, linetype="dashed", 
                color = "red", size=0.2)
```
Now I want to look at season. According to the NPS protocol the seasons in southern Arizona are:
Fall (October 1 - December 30)
Winter (January 1 - March 30)
Spring (April 1 - June 30)
Monsoon (July 1 - Sept 30)

```{r}
data_Merge2$Season1 <- as.numeric(str_sub(data_Merge2$DateAsDate,6,7))
  
  
data_Merge2$Season <- ifelse(data_Merge2$Season1 > 9 & data_Merge2$Season1 < 13, "Fall", 
                                           ifelse (data_Merge2$Season1 > 0 & data_Merge2$Season1 < 4, "Winter",
                                                   ifelse (data_Merge2$Season1 > 3 & data_Merge2$Season1 < 7, 
                                    "Spring", "Monsoon")))
```

I would also like to include a variable to el nino or la nina years, as per a discussion with NPS regional hydrologist, Salek. 
```{r}
# Bring in the data
Nin <- read.csv("Data/Raw/Nino_Nina_Raw.csv")

# Pivot - begin creating a date column by converting to long format
Nin <- Nin %>% pivot_longer(cols=c('DJF', 'JFM','FMA', 'MAM', 'AMJ', 'MJJ', 'JJA', 'JAS', 'ASO', 'SON', 'OND', 'NDJ'),
                    names_to='Month',
                    values_to='NinIndex')

# Use the central month
Nin$Month <- c("DJF" = "01", "JFM" = "02", "FMA" = "03", "MAM" = "04",'AMJ'= '05', 'MJJ'='06', 'JJA'='07', 'JAS'='08', 'ASO'='09', 'SON'='10', 'OND'='11', 'NDJ'='12')[Nin$Month]

# Create a date column to represent the first date
Nin$Day = "01"

# Convert all this to numeric
Nin$Date <- as.POSIXct(format(paste(Nin$Year, Nin$Month, Nin$Day, sep = "-"), format = '%Y-%m-%d %H:%M'), tz = tz)

# Remove the columns with no data so that it is properly attributed.
Nin <- head(Nin, -4)

#[https://stackoverflow.com/questions/50167509/converting-monthly-data-to-daily-in-r]
# Create an xts in order to attribute each data point on the first day of the month to ALL days of the month
NinXTS <- xts(Nin$NinIndex,order.by = Nin$Date)

# Lengthen the dataset to represent all dates
Nin <- na.locf(merge(NinXTS, foo=zoo(NA, order.by=seq(start(NinXTS), end(NinXTS),
  "day",drop=F)))[, 1])

Nin <- data.frame(DateAsDate=index(Nin), coredata(Nin))
```
Merge this with e coli dataset

```{r}
data_Merge2 <- left_join(data_Merge2, 
                          Nin, by = c("DateAsDate"))
```



The columns are getting messy. Remove duplicates and nulls
```{r}
# Remove extraneous columns and reorder

# I had previously kept hydrologic condition and event in, but there are so many nulls I don't think it will be worth it. LAURA may be able to attribute the condition (rising or falling) based on the staff gage. Will need to speak to hydrologist.

# Columns are organized so that the "metadata" columns are in the beginning. These will not be used for correlation views.
data_Merge3 <- data_Merge2 %>%
  select(ActivityConductingOrganizationText, MonitoringLocationIdentifier, QuantificationLimit, DailyFlowCount, DateTime2, StandardizedResult, Discharge_CFS, USGS_Staff, QuantificationLimit, Precipitation..in., Tmax..F., Tmin..F., Previous3Precip, Previous7Precip, Previous30Precip, Season, NinXTS)

# Remove old data.
remove(data_Merge, data_Merge2, DailyClimate, Nin, NinXTS)
```


```{r}
data_Merge3 %>%
  ggplot(aes(x=Season, y=StandardizedResult)) +
  geom_point() +
  geom_hline(yintercept=126, linetype="dashed", 
                color = "blue", size=0.2) +
  geom_hline(yintercept=235, linetype="dashed", 
                color = "red", size=0.2)
```
Notes on above diagram: Monsoon values seem significantly above blue line. Winter values have many less maximum values.



graph this too.
```{r}
# Missing values are expected.
data_Merge3 %>%
  ggplot() +
  geom_point(aes(x=Previous3Precip, y=StandardizedResult), color = "red") +
  geom_point(aes(x=Previous7Precip, y=StandardizedResult), color = "blue")+
  geom_point(aes(x=Previous30Precip, y=StandardizedResult), color = "green")+
  geom_hline(yintercept=126, linetype="dashed", 
                color = "blue", size=0.2) +
  geom_hline(yintercept=235, linetype="dashed", 
                color = "red", size=0.2)
```

I should change the categorical variables to numeric for corrplot. After changing them to text. oops. 
Numerical Categorical
```{r}
data_Merge3$SeasonNumeric <- ifelse(data_Merge3$Season == "Fall", 1,
          ifelse(data_Merge3$Season == "Winter", 2,
          ifelse (data_Merge3$Season =="Spring", 3,4)))
```


I want to change the organization to numeric as well.

I'd like to look at organization to see if this has any noteworthy effect on the results. If it appears significant, this could be due to the time of testing (project funding is more likely when there is an issue) or due to the personell collecting the data. A strong correlation will indicate either of these issues.
```{r}
data_Merge3$ACO_Numeric <- ifelse(data_Merge3$ActivityConductingOrganizationText == "U.S. National Park Service", 1,
   ifelse(data_Merge3$ActivityConductingOrganizationText == "NATIONAL PARK SERVICE", 1,
   ifelse(data_Merge3$ActivityConductingOrganizationText == "Friends of the Santa Cruz River, Tubac, AZ", 2, 
   ifelse(data_Merge3$ActivityConductingOrganizationText =="FRIENDS OF THE SANTA CRUZ RIVER", 2,
   ifelse(data_Merge3$ActivityConductingOrganizationText == "U.S. Geological Survey-Water Resources Discipline", 3, 
   ifelse(data_Merge3$ActivityConductingOrganizationText == "ARIZONA DEPT OF ENVIRONMENTAL QUALITY", 4,  
   ifelse(data_Merge3$ActivityConductingOrganizationText == "INTERNATIONAL BOUNDARY AND WATER COMMISSION", 5,  
   ifelse(data_Merge3$ActivityConductingOrganizationText == "UNIVERSITY OF ARIZONA - MARICOPA AGRICULTURAL CENTER", 6, 
   ifelse(data_Merge3$ActivityConductingOrganizationText == "HARRIS ENVIRONMENTAL", 7,
   ifelse(data_Merge3$ActivityConductingOrganizationText == "NPS", 8,  # This is SODN data, Separate from other NPS 
   ifelse(data_Merge3$ActivityConductingOrganizationText == "VOLUNTEER GROUPS", 9,        
          NaN)))))))))))
```


## Begin looking at the correlation

I want to look at nulls from the climate data. I think this will have an effect on my corrplot. With removed rows, I still have 906 to play with.
```{r}
data_Merge3 <- na.omit(data_Merge3)
```

## Data Exploration 2.0

Now that I've looked at the overall data, make a test and train set to proceed.
```{r}
# De-randomize (for now, at least). 
set.seed(24)

#subset the data by 70%
subset <- sample(nrow(data_Merge3),nrow(data_Merge3) * 0.7)

#create training and test data from subset
traindata <- data_Merge3[subset, ]
testdata <- data_Merge3[-subset, ]

```

Pairs examination
Use the *Training* data to train the models. Test data comes in for validation procedures.
```{r}
# Ensure that only numeric data is contained. There was a previous cleanup but it still contained metadata.
traindata_numeric <- traindata %>%
  select(c(6:14,16:18))

# I can look at pairs
pairs(traindata_numeric)

```
Correlation Matrix
```{r}
# Create correlation matrix
cor <- cor(traindata_numeric)

corrplot(cor, method = 'square', order = 'FPC', type = 'upper', diag = FALSE, addCoef.col="black", number.cex=0.75)
```

It is important to ensure that I do not include correlated predictors in this model. If I choose a precipitation variable, it should only be one. Same for temperature and discharge/staff. 

Season and temperature are also extremely strongly correlated. It may make sense to use only one variable. 

Currently I'm looking at:
1. Previous 30 Precip - interesting that it has mroe of an effect than 7 an 3. Possibly something to do with flooding washing out? Or interactions when the water is extremely low and it hasn't rained in months?
2. T min
3. USGS Staff - there may be slightly more values between discharge and staff since discharge is sometimes questionable at high levels? Interesting that they are different by 0.1
4. possibly NinXTS?

```{r}

```

Note for LAURA:
Do I need to look at time of day?
What about distance from Sonoita. Number from north to south or vice versa?
What about the rise/fall of stage?