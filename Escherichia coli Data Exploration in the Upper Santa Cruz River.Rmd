---
title: "Escherichia coli Data Exploration in the Upper Santa Cruz River"
output: html_notebook
---

```{r setup, include=FALSE}
library(data.table)
library(dataRetrieval)
library(janitor)
library(knitr)
library(lubridate)
library(tidyverse)
library(zoo)
opts_chunk$set(echo = TRUE)
```

## Processing the USGS File

### Notes
This data is from the USGS continuous monitoring gage of the Santa Cruz River at Tubac. Data is all published and available for public use.

### Code
I have commented out the data pull for USGS data. It is many years and takes lots of time to grab all 300k records when running this script. To save time, I've outputted the result of: 
readNWISuv(site_id,c('00060','00065'), "2009-07-01","2022-08-30", "America/Phoenix")
as USGS_09481740.CSV and saved this within the raw data folder on 9/12/2022.

Original code may be un-commented and re-run if desired. Data after 1/10/2022 is currently provisional and is still subject to change during USGS QAQC procedures.
```{r}
# Inputs formatted for read NWIS function.
#site_id <- '09481740'
#startDate = "2009-07-01"
#endDate = "2022-08-30"

# Input timezone for NWIS and POSIXT functions
tz="America/Phoenix" # End date is 8/30/2022 to avoid null data at this time (9/5/2022)

# creates table in R
#USGS_Raw <- readNWISuv(site_id,c('00060','00065'), startDate,endDate, tz)
#write.csv(USGS_Raw,"Data/Raw/USGS_09481740.csv")

# This reads in the previously created CSV to save processing time.
USGS_Raw <- read.csv("Data/Raw/USGS_09481740.csv")
```

2015 had some times that were off by a few minutes from the normal 15- minute interval. This will ensure that everything is standardized. Values are rounded to the nearest 15-minute interval.
```{r}
# Create a New Dataframe to edit. I prefer to keep the old data available during this process in order to be able to quickly compare.
USGS_Standardized <- USGS_Raw

# Round data and put into a format for merging. Used DateTime2 throughout this notebook to indicate the processed DateTime.
USGS_Standardized$DateTime2 <- round_date(as.POSIXct(USGS_Standardized$dateTime), "15 minutes")
USGS_Standardized$DateTime2 <-as.POSIXct(format(USGS_Standardized$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)
```

This indicates that there are duplicate entries which have arisen from the 15-minute standardization. This is because the data was not collected on the standard 15-minute interval. The separate values were rounded to the same value.
```{r}
# Loaded plyr for later testing and it made this piece of the code upset. "dplyr::" should not be necessary, but it avoids issues.
USGS_Standardized %>% group_by(DateTime2) %>% dplyr::summarise(n=sum(n())) %>% filter(n>1)
```

Ah. This is due to the rounding process.
The following chunk creates a mean of the values (which will be within 15 minutes of each other, so it is assumed the values will be similar enough to merge) to use in place of the two readings.
```{r}
# This summarizes the flow column by the rounded DateTime. It will take the average of the two values if multiples exist.
USGS_Standardized_append <- USGS_Standardized %>% group_by(DateTime2) %>% summarise(mean(X_00060_00000))

# Rename the column to something more useful.
setnames(USGS_Standardized_append,old=c("mean(X_00060_00000)"),new=c("Discharge_CFS"))

# Merge the rounded data back in and rename the column name to something more useful.
USGS_Standardized<- merge(x=USGS_Standardized,y=USGS_Standardized_append, by="DateTime2",all.x=TRUE, all.y = FALSE)

#Remove the pre-averaging flow column. This allows to remove duplicates from the append pairs created above.
USGS_Standardized <- USGS_Standardized %>%
  subset(select = -c(X_00060_00000)) %>%
  distinct(DateTime2, Discharge_CFS, .keep_all = TRUE)

# Re-check for duplicate values. This should equal zero.
USGS_Standardized %>% group_by(DateTime2) %>% summarise(n=sum(n())) %>% filter(n>1)

# Remove append table - data has been merged
remove(USGS_Standardized_append)

```

Though the data is nearly complete, there are still some missing values for the 15 minute intervals. Currently the missing rows are not included in the dataframe.
This will create a full time series to include indicator of no data.
```{r}
# Create the time series, formatted as POSIXct
TimeSeries <- data.frame(seq.POSIXt(as.POSIXct(format("2009-07-01 0:00",format = '%m/%d/%y %H:%M'), tz = tz), as.POSIXct(format("2022-08-30 0:00",format = '%m/%d/%y %H:%M'), tz = tz), by="15 min"))

# Rename the Column DateTime2 to indicate properly formatted column.
colnames(TimeSeries)[1] = "DateTime2"
# Follow through on the name and format this bad boy.
TimeSeries$DateTime2 <-as.POSIXct(format(TimeSeries$DateTime2, format = '%Y-%m-%d %H:%M'), tz = tz)

#Left join data with time series, sort by DateTime2
USGS_Standardized <- merge(x=TimeSeries,y=USGS_Standardized,by="DateTime2",all.x=TRUE, all.y = FALSE)

# Remove the created time series. Data has been merged
remove(TimeSeries)

```
Notice that 2009-07-01 04:30:00 is now a row which indicates no data.


We now have to handle the nulls we created.
```{r}
# This tests to see how many nulls are present in the current dataset.
# 9/5/2022: 40544 records are null
count(USGS_Standardized %>%
 filter(is.na(Discharge_CFS)))
```

I think I should be able to fill some of the nulls by taking the average of the two surrounding values. 
```{r}
# This column is a dummy column which allows me to count more easily in the next block.
USGS_Standardized$date <- as_date(USGS_Standardized$DateTime2)
USGS_Standardized$count <- 1

# This counts the number of readings per day
TimeFlag <- USGS_Standardized %>%
  filter(!is.na(Discharge_CFS)) %>%
  group_by(date) %>%
  summarize(value = sum(count))

# Append the reading count value to the standardized data
USGS_Standardized <- right_join(TimeFlag, USGS_Standardized)

# Name this something meaningful
setnames(USGS_Standardized, c("value", "X_00060_00000_cd", "X_00065_00000", "X_00065_00000_cd"), c("DailyFlowCount", "Discharge_cd", "USGS_Staff", "USGS_Staff_cd"), skip_absent = TRUE)

# Remove extraneous columns and reorder
USGS_Standardized <- USGS_Standardized %>%
  select(DateTime2, date, Discharge_CFS, USGS_Staff, Discharge_cd, tz_cd, DailyFlowCount)

# Remove Timeflag table. Merged into data
remove(TimeFlag)

```

Start to fill null data where applicable. This will fill in all data into a new column.
```{r}
#Filter for columns with a daily flow count greater than 88 (no more than 2 hours per day missing.)
USGS_Standardized_Fill <-
  USGS_Standardized %>%
  filter(DailyFlowCount>88)

# Fill in missing data
USGS_Standardized_Fill$Filled <-  na.approx(USGS_Standardized_Fill$Discharge_CFS, rule = 2)

#Join back in
USGS_Standardized <- right_join(USGS_Standardized_Fill,USGS_Standardized)

# Remove the old data
remove(USGS_Standardized_Fill)

```

Check how many nulls there are now that some of the data is filled. More data filled would mean more assumptions.
# 9/12/2022: Current count of null data is decreased to 37385
```{r}

# Fill in the un-filled values
USGS_Standardized$Filled<- ifelse(is.na(USGS_Standardized$Discharge_CFS) == TRUE,USGS_Standardized$Filled,USGS_Standardized$Discharge_CFS)

count(USGS_Standardized %>%
 filter(is.na(Filled)))

```
Clean up the columns 
```{r}
# Remove the un-filled discharge column
USGS_Standardized <-  USGS_Standardized %>%
  subset(select = -c(Discharge_CFS))

# Rename to something meaningful
setnames(USGS_Standardized,old=c("Filled"),new=c("Discharge_CFS"))
```

Output the file to processed data so that I have a checkpoint and don't have ro rerun everything. Commented out because it will not be necessary in the final code process.
```{r}
#write.csv(USGS_Standardized,"Data/Processed/USGS_Standardized.csv")
```

## Processing the E. coli data

### Data Retrieval
I have raw data from the water quality portal (EPA) a summary of ADEQ, USGS, FOSCR and NPS (Tumacacori) data along with a download of NPS-SODN quarterly sampling data (approximately 35 samples). 

Data location and security: https://www.epa.gov/waterdata/water-quality-data. Data is published and public. SODN data is unpublished and should be considered provisional for this project.

Storet: https://www.waterqualitydata.us/ 
Country: United States of America (NWIS, STEWARDS, STORET)
State: Arizona (NWIS, STORET)
County: US, Arizona, Santa Cruz County (NWIS, STORET)
Site type: Stream (NWIS, STEWARDS, STORET)
Date from: 7/1/2009
Date to: 8/30/2022
Sample Media: Water (NWIS, STEWARDS, STORET)
Characteristic Group: Microbiological (NWIS, Storet)
Data profiles: Sample results (biological metadata)

SODN: Data was pulled using a sql query on a DOI/NPS server. More information can be provided by the SODN data manager, currently Helen Thomas.

### Sampling Locations Information

Storet: https://www.waterqualitydata.us/ 
Country: United States of America (NWIS, STEWARDS, STORET)
State: Arizona (NWIS, STORET)
County: US, Arizona, Santa Cruz County (NWIS, STORET)
Site type: Stream (NWIS, STEWARDS, STORET)
Date from: 7/1/2009
Date to: 8/30/2022
Sample Media: Water (NWIS, STEWARDS, STORET)
Characteristic Group: Microbiological (NWIS, Storet)
Data profiles: Site Data Only

SODN: Single sampling location coordinates were provided by NPS and manually added to the storet download CSV location file.

### Sampling Location processing
The storet download file was placed into Arcmap and points within the sample range were exported to the included raw csv file. The locations for data download were chosen from the Upper Santa Cruz River. Locations are limited to North of the NIWWTP and Sonoita Creek confluence to avoid data noise from two likely point sources. All locations sampled should contain the same water without significant other inputs

### Below, The data was limited to:
1. sample location (above)
2. E. coli only (not total coliforms)
3. same method (EPA Standard Method 9223B, Colilert, sampling procedures)
4. comparable date range (Post 2009 NIWWTP significant upgrade project)


## Code
Read in the raw data files
```{r}
#This reads in the checkpoint file, if created.
#USGS_Standardized <- read.csv("Data/Processed/USGS_Standardized.csv")

# This raw file contains the Storet and SODN data. The 30 SODN records were appended manually.
RawStoret <- "Data/Raw/ecoli_Storet_Raw.csv"
RawSODN <- "Data/Raw/ecoli_SODN_Raw.csv"
RawLocation <- "Data/Raw/ecoli_Locations_StoretandSODN.csv"

RawStoret_data <- read.csv(RawStoret)
RawSODN_data <- read.csv(RawSODN)
Locations <- read.csv(RawLocation)
```

Merge the SODN and Storet Raw Data files
```{r}
# Merge the two dataframes together. SODN raw data was outputted to match the STORET format. Fields were manually filled by data management.
ecoli_data <- rbind(RawStoret_data, RawSODN_data)

# Use Janitor to clean null columns
ecoli_data<- remove_empty(ecoli_data, c("cols"))

# Remove the pre-merged raw data
remove(RawStoret_data, RawSODN_data)
```


Now we must clean up the data further.
Keep only the columns which contain E. coli data. Other colifoms or enterolert (etc) procedures are within the STORET output. 

Note: Fecal coliforms may also be applicable, but for the initial study I would like to stick solely to Escherichia coli. Nick Paretti (USGS) indicated that this maybe a better options due to the increased varaiblility. Fecal coliforms (non-species specific) have maximum results much more often and may be less meaningful.
```{r}
# STORET has standardized the output as "Excherichia coli"
ecoli_data <- ecoli_data[ecoli_data$CharacteristicName == "Escherichia coli", ] 
```

Remove columns with methods other than 9223B
```{r}
# The methods are not standardized in STORET. It is assumed that all forms of Colilert and SM9223B represent comparable results. Colilert comes in both an 18 and 24 hour test, but they are assumed to provide a comparable output. 
ecoli_data <- ecoli_data[ecoli_data$ResultAnalyticalMethod.MethodName == "E coli, water, Colilert (24-28)"|
                                    ecoli_data$ResultAnalyticalMethod.MethodName ==  "COLILERT (EDBERG)"|
                                    ecoli_data$ResultAnalyticalMethod.MethodName ==  "COLILERT"|
                                    ecoli_data$ResultAnalyticalMethod.MethodName ==  "SM 9223B", ] 
```

Remove dates before 2009 when the plant was updated. June 2009
```{r}
ecoli_data_9223b$DateAsDate <- mdy(ecoli_data_9223b$ActivityStartDate,tz="America/Phoenix")
ecoli_data_9223b$DateAsDate <-format(as.POSIXct(ecoli_data_9223b$DateAsDate),format='%Y-%m-%d')
ecoli_data_9223b_date <- ecoli_data_9223b[ecoli_data_9223b$DateAsDate >= "2009-07-01", ] 
```


Merge the files based on monitoring location identifier.Monitoring locations were outputted from an ARCGIS lasso of the range between the NIWWTP/Sonoita Creek inputs and the end of the Upper Santa Cruz River.

This will decrease the number of rows in the dataset because it excludes measurements outside of the study range (namely the sonoita creek data)
```{r}
# Merge on the Monitoring Location Identifier (outputted by STORET). The single SODN monitoring location was manually added to the end of this file before import.
ecoli_data<- merge(ecoli_data, Locations, by = c("MonitoringLocationIdentifier"))
```

Remove unnecessary tables. 
```{r}
remove(Locations, ecoli_data)
```


